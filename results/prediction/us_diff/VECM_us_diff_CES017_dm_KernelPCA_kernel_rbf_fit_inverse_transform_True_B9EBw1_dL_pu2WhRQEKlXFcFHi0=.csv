# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; CES017
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=10, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.051392398270797106
0.05309172778571194
0.028125177870704912
-0.00672833123573751
-0.043051586116581546
-0.008158462823112542
0.024620017992181234
0.011500245575127098
0.012883145840050781
-0.011899840450869847
0.019814086264290768
-0.02782889115232275
-0.027662553828712466
-0.03324849428537285
0.025624949555594446
0.027159138836112155
0.00023917253231817842
-0.013093769384635294
0.024661200591920958
0.020971925282878667
0.0005770612851579288
0.050269271925652795
-0.039545693645271246
-0.01820639303263894
-0.04651278698959166
0.003244144179759153
-0.018321469491887166
-0.03005024855146196
-0.02052863154223958
-0.08019251519984619
-0.08791885759204045
-0.005612632250902997
0.044130574713649694
-0.01534556475288491
0.015423602560795068
-0.012985648896788824
-0.029332758230863856
0.013990963996601031
-0.0031337817110166435
-0.034538719049803816
0.020113639658255376
0.01737568322084903
0.02944763532241531
0.03391523909732841
0.025668617051488863
0.024976154907350386
-0.012876247003051754
-0.009890256695132788
-0.0218672384949548
-0.03227167506262624
0.0063973130001890345
0.04335778730365667
0.033511856732437374
0.053729710647550236
0.010172655656094884
0.003549433129871625
0.029408496704780285
0.012857443149529545
0.010828320467521793
0.011913452196491621
-0.03503385502999825
-0.0001353313892607788
0.02196880012509109
-0.016892194918402197
0.03235697542578789
0.011952617306578785
-0.016687108618618558
-0.024580155788026352
-0.011913367594521369
-0.015823620778185388
-0.03795203805530737
-0.051297083766403094
-0.08642778758363737
-0.08782113853778793
0.012390572444074082
-0.022673468728931268
-0.01048758707921997
-0.050256503230645885
-0.06647569955227374
-0.017173256080720498
-0.012790734372113373
0.004448691907229558
-0.004254342302653199
-0.004795929956886385
-0.010292083130055956
-0.008197012694671166
-0.014779637126871413
-0.0015779423804789684
0.02227093061006414
-0.004918847983503104
0.028278187077342923
-0.005606605006211865
-0.035968120067766264
-0.007599359058531843
0.015418970248034035
-0.026725944474292194
-0.027120952329442116
-0.03182969544984194
-0.042094424043742404
-0.014253614646509026
