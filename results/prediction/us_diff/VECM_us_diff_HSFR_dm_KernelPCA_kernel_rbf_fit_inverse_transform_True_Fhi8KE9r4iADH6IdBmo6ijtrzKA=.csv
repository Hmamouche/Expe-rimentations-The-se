# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; HSFR
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=12, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.08132466237812792
0.03907032314273326
-0.008338051712183797
-0.10048512342655715
-0.035551655103604175
0.09640917117362902
-0.010612791314949657
0.009162875926023148
-0.03979151786265993
0.022532525136137564
0.04716653698417497
0.1168402890922012
-0.09455557220038013
-0.018549680859673935
-0.09195777638273336
-0.06620762561015407
0.13222236554803385
-0.04024115174941986
-0.0014598096304495878
-0.06810247808248822
0.04238700863364793
0.047470958766526634
0.0013638632703234562
0.030026641386709206
-0.06133936599901533
0.04937975491971049
0.022855130136294935
-0.10368617646752512
-0.052786228848154704
-0.049600350598447515
-0.07257303557733448
0.09180862267305798
0.020077787952848924
0.008274961851592016
0.024624382725034977
0.022023869089522863
-0.007680339327854474
-0.038905467485091785
-0.002264532509500112
-0.1500347487350866
0.07804021057855849
0.055282070034554646
0.04092889847915994
-0.10365274361054062
0.10315368428578527
-0.13228041237699267
0.014077780601173082
-0.023125053740998568
0.024064321836421564
0.022037307367586214
-0.029394015223256388
0.04263578038902688
0.026029067653565913
-0.05379937902231186
0.08356581332923287
-0.05104112667751198
0.03610106963418817
0.014732179664711182
0.005342521793286859
-0.045089982660839495
0.046286649951137876
0.01827517356517925
0.01241517585217208
0.0167475931960915
-0.01093833444040984
-0.012679537443199808
-0.05965775853866131
0.04257074079296684
-0.03533041942948263
0.05354650156114456
0.024506193605097048
-0.0018699498178443748
0.021981073042809912
-0.006124767985950885
0.08759884430786141
0.04873594818810529
-0.06998281110817343
0.01475629465816272
-0.07594738282484965
0.004643636039619044
0.08545522473215275
0.03138792017336179
0.046595584268412954
-0.004328227113534519
-0.009330651786489324
-0.029187419392222635
0.007432756044328668
0.0183578947724565
0.02041475047281038
-0.0017272074406816454
-0.010549222272416094
-0.024151465657721628
-0.03696271105732452
-0.026808394888497657
-0.038462773322290277
-0.0235844374689611
-0.056896035408386336
-0.186664145072258
-0.007243476956143581
-0.1017031628626243
