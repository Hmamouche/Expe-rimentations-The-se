# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; Sfygt10
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=12, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.08953312996511503
-0.1148362015384182
0.14735038922486643
-0.008720083418231592
0.02051812369176593
0.11157016243034146
0.05516359620779353
0.14931724657953344
-0.2899253007207012
-0.02436602364245531
0.074937800652364
-0.00273323406435811
-0.052402804188724714
0.024111344526017653
-0.11089335965631406
-0.09856930359905992
0.15666996178233508
0.20027463029800294
-0.14739560956820047
0.04972730845730671
-0.1296896961371743
-0.14433856204525944
0.0018750721891400094
-0.129540861747172
-0.08631300980683904
-0.17197303538788045
-0.1426691714151131
0.13583835417846313
0.04715704868503135
-0.10356787358868638
0.2754064713011966
0.16270962009232876
0.07683757786869921
-0.02226961200032304
-0.16528776208490312
0.09686767175782376
-0.0074330644790175995
0.22067443018550711
0.18232058603109108
0.006278459081885354
-0.13585202754157374
-0.015695572369569387
-0.01485153647291123
0.041346592925704875
-0.01958508680478161
-0.00569872763885176
-0.07461198551273315
-0.04146800850274221
-0.21064521298859562
-0.043889585992072655
-0.0370641002132057
0.10523373620249449
0.03163076315394224
-0.030164196783997017
-0.06908026220549271
-0.012930436744643548
-0.0952584084072449
-0.08056043754962261
-0.007727768465746866
-0.015799513603323427
0.054305869727872894
-0.05362303294028871
0.031166141452511145
0.1349809129553113
-0.10047210618108897
0.013289968146608974
-0.04146768568808397
-0.05079278493056373
0.0603748530206484
-0.0910334636892335
0.021004424818822476
0.09349346002363507
0.16357262259119448
0.3212893134970986
0.02818132928299788
0.11797769019781475
-0.2624158361509304
0.19374516981453108
-0.05125738499575798
0.05829875374302858
-0.08184335692545999
-0.12444057600393939
-0.05471852947471008
-0.09599735220882945
-0.07462114212753745
-0.06607680580164298
-0.10086527341725349
0.030171220318099035
-0.09513182504907121
-0.06753235745810215
-0.044046519457942565
-0.031208901856328665
-0.11184984614384828
-0.03343735368008324
-0.05946382246243596
-0.1297281638871863
-0.012391873686623135
-0.0009114124746931115
0.11886694240844184
0.028416986520115678
