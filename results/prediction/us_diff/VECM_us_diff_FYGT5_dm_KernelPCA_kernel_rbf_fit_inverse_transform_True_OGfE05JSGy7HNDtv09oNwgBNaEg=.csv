# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FYGT5
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=7, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.1596993492503197
0.06756708740470936
0.09247532889998124
-0.007010636407974655
0.01882733421727866
-0.06876742673489719
-0.0212217077095734
-0.06715141190679555
-0.03292382985862547
-0.06733026422321094
-0.06064129102040246
-0.01620232701388129
-0.07782236042663392
-0.022569964921504287
-0.07239903550181642
0.01112712476399048
0.0798735844489141
0.04836211758966126
0.040770271046300585
-0.027823881152503213
0.030892027987399245
0.07160456753727241
0.002163497737920711
-0.01643642159837444
-0.031090985804112525
-0.016997146795362315
0.0024007217768018674
0.0031300083859600453
-0.06779224922696338
-0.03316168617854816
-0.006394088871094928
-0.06454919329333428
0.0466867739835109
0.022628668076677242
0.01083292892513358
-0.04452313570725051
-0.06777261902401976
-0.00043384062056995437
-0.04971872604324827
-0.0341847193819558
0.03128198348875901
-0.0002872994575463389
0.023073787513873345
0.048636917153304024
0.04000890766783147
0.09330200998797279
-0.00839473918369037
-0.025038154380143715
0.009314365598468752
-0.03254678577067489
-0.020087833228883023
0.01907382261344696
-0.008877626352813731
-0.004312336669618366
-0.0033732220804143995
0.031820365299130696
0.03331444825564259
-0.009955457824739958
-0.00668638166970488
-0.041518876458535595
-0.015471618249576186
-0.04328594552362987
0.02128312236088604
-0.032413718174933825
0.020975947315229466
0.050438164381304766
0.023865708533150634
-0.010942943088267137
-0.008729471097043282
-0.02410545599982818
-0.025030800073807317
-0.02292346440998621
-0.031376661707678576
-0.08554133702539464
0.03046157126023423
-0.021835602823945578
-0.046671642015051105
-0.02095835015355279
-0.06668846002408485
0.002362755905264073
0.0416801931615612
0.024079858545835945
0.0633569982687929
-0.002300303425441982
-0.040790479628030736
0.02520977969881402
0.010029586337615103
-0.0029047549042401956
0.01815553935986752
0.02424882404485606
0.04063471677581882
-0.009110185854390305
0.009113603405910461
-0.005230056497709152
-0.016288545653644636
0.045684817997897496
0.027907353209771793
-0.08312352263852034
-0.07207077760050343
0.016004728965874928
