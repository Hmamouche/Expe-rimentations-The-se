# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; GDP286A
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=9, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.006876460864627601
0.00403562046457644
0.009881389853564376
0.007112163643089584
0.005641196564042696
0.005160970974401584
0.005147143347648526
0.0013542939200760612
0.0036821412110527776
0.004624996241370644
0.0033348192624232326
-0.00015915747963008613
0.0026608009313242335
0.004087404478620626
0.004993019302942351
0.006382690313732259
0.0041892238788694075
0.005680225528683009
0.002080833356272869
0.0023412141807277946
0.0028545977587325962
0.004781243947408326
0.004271794554990603
0.004028933110719679
0.0038675205892265098
0.005412744372018618
0.005141100452066045
0.004903311651183116
0.007821239331192465
0.007899512504844617
0.00431289103789447
0.006689367788092548
0.007298239280107895
0.0013764380766196484
0.0026404575864990448
0.005379812595334927
0.003262066499377556
0.004190347625800679
0.005315524758730015
0.0018797089559576471
0.004058695649445126
0.0044167295294509044
0.0035330795708760883
0.006649584217673738
0.006926200471778477
0.0032916735596103287
0.004270794117986804
0.0038234800353121033
0.004802946552100961
0.005089459689249276
0.005036337416971252
0.0032493280650832328
0.005319412345257428
0.004489974912689971
0.001645265570463864
0.004284561372349475
0.004042503571798916
0.0040263179618744436
0.0005358939331818777
0.004425255274970323
0.002506568384105158
0.004103138158195252
0.002620012343164872
0.003773289383559523
0.007231944272925449
0.008656332911188978
0.008458848353149112
0.00554777179533729
0.007653318836941772
0.009033457738347623
0.004354592621961344
0.007985052142654814
0.004438895176052681
0.0026010256383272993
0.005183633538151757
0.004144464457699074
0.007359574758662029
0.009439363986158885
0.008275373473975072
0.0051159316320798075
0.010491714052814984
0.013817174205446267
0.0012020244591420199
0.01068907801549039
0.012005935562902841
0.01276683126604497
0.010925278725788433
0.012666645558922772
0.01427595098894581
0.014864631431060897
0.0109637165725754
0.015222721638439912
0.009642394952051384
0.010973736676051352
0.010556664864676456
0.008719844244081143
0.012618478730207711
0.014584567033026723
0.011290081677840577
0.012776960003474298
