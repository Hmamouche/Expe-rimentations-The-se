# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; EXRCAN
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=6, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.0066906482358897144
0.0074345362997386005
0.039198651784166864
0.03965329785627748
0.037339048404670394
0.024520382525238322
0.021365505837746365
0.025235521151629765
0.0034448329178273546
0.004969621352785812
0.026455812007774297
-0.0035394739629315006
-0.00966630039203738
0.004696148484075869
-0.025446611108196217
-0.024620234122176232
-0.026883886178503957
0.0023948101137725923
-0.03820109894144859
-0.031491802007818405
-0.03960462373682453
-0.014107201675136306
-0.03624166377104311
-0.020437354138235705
-0.018159020164361692
-0.014841447763157196
-0.008086625905017361
-0.02468223806383465
-0.031715809167586706
-0.02627784463306663
-0.0020206524930460393
0.02673524493259087
-0.016740133321982445
-0.0015380128405881079
0.03692317544566713
-0.011797997998288384
0.02505734622849398
0.06584305744265281
0.008952371424497257
0.045917326611868654
0.021988748572663377
0.037631596965493744
0.04360285518223292
0.03892330854530637
0.02458316539477922
0.03644396010813262
0.009932746337998347
-0.015523388287722156
0.02442169909323944
-0.055113446380139736
0.021951761474346194
-0.001968497055244666
0.0012002390372375764
-0.009748477563744532
0.004719743180276952
0.005706038214645284
0.023735233819114517
0.03350141561258412
0.015317071857586259
0.03058860895716732
0.0627388153350524
0.035932251774052466
0.04152105795302277
-0.026006667351213795
-0.001835816135980401
-0.018663074276065766
-0.009281040211399312
0.00967992797296287
-0.012201975697175436
0.052535036716144935
-0.01898466859025119
0.04474949936742542
0.009035582935502018
0.028453023795365566
0.013178126536111032
-0.004822110273094246
0.013548764686531675
-0.027454017677990623
-0.010890567972113886
-0.08885250916115121
-0.02016855428692296
-0.12989660582717108
-0.01852994618368611
-0.043700442236975216
-0.03503752398206006
-0.04360543889515827
-0.037354341786982055
-0.05054818750473572
-0.039365702977021036
-0.025700800031577774
-0.04178857628139953
-0.04415825241034676
-0.022398029813278642
-0.023944390464956267
0.013395332068584068
-0.02755270836374397
0.009552310232150396
-0.10555862832110888
-0.02511870422317411
-0.07795997708278213
