# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; GDP286A
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=1, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.005407157908437977
0.0021009804380122364
0.009210975003581744
0.008191044805921041
0.006502246012166606
0.007261742060161293
0.005692923822986199
0.0037322460458841526
0.002501308521371817
0.004642804491897841
0.0020735888920190553
0.0018946655474352979
0.003020163960883025
0.002512737163166731
0.0037407482195274404
0.0038357496026464647
0.004159434257714012
0.0033828530815259196
0.003780996125767162
0.004331325888482246
0.003306620233850908
0.003910581418033135
0.005157059137390403
0.004212321950378281
0.004162484185881022
0.005020028609554036
0.00510036744614989
0.0048355612050727205
0.005604630701289967
0.008524855652101352
0.006468265638878221
0.005097967179025789
0.005723669053111422
0.0030422302599250302
0.003446885759733534
0.004856648978917947
0.003927927741565074
0.0036123632434417394
0.004782622066656318
0.0037386723086687726
0.003778188492868919
0.00496462518317143
0.004344690328571221
0.00442275062485701
0.004344529791280605
0.005342144419152936
0.00536003918466485
0.004242867602711267
0.004670390174188549
0.0053347706874658785
0.0046901326303551046
0.0028334598713278163
0.0055724280345076675
0.0042814120454486605
0.0016331083236222766
0.004477759303775097
0.003695929132794311
0.003965802219399724
0.0015057085266698324
0.002782623879885397
0.0035941128618607854
0.0022926725260637624
0.0035568617239065165
0.0046146618695681665
0.005505933685440217
0.00714361803264851
0.00903311105350461
0.0070516909977805225
0.008677628398620242
0.008506723292731
0.004713831251052904
0.00654751624215692
0.004123003978793255
0.0035354727063488397
0.004895591553652333
0.005058996236056559
0.0054209214457695155
0.008790506552156456
0.009326505235079368
0.007590904927371773
0.00972462873048854
0.011520735354083277
0.004318182240925681
0.009007537103486164
0.00942382161388938
0.014372223173622531
0.01255833385520171
0.011962815817796516
0.014472233305911608
0.015351350357589945
0.01105375058211301
0.014923149326682407
0.010789261630155401
0.009674445689437183
0.011044151358514085
0.009999169358589043
0.00969071316264332
0.014931677981188997
0.01299674228191675
0.012910195811639573
