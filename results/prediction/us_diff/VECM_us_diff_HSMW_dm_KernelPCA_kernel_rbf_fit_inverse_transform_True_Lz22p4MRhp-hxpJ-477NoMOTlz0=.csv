# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; HSMW
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=11, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.2840879763292467
-0.10555901324483637
-0.2502725307690579
0.0564138119722041
-0.15982754461785714
0.23638798381037973
0.1236266140869404
-0.07398644520031872
-0.08906025501639296
0.04446939548131512
0.029294362175779853
0.07647888673420014
-0.09296605025009881
0.0937807122581557
-0.10356191256232182
0.029624795193344988
0.141713144781763
-0.10650467330346394
0.018544970919261432
-0.11723166713748959
0.07125331126369416
-0.023377408082647808
-0.027950882851516867
0.1302549983724574
-0.12372284235414903
0.1004084200248614
0.11691761587212904
-0.14816279808621588
-0.10277410011194796
0.0022752955342712003
-0.13605477083417564
0.1649428625415415
0.026993545079692358
-0.06170354691246255
0.026417972900780767
0.0323228498384634
-0.04429751301164998
0.08753571655045136
0.10527669981539933
-0.16660528468657793
0.07648143217865305
-0.022501495365232528
0.07739295823072076
-0.19728937468635796
0.1922852613640142
-0.13196130425733368
0.09552734850375962
-0.09777260516541696
0.003054596967498449
0.057416709919484464
0.023563334778976534
0.03419433662607526
-0.05280627679160245
-0.06291582812451725
0.14460629576189737
-0.09509241374873087
0.023023791440872434
0.02643362188741138
-0.016913079597385132
0.040449975071008025
0.04815419755332087
-0.02778230639751666
0.08080031857549816
-0.05562822814330094
-0.06378207042684189
-0.005422534068160387
0.006241877355211974
0.0033119547956729084
-0.03300669567978691
0.10341674476157398
-0.03227773055147995
-0.016559387605356517
-0.07036153118556182
-0.01092819360592119
0.20944545855970947
0.0221865536968176
-0.12009709553905318
0.0415601551005801
-0.000832945564853832
-0.05317560061323454
0.02277990227155131
0.06355402260883511
0.07480741934774407
-0.008259466616991975
-0.06020775410426238
-0.08306061490000591
0.01754328593309707
-1.7010718592929547e-05
-0.042463814864978494
0.0274798925457105
-0.029600158717998856
-0.09489048613547124
-0.05645521718859765
-0.026623436225732416
0.0033569880934463045
-0.05387963755986552
-0.08966415715785767
-0.1392409187894352
0.11278501202930176
-0.0536299446285058
