# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; Sfygt1
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=4, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.16717892027753245
-0.10912869588010592
0.009053938571790272
0.0870791100122238
0.0428495109817136
0.07304715201031999
0.05944250644720133
-0.07350735817684986
-0.1754732816868676
-0.009769954074833206
0.00577475884362047
-0.019876374544172487
-0.031058147812400683
-0.01787272262235904
-0.11401181827078366
0.05523819179310404
0.0566557618573408
0.08805880844155307
0.0993502541876233
0.02358561715114371
-0.028173010084761477
0.023459000355807037
-0.03993943411528722
-0.010672261674769007
-0.0623322445494085
-0.08560876165384021
-0.07856819357866916
-0.022012704056281383
0.03989493820256162
-0.009448711994894513
0.054366744839988886
-0.013200713636561497
-0.05511695607290842
0.05298836779621913
0.017694126194036014
-0.059232997448167644
0.005674551798395117
0.026751941810562015
-0.015833140363181526
-0.006347496133800397
-0.00844770571078413
0.014416507735702761
0.033320782281219125
0.08889986591649704
0.03112669589058418
0.08492815556194369
0.07068885662839465
-0.026952754021426988
-0.04179693716786226
-0.14432625139749716
-0.09548060860805437
0.01947801557259863
0.0415180404790013
0.02198236511744514
0.0798766720705047
0.04476784098912268
-0.05051897250565706
0.021030703732314332
-0.011054895097827401
-0.059018262290714475
-0.03121374824482039
-0.04974882734082407
0.012612428710555018
0.040454380551026584
0.010735762244969796
0.03499991329183474
0.015233426165386907
0.0239771667771995
-0.03995003518381792
-0.034868702971556594
-0.12233929387002829
-0.020291291134382716
0.00602933234197706
0.03370130703743137
0.008346397028233261
0.09236536827677923
-0.04204653864367383
0.02785302070110209
-0.1052420193903098
-0.004544163178101291
0.016580437118786144
-0.026857656507063905
0.05367519050162239
0.06929208347969576
-0.021073278440940466
-0.022673052381323066
0.026944128498946136
-0.011294426040040375
-0.022047696511104532
0.008215872072316227
0.031112104848533422
-0.04162931309160816
0.007704589956694651
0.019618006684209235
-0.0872087736944236
0.002996432085050982
0.024362538913076444
-0.020953247921531002
-0.029068412815417072
0.06413673454407526
