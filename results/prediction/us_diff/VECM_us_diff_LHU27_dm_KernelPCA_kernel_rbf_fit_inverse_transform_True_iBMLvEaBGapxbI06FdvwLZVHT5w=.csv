# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; LHU27
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=15, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.027886059416242955
-0.18659760034981016
-0.022486715779230976
0.0806757677166299
-0.03256433503219692
-0.008185504834132697
-0.06790537329301051
-0.002298721932249524
0.04611279261706507
-0.024625647769324802
-0.024454630478243602
-0.0018399999076293694
0.025800089729133033
-0.04714439849855613
-0.022972737656760202
-0.07525288500695641
-0.10321830054823831
-0.013756993657812383
0.001182476610361794
0.013750857223211281
0.00931081082594716
-0.043169696784681064
-0.006696605531783542
-0.002181936827103865
0.010842987223697295
-0.012142477390086162
0.006115426168516725
-0.0023035297785180724
0.06929183515435633
0.04641539323619226
0.08393697240690977
0.025104787414428082
0.005874137590372863
0.019574281304957822
0.05398780413168293
0.08396386402953325
0.09896902722156342
-0.007995031809119355
-0.028219347247908044
0.0104821558539289
-0.048736929708921115
0.035104544448300426
-0.025739119897628303
-0.040938328058480855
-0.03432520123221355
-0.019062484823122172
-0.07944744663625208
-0.006651129006562305
-0.034102397491074965
0.009941247022802663
0.02370630798935866
0.03296611652991336
-0.029806462340911805
-0.05807209976279196
-0.05207573733998123
-0.003064169604338871
0.0035971150326564163
-0.027858995289001563
-0.02818814686221721
-0.02909168835553478
0.039725192891816266
-0.006604175092532303
-0.01639531619538148
-0.0524461950559004
-0.010610852579653988
-0.049174155942360506
0.01816956263962062
-0.029971177633383528
0.027858833621206967
0.028098894828643924
0.03909083266287961
-0.003992890734923406
0.10025238512057409
0.10553939296654566
0.06945158837372711
0.05564861174985419
-0.01990552495104907
-0.01824572112420783
0.045542513334425466
0.08606532280858047
0.022468315335603173
-0.05846337952254193
0.005181185189879612
-0.016702965791837478
-0.05002705692183149
0.003073116737575278
0.000156870022587215
-0.03677684987734495
-0.0404029168667394
-0.0045720370196860585
-0.04328471085447
-0.027429857354702016
0.024210062488671108
-0.04300797042732649
-0.011309347593972527
-0.016763151087647726
0.030324438824843754
0.030675857705677965
0.04297071982200081
0.010091924745453056
