# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; LHEL
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=6, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.00220195381882091
0.029074958161235884
0.06286276081953976
0.01541916774172783
0.028464043287039595
0.05471810198787547
0.06648244958946005
0.02554340168138093
-0.0023016904900520756
-0.027276300330577173
0.007021468238303026
0.01547786180621511
-0.012413522729598728
-0.0011791521127722682
-0.008831581886897165
0.010828732208200676
0.04217564807303327
0.0192406346622528
0.012981864298697176
-0.022616895169408745
-0.0010833180327752979
0.005428784480643423
-0.025086298957536085
-0.02557995435050227
-0.020139585552847464
0.009179630938093606
-0.027056483813634027
-0.07457798611996475
-0.08299195419157848
-0.13813238881006232
-0.04833522790322653
-0.014960255177664156
-0.001727843273217823
0.002928697157989015
0.030413191736221406
0.006708362540076103
-0.008507494425102641
0.026410331477491188
-0.010372290220312034
-0.007872722843816729
0.012254048045242234
0.06477733150477039
0.09532436790459976
-0.01617049858078423
0.0244846462903548
0.024399133450551065
-0.0051544671879778245
0.0029901816101154007
0.025440171936060737
0.0039819165591953734
-0.005226218890287498
-0.029110566316063496
-0.009579227105114168
0.015305665872976906
0.023574286682892177
0.018447685396174536
0.028963976142828363
0.007875184700315679
0.028272789778714384
-0.011535182807595626
-0.01921423628477054
-0.01363911979249249
0.028015182726141404
-0.05049245783005599
-0.008825600035122188
-0.0015301035086981766
-0.011448617955452807
-0.027811722422614035
-0.028809857099206183
-0.032016665359973
-0.060606306291121875
-0.08536013951651364
-0.07355929653782019
-0.08731864577071666
-0.045889524347317676
0.00022777636946046467
-0.041981291008272595
-0.0162122144004707
-0.05407085137198186
-0.003279274880125016
-8.066203344150175e-05
0.04079533537538607
-0.0016425161771525497
-0.028385560665797557
-0.014156982884784156
-0.009722549949827516
0.008205237656651408
0.0031706259709607786
-0.004993077077483265
-0.005759160695415156
0.011407289746107437
-0.07073694067470332
-0.028091478569261558
0.007652591624460117
-0.013009996095755831
0.007142498682410139
-0.014845760335002008
-0.06501920451617604
-0.00888661830824914
-0.039669140696744945
