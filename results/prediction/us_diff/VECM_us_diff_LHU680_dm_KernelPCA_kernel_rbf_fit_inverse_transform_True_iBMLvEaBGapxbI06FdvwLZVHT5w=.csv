# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; LHU680
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=15, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.09417429267826628
-0.19754254574248287
-0.011285172507763055
-0.01882485532751428
-0.015431554895210187
-0.024339310562262694
-0.04036208565826394
0.07918033327464465
0.07119376423765617
-0.014310342865048006
-0.04275663593773924
-0.02076018365716829
0.05272264133797625
-0.12636379668153216
0.048072943925997216
-0.02447695473330767
-0.09475496872411865
-0.03990739819613444
-0.015254436494782982
-0.018843803685896394
0.0032000780903801915
-0.0182178904801261
-0.019632147667761377
-0.03237877877915537
0.009106304078919394
-0.0006971062475614916
-0.008210755362520458
-0.012653184491495273
0.07290257475283393
0.024285123513889423
0.08267037995030738
0.08698979339780921
0.03369687659551875
-0.024971544076214547
0.030783486230282372
0.07279696375520964
0.07608628485221212
-0.017943782080877595
-0.004777075334934945
-0.01905885266451943
-0.05586170340786711
0.057989758446576015
0.02986503975810794
-0.031286179948933526
0.024868962452047907
0.003338212924410335
-0.09654994331177935
0.04895322289447819
-0.049146036458027474
0.036098802842815705
-0.018700984044135213
0.059098554949911286
-0.03623035312196058
-0.034966045990607285
-0.021629224079420105
-0.03659186477723105
-0.017019589595084088
-0.038480693182311586
-0.004351768975267421
-0.043688790462648584
0.03732919577643004
-0.022723785114566082
-0.030848064502016617
-0.014789607123873828
-0.04260712226938711
-0.009878573393586526
-0.07037037989794856
-0.03816707354889416
0.0021752736413424905
0.00015970711187311432
0.06269038183133842
0.0382529795050395
0.17567461630209086
0.09002720394949819
0.017885186648874216
0.05459163592826142
-0.057334892290480215
0.042958136072325306
0.018246485048845393
0.08804346686521955
0.046807674457449946
0.023171445223332976
0.0491487116870381
-0.021190928307613877
-0.050774477362440645
-0.002458390825786523
0.004748965840974555
-0.028642274719734753
-0.016938398406200553
-0.021316361652534206
-0.05433608400633264
-0.03879735911113272
0.0044272450872357705
-0.038494946723834536
-0.005763468563804816
0.009914325074879826
0.04852431815893707
0.03152034113375542
0.01731507255604305
0.0003007095217925117
