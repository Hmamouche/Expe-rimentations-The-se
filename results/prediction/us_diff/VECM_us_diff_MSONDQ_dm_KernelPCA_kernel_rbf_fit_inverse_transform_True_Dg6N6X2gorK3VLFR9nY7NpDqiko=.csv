# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; MSONDQ
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=3, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.03617376342621355
-0.010071592304195284
0.028551305514105417
0.04462806170299939
-0.018059074544777842
0.021982897876280945
-0.010785380286085756
0.026625320034107475
-0.010405688874466397
-0.009907673490890855
0.013914131198353814
0.02199136999522039
-0.02913259776880108
-0.0466390061520456
-0.0038174395192253456
0.017694421430763146
0.02664752841859469
0.035280366218598196
-0.00828086770206497
0.05709364603184084
0.003031516903776575
0.054306443313825406
0.002874834772688948
-0.008108600409759026
-0.017050077557664748
-0.013860127249314744
0.014371632366328182
0.002009811285200296
-0.050391112888393846
-0.05555533985091281
-0.012113078303234567
0.051575185198563
-0.005271649473482988
-0.02362670740568827
-0.011866745535672803
0.0032015677858550223
0.005880152313129135
0.015668263161468972
-0.02527130357045181
-0.021431687946627035
-0.006347781312684852
0.035563816118564656
0.05750221113434713
0.038402079938394756
0.02211391524094288
0.021071466620737167
-0.004240176457906973
-0.006746924628650373
0.007951876690462144
0.005341509827698043
0.051007599708758886
0.04076605562100353
0.004546197203725809
0.016083336762899678
-0.0007418445516178367
0.025051246443403424
0.03180007018272101
0.0481028197454597
0.05379495119038151
0.014028852704078044
-0.019453043834748617
0.0037147808118788523
-0.0076180213184823655
0.006696379138549691
0.0008080965556293614
0.038378538305578844
0.03233255333437772
-0.0048275505640370635
-0.005411418666006579
0.005668605797463236
-0.006433298664290651
-0.012924191815141026
-0.05695335440532674
-0.07116587373259411
-0.06002544994826812
-0.038017379501874726
-0.050425418530714625
-0.01129489454884804
-0.03263132998021949
0.0008817543680202369
0.03038426907192554
0.04585061887041838
0.04785711752481076
0.003343867164833342
-0.014958022972317155
0.014376597190961331
0.01435958030809218
-0.004616014089421033
0.03791959157694556
0.02660708290722467
0.07385095734669019
0.027373044677096923
-0.036126398878218995
0.015262278702096987
0.0077315452961945474
0.01270467263823064
0.0010058792122669266
-0.0038051219718758
-0.013829798714036055
-0.009927507819887852
