# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FSPXE
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=4, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.010657889943985708
-0.04623119339625942
-0.03200093970994161
-0.03878265229275754
-0.01325111421495782
0.009461429323181992
0.023727793789130147
0.02427475094769979
0.023892414231726406
0.0356348642820248
0.042350920539719644
0.03652239382822136
0.006304209073514295
0.01703755732953182
0.05962885557473204
0.0006845772640364225
0.014988155285950221
-0.03986017480347029
-0.0377013558127924
0.014872796103500934
-0.0593041200723942
-0.034518953101198256
-0.006429372073802633
-0.0010372407320934224
0.015272847766799583
0.03856673534515712
0.015151038917888422
0.026872319865813333
0.011551057892137041
0.015525862837590857
0.06202120117421938
0.02048725032882933
0.000453563241008267
0.031150530592033113
0.05222975742969799
0.02700882625337119
0.01588112132355269
0.0349223947088647
-0.005277239323945478
-0.01571911379407305
0.0032853747021581456
-0.0064157514783773975
-0.04196095973278585
-0.05749561529586293
-0.03250083141865239
-0.0501602478956601
-0.03728158440192947
0.0034053589060247863
0.005583737228645487
0.013919730914183238
0.027517365737420814
0.011708763198766884
-0.004171458911909062
0.019283937555462958
0.0033660491253767267
-0.008853368951084215
0.024751461866252947
0.007896077808887281
0.033902292093897585
0.06077220946551296
0.021125457530698236
0.058378274845118816
0.07255844506709787
0.032695091470779834
0.005005480287961447
0.012579440025897407
-0.020493165584584985
-0.04342903577131361
-0.019553838155643176
-0.03422697716427956
-0.0048133694667674394
0.002474382299928613
-0.003386223413603129
0.09530114099690938
-0.011742390898006671
-0.06001351439669511
0.3901176651363576
0.19532558892576468
0.014400734348387092
-0.09353198668546908
-0.039781851933110166
-0.049379370404702874
-0.049901298469352616
-0.05206452368994623
-0.0953848878804726
-0.06256863503469842
-0.04658388907103751
-0.01872419170773074
0.00950627868633654
-0.01211631102007455
-0.016729822606313543
-0.03580307726232312
-0.009493145449030377
-0.015338024296416907
0.012446326628779803
-0.0009164029551584178
-0.008724539131864694
-0.008098623832003809
0.035078968605112275
0.018596485787015
