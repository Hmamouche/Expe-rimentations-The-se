# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; LHU680
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=3, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.01026720729814732
-0.080345231231621
-0.049707414557977406
-0.08983110438386688
-0.03219701148238424
-0.0033947835680257863
-0.010729622130345973
-0.0017904566459630983
0.012833500283926524
0.004920362456630791
-0.03019339932096977
-0.009758171042437369
0.01557265723924171
0.007109658184950271
0.003882196637427783
-0.03686174781437906
-0.049756069076517626
-0.0515785272913176
-0.008894548295424291
-0.039441446891971106
-0.016656497933509797
-0.028115251794693735
-0.02725910479459166
-0.01932419405606937
0.016071794002465403
0.014421224209637798
0.001172263417246393
0.00841266323994222
0.03556660386267531
0.04919574901165892
0.06176552228297411
0.06523538159961911
0.022786288927515524
0.039058637610582016
0.07356937719803837
0.053308424244883405
0.0517611955589085
0.06567144850694377
-0.008775962261933994
0.005048783069262833
-0.0010169017454804445
-0.011797404785606452
-0.007949854072975868
-0.017995538517170832
-0.018312841599826882
-0.04077524550599963
-0.059200196739427384
0.0033181656827586405
-0.027482291528899614
0.0019057022056550542
0.016288285030654395
0.028447892809211867
-0.026942554190011205
-0.03577712862868182
-0.03166804461797453
-0.03160308395408116
0.0033109779642818086
-0.04282483645121938
-0.023808687431844208
-0.027772962203600884
-0.0033727013664722394
0.009080447686913028
-0.03989201841494579
-0.012037093386515273
-0.03534932473477653
-0.031338459362558954
-0.047146334276721866
-0.010799904710322782
0.013602407041749124
0.00373391729181209
0.04980165069950745
0.031825617289949176
0.07512696327191294
0.0817253728405307
0.0657588536150955
0.04456555901567821
0.04716182863159613
0.06057279109268138
0.029358697580247346
0.0779195554549588
0.028054407679207305
-0.004501200665983519
-0.013459394655375455
-0.011381957297818535
-0.04367601618238476
-0.007357608472834448
-0.003086939780826141
-0.008119435308368096
-0.034814642959536936
-0.03204104870979855
-0.04001339107591989
-0.032960923251410694
-0.008523908314141935
-0.009707010156302992
0.006668151085424158
0.007493511186442067
0.028286656892994474
0.018417865409603183
0.028975693497209555
0.03731501639201915
