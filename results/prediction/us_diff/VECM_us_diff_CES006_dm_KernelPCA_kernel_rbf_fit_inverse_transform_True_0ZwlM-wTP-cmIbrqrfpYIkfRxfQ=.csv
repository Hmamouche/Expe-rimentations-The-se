# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; CES006
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=13, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.22621403956016
0.05754869111970157
-0.03585852502502801
-0.15267724208589542
0.030215007779685648
0.008710988830603159
0.0015461032825975565
-0.020726609164565923
-0.0037626265111441147
-0.021475786086853476
0.013762203191255921
-0.07556194515874312
-0.06314966547533404
-0.026652860744989352
0.011058011377109793
-0.02705413392191654
0.05602170152358709
0.015053693691691728
-0.09419814727165302
0.04912231387258897
-0.07262295316210128
0.03616683170396413
0.00026169627550710027
-0.031663758091810615
-0.02256366859086975
-0.019086470564920445
0.02147098984277438
0.024225616024479986
0.02227031268523657
-0.05738631340475632
0.04420772000573225
-0.08537834971197303
0.009762109226555887
-0.029467050635040626
-0.04906501252809216
0.020272537576691042
-0.05056531591418773
0.02802554601901499
0.05687515624396934
-0.009676150564843014
-0.02260178588490174
-0.035383045125787
-2.6549793055349533e-05
0.05133372150827116
0.01798835847141398
0.0030356521395746155
0.022571037988767095
-0.04634017421321001
-0.02318442668404638
-0.018786249197577144
-0.027165124671441193
0.05200180982370981
-0.0033948523590794748
-0.03160718711860047
0.03973307896798521
0.0007082058062110874
-0.000634124123315101
0.0026966733744837107
-0.02148346285197071
0.020711625101461
-0.015040580055414705
-0.037850153676213076
0.019764086680485585
0.005170258334007298
0.0032288327087723672
0.003534496608296892
-0.016065565218296697
-0.033029180525419324
-0.01969774643642244
0.008673808221074983
-0.02790611373188217
0.007459408227601715
-0.027398890510928635
-0.07863906081785818
0.07275875182299008
0.011537533974266448
-0.023341552166856495
-0.018441090374211985
0.0007732713925115049
0.00426072633123689
0.00371791465700572
0.005030130475393501
0.0370709045126275
0.026825657109012798
0.012218081743709495
-0.03278517788804361
0.03545515868814528
0.012399289660095639
0.02522527396010086
0.024049359252068168
0.02565368943420666
0.030842999424856903
0.01826192477189441
0.04149596356187373
-0.032147250265378416
0.03641798887362756
0.03058607390562556
-0.00022965081654256671
0.020031420010675597
0.002493948866242363
