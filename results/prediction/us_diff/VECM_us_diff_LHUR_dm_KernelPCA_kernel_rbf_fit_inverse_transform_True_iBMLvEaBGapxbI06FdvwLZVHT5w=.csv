# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; LHUR
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=15, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.09553650757384023
-0.32159413816477117
-0.059595842497794566
0.09855928828181773
0.07781089113114427
-0.053302508985726325
-0.08902736953874096
0.11323334510959705
-0.1217218407783839
-0.014554721182439773
-0.07101366044935498
0.01886755161589144
0.006994222847160071
-0.006449229593097374
-0.015665651997489153
-0.05819728046987482
-0.060365409072212536
0.07239258873579174
-0.06768200908774105
0.002816963263599471
-0.016549578339329865
-0.15041548759603163
-0.01719285917043228
0.026015721432261642
-0.015010856411396416
0.04103035762400691
0.02935444343005944
-0.014656231048674691
0.07167516695199382
0.08160733483398859
0.0731137831483119
0.017322640873018814
-0.02848703134569509
0.0054717511768421455
0.014456145661579574
0.014928791919573636
0.04565321411129878
-0.014331088624162651
0.02088131011804334
0.039613757613689106
-0.08359679082156982
0.015104386632163866
-0.0582999230356184
-0.018836588905976122
-0.012333348600581719
-0.04012207634346728
-0.04200641652230476
0.017199127723779954
-0.02712360184380783
0.014818579859868728
0.016162494663610662
-0.012537586782510561
-0.03965105324002102
-0.024434686297931066
-0.04790375132181402
0.01519785965282249
-0.03253971021950661
-0.028031529003148287
-0.029040035138037334
0.013677726243759234
0.03416632664957187
0.01772363178054598
-0.04356847677153
-0.019603222394370422
-0.00431641745950791
-0.024535475336953636
0.07097542111299615
-0.03887172916587014
0.02384623475323927
0.021386904972071137
0.05999212315043932
-0.0016130679465633894
0.07415037437970083
0.07309113793544723
0.009176969961525526
-0.039112516156549856
0.009902317294915768
0.047196710326871255
0.037378281542008944
0.0038694183639443464
-0.042657287458125
-0.052329503713212114
-0.04056620589469899
0.034634796416394435
-0.016739196083835013
0.009538700799889315
-0.007616553453044758
-0.025260097900268095
-0.024566751445462325
-0.007689150720711963
-0.026549279469501808
0.02005638791864846
-0.0054190749937786005
-0.04070983694531734
0.014189605974990005
-0.010340191183414053
0.008252009145869165
0.052135295063056135
0.052771702125173646
0.04540914549336838
