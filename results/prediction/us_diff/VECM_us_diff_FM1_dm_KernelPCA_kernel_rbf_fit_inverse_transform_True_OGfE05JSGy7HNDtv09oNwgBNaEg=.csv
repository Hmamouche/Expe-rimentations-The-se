# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FM1
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=7, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.010177270502290238
0.007598346988135182
0.006737427881921845
0.006833844649158721
0.007283267887876967
0.005751692955331347
0.007048113723954757
0.009521380507516584
0.009851481388982605
0.01019720727144044
0.013006335018637434
0.014010511519725013
0.015938294825114094
0.016883236158483944
0.01588529203405983
0.013190744737398612
0.003438130109491213
0.009664249321466974
0.00857632436363203
0.01065882846169367
0.006911153538258016
0.004370792279817836
-0.0021285625313051185
0.0002488604790488484
0.004146676015893116
0.008213492150573997
0.0051252419522174585
0.0020216511666845062
0.005211360681953614
0.004488532552401269
0.010144915326593763
0.010096341888090995
0.007725565304208587
0.015188276689403996
0.022222446891207046
0.01872646121330002
0.016390114626211667
0.021604431533297185
0.02085525622022789
0.019864981725189766
0.015313095162198172
0.02334480879996743
0.01191679788927354
0.007944318699361444
0.006284851921490432
-0.0010167280157635534
0.0028492399378451
0.0012721167050882807
-0.0004160005446353619
-0.004007387378861679
-0.008854427320469895
-0.007164172153535261
-0.00935184373508669
-0.010319393292999015
-0.008135340397400695
-0.01013997062692116
0.0026952777784174063
-0.0011869384230530195
0.006733999815724749
0.0014863588700035164
3.990636493607586e-05
0.008221365183124366
0.00418979173592476
0.0038254259476134637
-0.0020322345583692327
0.0031001099783275407
0.0023206344334888696
0.0005292600133166617
-0.0012812290776671369
-0.004986881283036093
0.006120515307589056
0.007028052092796052
0.026624361869365808
0.01789151522712293
0.0070599104553415466
0.003139621175212928
0.007226060788492849
0.009959014063661524
0.014237660777172768
0.018521060335049754
0.012838845366919087
0.009455321763337715
0.011842217855998302
0.015093397548032716
0.01323247685660241
0.011070764401853712
0.0035410493993988912
0.0005717298155889575
0.0030296266965863998
0.006075719230965974
0.004161243271916524
-0.0009629455711516854
-0.003069661159378299
-0.0037793467759055267
-0.00027436926067408583
0.00597951391540935
-0.0007911063923821038
-0.0014680160993448708
0.006249438943025247
0.002515389427441249
