# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FYGT10
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=14, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.02027223234220403
0.1299162075916515
0.21105068125795745
-0.11616503471860137
0.03332318412585231
-0.04424263649478871
-0.017533262396387944
-0.00808625492118667
-0.05898697248729691
0.09932752285623724
-0.13888281496252594
-0.15731050719310471
-0.09460152152271833
0.0021129605149947106
-0.005536614007747083
0.10181569224504461
0.07327357812922494
-0.03366485307594411
-0.01751131924584477
-0.05261698459792813
-0.058448759221421946
0.07124383616743732
0.03509223493411572
-0.08795498206858908
-0.029678472969795394
0.01670055251076298
-0.01569317077430433
0.040551904732476954
-0.005739644786834652
0.017985667879078245
-0.07416484491514633
-0.09113983489097274
0.07890208148797116
0.034300150348015965
-0.0420280819743414
0.005680307032766289
-0.09888548786112952
-0.005662020893417073
-0.03294722805394702
-0.047138377531874616
0.007194824393379061
-0.011324647190492892
0.029510340295374013
0.03836433002269555
0.03970021254659848
0.08051600874642759
-0.026441585653259762
-0.009439648553330432
-0.06482765566346507
-0.016428478929607207
-0.052900657350603265
0.04316329466896854
0.049358685853191746
0.021481632392994172
-0.010737299025169234
-0.010047788250551335
0.010932740699205062
-0.04887674189057778
-0.030347920307549132
-0.03148965498648446
-0.04558173837965117
-0.06930593890503989
0.08344528417487959
-0.0016038772878174193
0.04120485902959509
0.08299075971205824
-0.03212787443904748
-0.06289347124173172
-0.02280759897246231
-0.05199683113005158
-0.022411798356042782
0.03257835656846653
-0.02367907546551236
-0.017126576748462068
0.029495880509482
0.010993951775617822
-0.07572402867705172
-0.03244488266043888
-0.08864689455386433
0.046037502060724954
0.02806884479481131
0.055193417516508636
0.044419890519591484
-0.043593492245252864
-0.04965261640980706
0.010788672695487549
-0.04397845259670459
0.010182063660593479
0.006860549065344897
0.04352834876451968
0.021492952990093118
0.05323386270558408
-0.024719184401645344
-0.028606357027326626
0.015758573227571307
-0.018408139803084738
0.05919297556240382
-0.04880689281455876
-0.04994747948164955
-0.0447963940161136
