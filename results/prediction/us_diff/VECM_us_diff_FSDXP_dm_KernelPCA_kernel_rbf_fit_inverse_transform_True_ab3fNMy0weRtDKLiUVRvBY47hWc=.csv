# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FSDXP
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=9, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.029567164035259417
0.1424160253654561
0.06254973074522714
0.1290022362492277
-0.07131128161253072
-0.02649442423557113
-0.11387491959272328
-0.030435009882890468
-0.08075604487768276
-0.0673712335223932
-0.08522254959099511
-0.06509058135627752
-0.014942340392554931
0.012137700645465259
-0.0030368806822115455
-0.014917271729971614
0.028511373840878585
0.03835035525499722
0.032273977528642896
0.02075942549589231
0.010044503376036934
0.028486152071253636
0.025639360107655763
0.017468814803654677
-0.033157195150680156
-0.017826396383265185
-0.006624392873828863
0.005977466826923012
0.0015445872764635587
-0.02768445566960284
-0.08572250907669367
0.04730286963597587
-0.0038619194913099416
-0.008577319349712806
-0.017343226547521205
0.011390225983997346
-0.04717643305569832
-0.01611868550961379
0.021041529546656577
-0.07169816404060837
-0.006324690340221078
-0.02552161412704448
0.05605077336961098
0.059539256952661594
-0.024071852549966518
0.010777060587158011
0.0020929295925218912
-0.0626736006498321
-0.07445407186379069
-0.029576658304111315
0.0015616346837935176
0.037960379883177416
0.03959736289530422
0.009375393651550703
-0.03436871965771941
-0.0035837181803802087
-0.018903928031844767
-0.03201643890258246
-0.01510037567249282
-0.015975899150358304
-0.012765516414408772
-0.04530757684124892
0.010631289072706027
0.047331415391424186
-0.02606120498204645
0.0035716407654148036
-0.026580697346635568
-0.017034080230931295
-0.027818934988347484
-0.039280437501791374
-0.0592384538817301
-0.02541832634112057
0.05560219420295147
-0.031273060016242596
0.017226827868950415
0.0076857352141303575
-0.007276967070723606
0.023215734035655154
0.03765232512374113
0.01797481797647707
-0.0011287154350980598
-0.0018494520422258492
0.0267262379568909
0.025146538544660545
0.02516131111321431
-0.04696373986263117
-0.020942644129431927
0.006639253207174976
0.010734821477453057
0.008901896482669753
-0.00578152232043546
0.0038070886442216345
-0.03217262889167513
0.0177432857238078
0.005891041794245435
-0.032088556391835894
-0.009842012357382537
0.020923192976853706
-0.030876141156282018
0.004410053260780094
