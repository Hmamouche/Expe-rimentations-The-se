# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; EXRUK
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=15, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.2013859790919118
-0.1560923425449556
-0.09760955133441356
0.026575238956588688
-0.10595089337943556
-0.018932592145437663
0.0538645899907689
-0.02445824696078327
-0.0079824658160004
0.11398813544235814
0.1034063871037675
0.0193977876164265
0.06216653300271144
-0.1415833035199886
0.03908901468370264
-0.0032996407396860897
-0.05054527486420331
0.17589048364917054
0.046967142279100225
0.012603083491270706
-0.06495365859152298
-0.01618892807742626
-0.025035862459837816
-0.029298634843671834
-0.02714623952568357
-0.003113801841784867
0.018583830510571595
-0.03180674099592354
0.08691792356401737
0.05561704022720599
0.11142080490053115
0.06928297082781242
0.007885353735955634
0.03866096042309476
-0.10745647734337524
-0.02117917729517427
0.058144057020160456
-0.06021549424356473
0.03688963076329546
-0.003176686715656891
-0.06297457528078845
0.028557270329237626
-0.08184133769516289
0.0017415538578311896
0.06394055524716097
-0.07118084456277012
-0.030525503083302808
-0.009992122352913895
-0.020495622945967476
0.04873157736645853
-0.050488511621859244
0.07595228806454754
-0.005898381956362341
-0.017645589459065535
0.009862681094293007
0.012971885507448976
0.016029294863132537
-0.013468928934374727
-0.03175434535258811
0.025325005617315335
0.02483208351493562
0.05041997872673506
0.003912015462380264
0.011508092284733554
-0.017727560400313216
-0.015172096854544178
0.01888644772909271
-0.054068532465229316
-0.023447049166074877
-0.010967441703237605
0.014143053215968628
0.023559792122417437
-0.007925416251269433
-0.06700166955756055
0.04127021046097515
-0.009625006906164565
-0.009663586528762804
0.07817656364632226
0.09071985752428675
-0.07195063124590034
0.013109765089441134
0.07048175807962193
-0.008091456832081654
0.03926742824204363
0.00036568891715725235
0.011348676652632842
-0.03110349714390999
0.04706987113116634
0.019165656971870206
-0.044243340318827235
-0.01171036158736767
-0.022793271465682954
-0.045015642901123594
-0.037793540307902454
0.11766215331348427
0.047217267692603176
0.004168405149531516
-0.02502746145552907
0.0004240123269592822
0.03964193878157974
