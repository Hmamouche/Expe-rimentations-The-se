# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; PMCP
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=15, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.04245762831674184
0.33196441862974785
-0.14131348679643124
-0.06083615031914119
-0.06673016214732859
-0.09427890183282689
0.1932383376841006
0.11419006050011722
-0.10724867001049204
0.013782878573801005
0.1548078546096501
0.020898988203901694
0.06559449443733245
0.15658633459946059
0.011561252522427845
-0.07562057379891252
0.11242237822637119
-0.20752512810601184
-0.11502221578339185
0.20154092819724762
0.027402004050250103
0.06108642402387388
-0.053665980064552
-0.12845842966228044
-0.08085383877873192
-0.10744417112345789
0.039792927451191513
-0.15289222678161643
0.027983715435626996
0.13491907944436352
-0.07113158159218404
0.2693952467481229
0.06017064885436202
0.016486436809304006
-0.13122252599942882
0.15424580115829395
-0.2302070314751123
0.011995383509502022
0.02079674746148371
-0.014063530780925709
0.1257449137345239
0.08794397414557739
-0.0030509692518631004
0.12316265550695954
0.0697986614475612
-0.0353661589331785
0.04491696165563128
0.014616595561342927
-0.11441630690052662
-0.16683293187515774
-0.17425670387044354
0.24858390572627131
-0.11889817558333084
0.04469567818750986
0.08463640303261726
-0.09147049765421619
0.042701264273428484
-0.033995807300330805
-0.08155306962116424
-0.026153344190898535
-0.1142060901417633
-0.10818924986254377
0.12671500002460984
0.09639722979738495
0.1566446866190339
0.03471627854227849
-0.002428808371541233
-0.05482052831002386
-0.02047122383620553
-0.08407945533915767
-0.032130684796113806
0.08404368801594259
-0.010553619548730186
-0.052845913121249144
0.1909916903114474
0.06733191336118181
-0.11818391866642422
-0.02031206056213769
0.05845601689129164
0.18052993372156956
0.06538090030475095
0.1391013959162608
0.0801869382820942
-0.03718034416888229
-0.10461759381652366
-0.003206220518625902
-0.12726062612328398
0.015596263864098
-0.029176435970371405
0.061366119509497094
-0.045493049163595645
0.05312332627723133
-0.08447318960249857
-0.01922789467083791
0.0667460627752557
-0.059830261656634756
0.1341788193509402
-0.16874455218988496
0.05550365554720261
0.04493549436864369
