# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; PMNV
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=15, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.42249078118826744
0.458892778722662
-0.1856649560567651
-0.46363448933841866
-0.18619399187617797
-0.15206841389691
0.14993804188709833
0.15418318370680598
0.007319663574497148
-0.05387403524717019
0.3505992904513461
-0.01669287743122813
0.18963715939234826
-0.210695165741699
0.04462914959016767
0.002385841764869938
0.16724035775641555
0.1880990154671887
-0.15741431835957764
0.04263265607031168
-0.2241678578670298
0.2106286983872063
-0.055105977861172085
-0.10729045581972516
0.08053482878579951
-0.1999139366513238
-0.03474035035629862
0.1894630354252332
-0.04605192326387861
-0.034061020895413546
0.19438164874700192
-0.015162991891391111
-0.023368371052011658
0.13578600603750476
0.14188579750307576
-0.12130932939294611
-0.016392098480726314
0.03730744054039985
0.03858612541495704
0.03483649632538122
0.11037484772150227
-0.0781727812536722
0.08590724390235582
0.051689887845158905
0.027935354234027272
-0.09850223207642397
-0.1031414919251869
-0.05400733567866019
-0.11786526064839786
0.0360389327690514
0.014097155510411422
0.03643932495671512
0.15084383618556282
0.05947360374200665
0.07187293500985156
-0.047009934968124126
-0.025209319230089962
-0.06011369468242393
0.006650368186903018
0.015893711103591477
-0.04779224281799652
-0.07683417644403592
0.21016688190536065
-0.019387805105125416
0.14948889419029734
0.10905041373172454
-0.13177848967721056
0.008761215814050892
-0.052240200594599474
-0.0151081962761919
-0.03846863663669994
0.05260401388155635
-0.056844372771095775
0.0426315165389379
0.19387972436820997
0.07334627103184642
0.00732417894378172
-0.033614615482235044
0.03057972426741663
-0.014339391785218064
-0.05779549344997644
0.12355510072423331
0.08547700532591314
-0.08306346672901213
-0.03741000673555604
-0.16433344621296098
-0.00401346435818295
-0.0044569491110444375
0.09622762830630653
-0.0020173416402182384
0.07559981040256392
-0.07961791370158788
-0.12064736952127299
0.021696552499829394
-0.07345636219629662
-0.025370457710227376
0.02067433023052176
-0.08699027389644937
-0.15982144622130887
0.1506330320113136
