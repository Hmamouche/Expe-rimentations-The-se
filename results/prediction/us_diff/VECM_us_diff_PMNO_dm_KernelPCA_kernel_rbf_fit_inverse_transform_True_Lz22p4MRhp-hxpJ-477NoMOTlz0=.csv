# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; PMNO
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=11, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.3779629780170293
0.10094305496786
-0.12771440404309187
-0.36455652432840574
-0.06157015168563165
-0.03763787989817678
-0.01657769668322892
0.14732580263586662
-0.06213044264044014
0.11761597339909727
0.16977881068215864
0.03669562395964791
0.026357117509408303
-0.059826390007979666
-0.017416383428694546
0.021310217166417532
0.00038862682455243977
-0.20453182775329556
0.08934416193090808
-0.027902847815067376
0.074809329959387
0.2329073114698232
-0.20037421265622324
-0.04371952804335309
0.024681511292091084
-0.00019664615257184055
0.034580436671127146
-0.07483169430290623
-0.030953316150980046
0.08282988845883307
0.07235992870336916
0.041114788289383405
0.037295313386573936
-0.01684068756051839
0.11832552036104232
0.08751491947081388
-0.017163170084995426
0.023432089492624944
-0.1367783769955812
-0.12116822048658751
0.07589232566494689
0.032943983369185255
0.08215592178467807
-0.14109044259839268
0.05290228060180194
0.03659812980431916
-0.09980776218209454
0.10229759137514192
-0.011488895091584024
-0.041077973003166464
-0.07939590155595896
-0.04148340719548141
0.005313122252041435
0.06817252148586773
0.0491549303599661
0.03215829830756682
0.10251446368281823
-0.0834608006478968
0.041804997048519435
-0.044614649300062585
-0.0995562455976166
0.006462047282322615
0.009722264944129458
-0.05053987558025361
0.07980956495984096
0.06272341788237912
0.029088136243713366
-0.10390479857543328
-0.07603320884461291
-0.007072140666146809
0.010973758982925443
0.09244675661897606
0.02821165420150014
0.15410721969707908
0.16776973870024253
0.04599152024894014
0.0047812587571214316
-0.09741458180253636
-0.13668226719858248
0.15126660105443415
0.030040890261230663
-0.013381410677281172
-0.04778410772034335
-0.06143048158478388
-0.07800379083252909
-0.07175129683168688
-0.014858768392265378
-0.007130643994637274
0.0010061768039108035
-0.07238218630975439
0.1524846441288549
-0.1022464233289809
-0.1055905160994173
0.07152790079101826
-0.005566297262854816
-0.07173225889244858
0.031323143976571016
-0.02452437817540519
-0.15915600147389475
0.012265233447559188
