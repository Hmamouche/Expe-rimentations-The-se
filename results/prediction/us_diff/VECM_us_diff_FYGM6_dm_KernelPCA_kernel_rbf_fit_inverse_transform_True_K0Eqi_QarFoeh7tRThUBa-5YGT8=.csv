# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FYGM6
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=6, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.13550750734877176
-0.06635907905211486
0.11221837202732235
0.009687882397653634
-0.02736027569295219
-0.054967216067035904
0.06306016036890005
-0.05047600347723899
-0.055766719541388746
-0.054372908118694385
-0.03807136114421405
-0.023226011055528017
-0.08066679526264692
-0.004661244235987627
-0.02049833252749481
-0.0236192841080413
0.030923433049233364
-0.028809982402166704
0.05525866984758928
0.0036059121544203925
-0.021938600673676256
0.016072708651251155
0.052428257323756916
-0.01125845948859719
0.007339628341714009
0.018083586036847933
0.00995852083220196
0.008107101593510786
-0.08044639838923537
-0.005446589591386528
-0.05678215065382757
-0.05838063696194947
0.00616427265952893
0.012917100952103008
0.019746450475436716
-0.031235938797640552
-0.08514834177976549
-0.02756954543229723
-0.00784209464963252
-0.05197538615749881
0.03701765833436585
0.02438236018156837
0.01558943620985475
0.020235323225264973
0.031553693647524694
0.06746256881823713
0.02228159309258615
0.0020507434198561107
0.011682491457488427
0.006780101868496374
-0.03312243160355194
0.007342360838948694
0.004286660364978091
0.012709710855274194
-0.0043149629449268515
0.02376694837653826
0.0022367468835563275
0.005803537339966136
-0.0069339870108534474
-0.049960681832227224
-0.03368917167052233
-0.04947551494120655
0.038500058298213215
-0.043939971613161655
-0.0054948334842459725
0.04248525305161702
0.019732152569243655
-0.0089380384441764
-0.0233029064348257
0.00478696533845935
-0.026734959507026475
-0.0007531488189100573
-0.04468015018234776
-0.11870692151139572
0.037756042071428214
-0.0674292504520424
-0.015413761499839286
-0.036560680257628334
-0.025958149990806305
0.005920339049183524
0.014796524946467424
0.06501022997791858
0.04150222575648668
0.014995122649487996
-0.013583236868500558
-0.009324246433268879
0.02009736531153505
0.01087595916911247
0.045212018091490544
0.040280992840847754
0.05854725875457964
0.01601522241328071
0.01576277745480445
-0.0097919801847202
0.02864954052426122
0.026608207817968035
0.004184874882379099
-0.04899844945005568
-0.05481093686272371
-0.03059527645736836
