# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; LHU15
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=2, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.08551508313524758
-0.08658447128189464
-0.06999938316722017
-0.04121608105593177
-0.019015720721127057
-0.01701193388149532
-0.017498244180763487
-0.0030570096838412816
-0.007809972173156959
-0.020293935830161425
-0.01573638866700935
-0.0014320626941175829
0.009536726143196715
-0.012152910002763409
-0.016076007300838042
-0.022800330810156942
-0.056513183446324534
-0.028466896337946097
-0.0024114420677596895
-0.02752557366177889
-0.002643665217517136
-0.03538194327116508
-0.015857667646911957
0.008050027062111081
0.03176972516672108
0.015192353898299371
-0.014638884096429187
0.00694548688379772
0.047256472400663004
0.05656145105356118
0.07023086764212082
0.0546625145403959
-0.002575248946433564
0.06272651718401842
0.05935853733428823
0.057452448102524326
0.03325737658027212
0.011704696212322317
-0.04002156134498399
0.012471767396429186
-0.004054199265640768
-0.021320325611118886
-0.022124668399425444
-0.08208769880937641
-0.017151001456636294
-0.04548076988423662
-0.035270089932773425
0.02755449989567062
-0.014390624132151336
0.032640683801589264
-0.01744468549691817
0.007657088405625026
-0.03787065798427082
-0.024028977790276223
-0.01659238166861953
-0.008230382471055944
-0.030121437283281316
-0.036000107338213024
-0.013449880866475715
-0.030577479583575377
0.026550960063400193
-0.007128964714128054
-0.007723136922448736
-0.038019850113191186
-0.009483350811819783
-0.030088049890919833
-0.03014569372313898
0.005479274060545848
0.02077880912326201
0.01854274190602529
0.05458103314974478
0.029910070625306748
0.070041683579328
0.0969228566954454
0.05283018887067878
0.0243619257397801
0.016971359340752577
0.059335444623398494
0.022181505829825282
0.059966535391574365
0.007011035933221493
-0.033537407263712626
-0.04810258338354894
-0.03225378995258517
-0.01357316815072404
-0.007311491723562187
-0.011239957828406897
-0.031284636034533984
-0.04050227023963675
-0.012653863590937531
-0.021892296634471953
-0.027563523461834032
0.0034096990052679453
-0.006289773210403027
0.012140453432624435
-0.003785586616113484
0.04200549089393656
0.014429728806862092
0.03485687493269292
0.06671090621218577
