# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FYGT5
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=3, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.08626787863915263
-0.03199783465197279
0.07248160523790367
0.052052838972710874
0.015750175111500724
-0.04577231040487181
0.04621743419263309
-0.08611271723042604
-0.05376843114784615
-0.0489420508286955
-0.05430604591361278
-0.03559554822540334
-0.07716153523307731
-0.07541661321809863
-0.032046583930020994
0.0345568495853454
0.04809553434441905
0.026939120516523314
0.040511809644941596
0.006948433015176182
0.016932743288277995
0.018020979350709898
0.021817889944663868
-0.024094242948099764
-0.03100501257723454
-0.01854620376753768
-0.00483056412004062
-0.015312406026711769
-0.015051052608432126
-0.027183980998480566
-0.01946345632678743
0.012773860521983855
0.005984063463375168
-0.039707727312498484
-0.002044496914561003
-0.04094410943276519
-0.04158525280261965
0.0014172867436483792
-0.04037103503805601
-0.02180038710423865
-0.016557387175516212
-3.423023081220236e-05
0.023009362706290677
0.054417290371445395
0.014747053735607689
0.0831286028856481
0.01129794014408779
-0.01752786066237686
-0.01132625892995299
-0.04975178446242966
-0.02616876719115418
0.015809905363567876
0.0069627642232119894
0.020627043306623072
0.017758041385108
0.00786279054655964
0.0014249059929129987
-0.003760051704386052
-0.014449233420295176
-0.028086362720998417
-0.0353738399037566
-0.03887172545690283
0.021301842566247658
-0.013371834300552264
0.031489867912576866
0.027543386944481416
0.0464814395183863
-0.0017201837913726644
-0.006701129747895269
-0.038064388098894876
-0.044166867297561536
-0.020016106165002844
-0.03583718488638712
-0.026926224165343182
0.026454922878404133
-0.031235504281416377
0.00041102597160266006
-0.04168085036356895
-0.040926449834406034
-0.027711988768393047
0.021349380082531115
0.012676168267180482
-0.01272501579041295
0.03095797297622209
-0.026425304038250556
0.016219330325274145
0.012972163014554738
-0.007613780861196218
0.02234597605446684
0.00909383291823019
0.03183535382787404
0.006755248144670598
0.004010713658021239
-0.015967664075230446
0.029878325839618757
-0.0066663376353658455
-0.015888820758554106
-0.021972317796677167
-0.06337878718998256
0.0037376330474944213
