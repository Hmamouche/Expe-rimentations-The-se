# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FYGM3
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=13, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.030423875960349754
0.03385327711095806
0.08900593848814295
-0.044931547261297014
-0.04071262584135189
-0.13323421832955773
0.0163534872016389
-0.05389793276177676
0.1093727833629178
0.13098427575984045
-0.09557081585562618
-0.11038492297577958
-0.07096841924400873
-0.014098768708731321
0.009594633634688154
0.05565689666668483
-0.02007433291239339
-0.11669487990873378
0.1334530982928214
0.0037109139530212693
-0.0060988916308410066
0.06779570391015446
0.054762813627265636
-0.08644854280318573
-0.03532233042484108
0.07177754905710103
0.039344114095650584
-0.03744257743467269
-0.015205647244292879
0.0694384289204024
-0.1863907542904185
-0.09725805254932164
0.07000945196750065
-0.0372940776013412
0.017580371042272995
-0.01782239181207307
-0.10359795701493385
-0.05857325453356319
0.024271967547501896
-0.033656621645276524
0.09236795196632369
-0.0010737549380497044
-0.017598724891404616
0.023921688872973614
0.017336555778801985
0.0764666508063938
0.004998576718239958
0.04811032790999773
0.0003174292770977364
-0.00656451961431841
-0.037791019808186777
-0.014664977479934964
0.01949231800023475
0.014066625646344175
0.010072892804091193
0.0036080274227491974
0.025698053453808288
0.0092085325720238
-0.03115191751340435
-0.01273916626290588
-0.01689558028790707
-0.05363378680433485
0.046520234472070834
-0.03411548888783669
0.022765152042158197
0.07043129708372803
-0.005790301023548241
-0.03749819053630787
-0.03182409163950736
0.01578801215729319
-0.045042301532680656
-0.017214345505902576
-0.08200605118844438
-0.12056398770924279
0.05982856706737305
-0.002864117334372351
-0.0026855167702464003
-0.039037490916285
-0.052054991083129276
0.033282499949294525
0.033011767139365183
0.037387529578448146
-0.010487224081763124
-0.02074611719711878
0.0018532532581908429
0.023937410169519443
0.01910799524643162
-0.020843105243503753
0.014506288396551409
0.02892587687766785
0.030402458841218873
0.04133431404007274
0.025988655452531907
0.00048422487198781475
0.01905708415750199
0.03577137739702102
-0.008220150612043793
-0.055879090069725364
-0.07617875553946304
-0.029438250606043576
