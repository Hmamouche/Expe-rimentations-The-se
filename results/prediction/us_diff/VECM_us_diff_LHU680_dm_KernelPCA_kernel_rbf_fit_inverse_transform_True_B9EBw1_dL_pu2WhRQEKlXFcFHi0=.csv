# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; LHU680
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=10, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.050753736563541846
-0.1186328338959932
-0.08822491089217901
-0.05954282111753598
-0.031218898155881074
0.0026974352113719284
-0.0079491035643732
-0.011358072447404735
0.04113654433661856
0.02020380629219508
-0.020334659730709356
-0.006182329654538006
0.022810272635410172
-0.04058787400608804
0.01747275047453896
-0.0004026888799278563
-0.05241616061235944
-0.06187954409086442
-0.00910624142083727
-0.03264912924477397
-0.007124480007879556
-0.01800354602720728
-0.025356938244821978
-0.052526291217238986
0.014389838082863187
-0.008501899111055665
-0.0031934479062152334
0.012030880529417996
0.064721391683652
0.03420526631015675
0.07349651915557333
0.06733931073567358
0.021780774820638123
0.009351796105281635
0.059964891801957645
0.03126424186146835
0.033767255841297264
0.04878947858929595
-0.023297486780499345
0.0005949291060059593
-0.017892840345549818
0.007394402318750056
0.01746114540713431
-0.00219694216330487
0.014849525085619307
-0.015881067455590064
-0.09839719000949591
0.03673127021344488
-0.04391935972748127
0.021337841331251113
0.014974762315483158
0.0349740458183387
-0.028047677247346033
-0.026293784858777352
-0.02940302505881573
-0.04624564540243108
0.008500909307958682
-0.046555888396089894
0.0031432721661423524
-0.033389080818513155
0.010021302083092466
0.0043468039872326635
-0.027909096130419447
0.001490721351612278
-0.06886660895748402
-0.012884108716114454
-0.05780436616252034
-0.04045829988068799
0.019310866687528024
-0.0248066163400965
0.08142315159914398
0.05084000471135327
0.10987340278055638
0.09719926007526682
0.03977093702138225
0.03685549304940443
-0.04008369721075922
0.047651074123920716
0.008716136438360444
0.07967560924290451
0.08043325206841645
0.016114029318156378
0.02439107475211527
-0.025642298707972085
-0.05505981261472569
0.011193694255780378
0.0044222080678119335
-0.028356167871252023
-0.0356983561186264
-0.029390662413792245
-0.04778961832503614
-0.017855449621640748
-0.0002794955578543975
-0.0313938548559381
-0.00941726588452174
0.0220017244462184
0.04488124929908647
0.0319554379098142
0.007342030027741955
0.02639578363425942
