# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; LHUR
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=1, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.09892623099714414
-0.06217824778880742
-0.06046492724144978
-0.03835260519743126
0.0035700518324342234
-0.015751312119430816
-0.00649558572343675
0.007599423521560445
-0.01249861847719495
-0.029182012177756823
-0.012195299697469544
0.0005272404432149018
-0.007625313330211156
-0.0008844641325418128
-0.028765662879285128
-0.032530014054685794
-0.05531137577019785
-0.02984544098866039
-0.005155321822854023
-0.032988805737647495
-0.0027990053102701427
-0.027716481204014133
-0.014557159574862085
0.021402160907963087
0.030139367930060695
0.023006773444019163
-0.01179188317849843
-0.00494542666794616
0.027512823696211983
0.0647621440396465
0.07127011500678615
0.01906784566552573
-0.0032561758079288766
0.020584639682923137
0.009524086190186878
0.03131030705209994
0.019341173587617153
-0.011313818408194946
0.0004149196234024058
0.0121554320340116
-0.028304314273145295
-0.03623669280909894
-0.03555248221860434
-0.06414679105661396
-0.03710417456225382
-0.0486682149477919
-0.001297030653066105
0.028294561840039707
0.009321262288363018
0.011318250509948754
-0.006699794381708759
-0.02170847287905963
-0.023492301555191403
-0.024279484167021873
-0.01990003412078095
-0.018422699382413067
-0.02958603672425053
-0.017802582492435468
-0.002811111320083233
-0.012725616849894868
0.01761522170298971
-0.0024666174455965388
-0.019883130952116978
-0.025639703096714227
-0.018623365430891575
-0.0355748903446572
-0.012589512570399431
0.003587910380967573
0.02144067493981628
0.020984614854796687
0.055804485868638586
0.024431388817554467
0.04380570533566369
0.05732320986287564
-0.004970538418707948
0.009131111698171069
0.008113119754775543
0.03379915856486303
0.03155272631315372
0.02732299777437576
-0.023083994492656974
-0.04637091045139854
-0.03770252634361466
-0.017514660303693162
-0.006675327556849646
-0.001657504387887981
-0.01016135163419294
0.0013702596450035102
-0.026850641637451513
-0.012108472421206894
-0.027534068640333302
-0.009914536446939336
0.0047103497904844365
0.010778579893731143
0.005077214948482725
-0.007108456232329633
0.02187827425433947
0.009364374765767094
0.038105532434861186
0.0472448623918945
