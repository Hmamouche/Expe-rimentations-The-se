# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; GDP272A
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=6, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.0065037516988195945
0.005258320729240481
0.008197756863798145
0.007217023272894525
0.005918687054157205
0.0033276876865391573
0.0077294814095219926
0.00414723737518656
0.003518770895472571
0.0036712909389328714
0.0027069884868459217
0.003795029383140195
0.004635624759409202
0.004448111405096911
0.005384729008198375
0.004017734630837114
0.006316606102193902
0.005908059224861964
0.005510202707922759
0.006641314896416225
0.0076246425752732406
0.007062039942545092
0.007892464976357236
0.007584007095690239
0.006072112304009289
0.005603110361461559
0.007121644154584989
0.008227217178401379
0.007994174835465143
0.007202399273300743
0.008184509793568159
0.0052940729328670045
0.007571902342328794
0.005226669357150202
0.0053226036879615145
0.005042612881634204
0.0035701410836220834
0.004725158093581741
0.005152572013017601
0.0039821099522857565
0.004315103708253623
0.004347468589426293
0.004980095996370242
0.0053814394581482
0.006174428933544602
0.005029221647601652
0.004558582492941155
0.004601094376751679
0.0032818236085754033
0.004266385637864826
0.003960308028243397
0.004066316077956681
0.004598604483750083
0.004988594083093666
0.004197929349561249
0.004101859702418884
0.0035301687771119986
0.003696106602130568
0.0019961708033412887
0.0019281804103246142
0.002677198468307658
0.0024943167867408755
0.002732898073114107
0.004712088839844296
0.0037501862054856033
0.005230407130418445
0.006255190207749516
0.004821564093057789
0.005471349581798837
0.0055180279700865606
0.005089153038695946
0.006457810878447268
0.005020078031538712
0.0060320549556514575
0.005286564063359058
0.0031477415421433405
0.0036961396143190667
0.004877774836310988
0.006583189697223479
0.0030687127548719544
0.006253299274554778
0.006175311542502379
0.007029894257334422
0.00977187866801298
0.007758537082981839
0.008952193287440338
0.009172553038076538
0.00626727035355649
0.009873447977747918
0.009925291113386036
0.009007016964847025
0.010738238300076369
0.0078030565778771065
0.007756931180563644
0.008739912815220421
0.00717742984587756
0.007163457989879424
0.008317393626440293
0.005303953567515996
0.004480307950422456
