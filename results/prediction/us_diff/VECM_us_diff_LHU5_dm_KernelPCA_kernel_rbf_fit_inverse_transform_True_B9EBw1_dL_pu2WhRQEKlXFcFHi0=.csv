# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; LHU5
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=10, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.037250071731981714
0.04928253932564273
-0.04731609227121297
0.05113886646949365
0.06742754823519845
-0.10731016242295605
0.06773018757363192
-0.03526024878752269
0.008931257872666118
-0.027973880037036446
-0.03533969278641204
-0.011353506801035395
-0.013612308734630483
0.05822802263769978
-0.0471972621128651
-0.037699820769512525
0.007803643329600523
0.012252670806694989
-0.0522518474041786
-0.024964288085139057
-0.059755990044077764
-0.06111143148913954
0.02525070814885924
0.04497792808072983
0.0070078643355783125
-0.014681017176320102
0.012924744608172455
0.0334592954146517
-0.027802718926079426
0.06876075131964374
0.06593380636782636
-0.024952999743066855
-0.05041422248223529
-0.05345012553264723
0.017144109380969108
0.03843526733312484
-0.0037611314252931835
-0.009144461970033543
0.03703549353247033
-0.015329389980248867
-0.00955827957889214
-0.008457239889871467
-0.09559925138930682
0.0314618549049992
-0.09985005695929197
-0.038039841990529566
0.0028931238641924373
-0.0697710623457366
0.04315545777202947
-0.035594723711438084
0.021298179367784872
-0.04145636965686665
-0.034256064339513156
-0.03932845763287515
-0.02871809126304141
0.05056125288199994
-0.03892685899013959
0.016620234184822478
-0.01310107436003681
0.019213476425411966
0.028097937426314966
-0.001025634139176005
-0.025682005457670867
0.013205970381891467
-0.04222663532057597
0.019022070118732227
0.017477685045853272
0.05910833467644203
0.00027463342528295104
0.011445783202748444
0.0012709652141068267
-0.012567936452555592
0.020697604589213478
-0.004616731013699617
-0.06553017462525809
0.036389593506004944
0.028358941530288555
0.020707664872716636
0.05494838931845007
-0.03567509908641474
-0.047022673954628405
-0.03554938109081954
-0.05905809575918937
0.09634288158651917
-0.001366539903038734
0.013159938761324024
0.0005396819957008444
0.019363812669521335
-0.04312581073803759
0.04621302696993012
-0.055607769976758856
0.030727719192159268
-0.01756840288974118
0.00958024947548741
-0.052125196537133225
-0.005708426517659842
-0.01575965687726433
0.02868777077990748
0.0473101356080506
0.03499458304037671
