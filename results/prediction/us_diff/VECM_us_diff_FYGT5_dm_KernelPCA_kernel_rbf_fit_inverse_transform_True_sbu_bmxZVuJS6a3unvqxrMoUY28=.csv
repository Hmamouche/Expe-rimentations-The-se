# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FYGT5
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=1, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.017999506414166255
-0.031620183673571264
0.010704722803472047
0.04942506392514413
-0.03127544920852061
-0.0018939792174070417
0.02080348160241693
-0.058963101274289983
-0.034269319828909336
-0.029102948834964112
-0.05822581028847382
-0.04571006250113517
-0.06496698721030539
-0.055109069426286884
-0.02168946636411906
0.02945570865009991
0.022281137118249975
0.027895789465822635
0.005137493841164877
0.02459600293067309
-0.012661917266541742
0.010817885470584565
0.02866756403229358
-0.013242455848726029
-0.02026339599405495
-0.00754676380191674
-0.00032798553704191293
-0.000843908178857902
0.003004373035838226
-0.008599590957268888
-0.019140891027713332
0.006683996709306348
-0.01086273148280344
-0.020890845744495526
0.003467620228552999
-0.04843389884511907
-0.04732721930741004
0.015322644960620904
-0.046279341911183436
-0.022139876610388288
-0.005835320136269841
-0.020323745606335614
0.012233346030458605
0.04277772817135212
0.010921383496178345
0.06797800986433312
0.006666619147551328
-0.01906178424551684
-0.009848277340427859
-0.04595640810411794
-0.005019068875777597
0.025306527769835132
-0.007009585864696583
0.017779806066925013
0.009074048371187911
-0.005770270517391006
-0.018429713989299747
-0.002914486423164518
-0.022537974145756787
-0.017368413245363947
-0.03182916440204463
-0.023901524938610082
0.01852893510328845
-0.003288687328925819
0.01754265167206528
0.026277266559894423
0.025043319849269764
-0.0025734463698031352
-0.006595062873351639
-0.019524449363061462
-0.03216434952939735
-0.002166222301948558
-0.03019722461576926
-0.007929304494341618
0.028687963301824614
-0.02450459906879786
-0.010741309018205131
-0.022526501550245548
-0.04656595269047336
-0.02641364121993749
0.024997886304821512
-0.001960545815355968
-0.0033554024252052212
0.030676057411447324
-0.03856275163807805
0.0039734866901719994
0.018464964505351483
-0.006061035520728649
0.02384996052368972
0.012076958056275355
0.017080790810878998
0.017799992730332878
-0.0049515111621961025
0.00019818929356560855
0.01143490986154783
0.003597072643521623
-0.009700920734285922
-0.01305336701266483
-0.05166855970703096
0.0024369650160602756
