# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; PMP
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=11, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.34211535683299116
0.05011341475150985
0.04759341773733635
-0.4316613442926355
-0.14392218890074535
-0.11872954246542344
0.007753123197045866
0.25320533020873925
0.058759161265100626
0.03915273780912837
0.2238806554946727
0.020929078756455215
-0.013031554701695737
-0.007309774735486982
0.039948123996926224
0.017990002754185396
-0.06606148298513231
-0.09547345033274182
0.12903073061026518
-0.012040013138721904
0.0993441990558685
0.12190302160087291
-0.18805912432328226
-0.09841425010618735
0.0029224425218988834
-0.005143916463619922
0.04186139761214165
-0.017715092725087936
0.02080463204772459
0.09452795567307112
0.04058964212177024
0.045503847571154506
0.0039043743373871967
0.0006892481758153102
0.1374575238702856
0.06960476744136633
-0.0377782854714198
0.018589910169586527
-0.1405671395434876
-0.14198657282596408
-0.004726079815389427
0.06215810669994526
0.07838091345147127
-0.07171394682394112
0.0613928639553114
-0.07470937019427179
-0.1750352897963074
0.12265657826117239
-0.003524580069833347
-0.010765941076911624
-0.06053100388733124
0.012366833038357006
-0.04048376680104396
0.06157423243391422
0.06903145224864721
0.048782550556958015
0.06085801535541787
-0.03447567958682134
0.013569673475777458
-0.07674333451070461
-0.046466425535766215
-0.07851611963927378
0.07689591018739805
-0.007016419541371048
0.12126563736251032
0.09690035894708696
0.0014785787865861984
-0.1211072862373381
-0.02980204635077373
-0.09944621773336706
0.0020186127199716165
0.10892352108846343
-0.024818344104453827
0.06807788598931618
0.1769895011407132
0.07316460383423398
-0.09298021574541515
0.04338642213920392
-0.1416698960116862
0.08682950693205278
0.035584258551068566
-0.042650242911131835
-0.001437501582844461
-0.030328175074508218
-0.023569815126050567
0.007279642993382282
-0.1280171159713259
-0.05281032334897926
0.005895510968181059
-0.0890441434149992
0.16143681278618752
-0.04942171236000534
-0.0596956970489562
0.04101890223677969
0.0008396709188365752
0.00782420935234373
0.06115978501543147
-0.003713130711287413
-0.13303448804313886
0.05596530216101291
