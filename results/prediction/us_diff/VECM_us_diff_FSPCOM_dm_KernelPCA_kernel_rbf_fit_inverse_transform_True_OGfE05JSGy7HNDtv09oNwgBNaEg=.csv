# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FSPCOM
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=7, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.00530105060552793
-0.005263047845026099
-0.0019424371470550188
-0.007008712047504001
0.003329958205983432
0.0028906824271444733
0.007688482296100075
0.007681101845469664
0.006643152859959282
0.004797771091225823
0.012010730715970749
0.009777622431115079
0.005312841023419306
0.006193595560970483
0.016606619379129734
-0.00077351641317732
0.005056462843922958
0.01602090834508934
-0.006491962400439643
0.03819843753691812
-0.00617848597621173
-0.02605317964430462
0.0032127788157782604
0.008571651335511875
0.014101468407251922
0.012070954670289787
0.0070581988926181885
0.010028852215468571
0.001400762497503661
0.0007254110599393378
0.013299360921585903
-0.006499627119365529
0.005961228164232993
0.003080166511396016
0.012418961901875237
-3.414069321082142e-05
0.0035542050408517377
0.012658983202116912
0.006869505887510154
0.015721089143743084
0.007874729070860599
0.0048404972071154585
-0.007650957966004543
-0.009040499008201081
-0.001181468693671776
-0.0008899594071245984
0.0007569881942287564
0.01820198967166723
0.0257190581537336
0.022417525559004246
0.02384771765696826
0.01635972019140926
0.009718809143037561
0.020195598552314863
0.026472692023287114
0.025537806156053767
0.04256001864191209
0.04271044628542895
0.04970136506668002
0.03557863908287696
0.05639658230530168
0.037583237775316
0.00860693009094059
0.03542892709530274
0.0435865242259278
0.0409182654373858
0.043810029081032396
0.03170183072946755
0.030506070831150523
-0.005081676444407706
0.002202013496711449
-0.03690490201629127
-0.0562527925102513
-0.02288433526253894
-0.039845726644948155
-0.026235825746349476
-0.06368022243430935
-0.012893663107204982
-0.06892475251219793
0.013612673386451588
-0.03867860510371879
0.034326938036564426
0.0309202098267597
0.015034899050493083
0.01899948978414467
0.026442271660731997
0.03020643522641087
0.014794451921575014
0.013233557649491282
0.008308645656715612
0.03571023376105773
0.00038443755251494315
0.011036061373798551
0.045578123116602795
0.017323672945759205
0.047258325940591255
0.006410831771879677
0.03275888586670521
-0.035223881697425886
-0.0053541150945305045
