# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; LHU27
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=14, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.0558843776466198
-0.17101812843042685
-0.0472847047066484
0.07412586812480526
-0.019686524292276773
-0.013378037396358455
-0.07264781655538287
-0.00988398335250347
0.04880414967524397
-0.013042615693547822
-0.028179440811589355
-0.0050350225238600214
0.02118882307422661
-0.03389466381467035
-0.027491214163097234
-0.07681118694038457
-0.0999442032164234
-0.010015408788663494
0.0033724287223788167
0.02023311146213818
0.010868388638307185
-0.048454037138322685
-0.002990940432640192
-0.0012729860836132537
0.004616611453857122
-0.007715341178652564
0.0003540638668669896
-0.0007400249532432679
0.06241724885468581
0.049176948645189345
0.0872810526169897
0.045696189020228505
-0.0025510714897555888
0.00945906286682318
0.05721974498483559
0.08512525685678429
0.08994190949745666
-0.011611725291955727
-0.030062929800530356
0.012155287984861103
-0.04303971371750322
0.026939200072697636
-0.01853502218139199
-0.04456056783584636
-0.02960327918929441
-0.020933427724153984
-0.08288031922570589
-0.006400508073415349
-0.03804993126094411
0.014972785040643725
0.018677905776176063
0.04154893181685551
-0.035359434166844825
-0.05387501903224207
-0.05020976937417285
-0.008401354077924163
-0.0012434845428999156
-0.020543903383498003
-0.0265082740188503
-0.03320542433114395
0.03210337611833195
-0.00124937999010386
-0.015727273152244325
-0.03122221913770846
-0.039255803725072216
-0.033425971452260575
0.0014874123040962382
-0.013517087734270027
0.028795214524399024
0.02304005093253129
0.03387860059366111
0.00793636897733868
0.09282754953896188
0.11052768281890542
0.07012434509466625
0.05455601147398302
-0.02050365945645722
-0.019395441419379184
0.0437957542368488
0.08982606231001637
0.00930560803938671
-0.04849543706882558
-0.0073456486975637895
-0.013805720627451412
-0.04778351673253302
0.0012039865588146016
0.002723247982494282
-0.042583891792433666
-0.03535748433939622
-0.01436566746369169
-0.03215103518972654
-0.03156569384766329
0.029284303042041448
-0.04928603569287511
-0.017172841669555253
-0.009942725061043989
0.019459833625223605
0.04278032466388687
0.035376264290059904
0.015480072646913888
