# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; HSWST
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=15, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.018686063905898304
0.2641735465678312
-0.42415331774099113
0.1152311561683099
0.014148500199247488
0.08568189704186789
-0.06263840001791914
0.011635174836913204
0.15152251386210555
-0.05890575922428591
0.04173754092407813
0.1288561192033842
0.06847883649098596
0.00515294407902446
-0.13132714331532783
-0.06765658958680162
-0.035066063845474456
-0.11448879770152456
-0.09387611338301881
-0.15586951156178439
-0.015554734971629763
0.07084260441432137
-0.05515646181288568
-0.011166124302821379
-0.0252414728548368
0.09352627250959594
0.04499563191424624
-0.06787588651034816
-0.04865700308679437
-0.08104520917740182
-0.17975708810734356
0.04499382247943397
0.024026977961181528
0.04821339485052511
-0.03487290083206657
0.000673859141238721
-0.09193290632132575
0.07085961894345813
0.03656383734948803
-0.11894619908761496
0.07810565074490708
0.01822829467100405
0.028663705884224247
-0.03813234886722095
0.016262993462962286
-0.07272115423976594
0.08324505973366554
-0.07421380762610028
0.128388660394609
0.03363688009966816
-0.07730544034291646
0.024006092762788325
0.04939397237583131
-0.03643732991152575
0.058622389956008575
-0.040854074628793285
-0.027928968984823548
0.00022997753854693548
0.06978786760359397
-0.017165234908656395
0.03528391952468709
0.016836515913678907
-0.02330254714754207
-0.01949885464648273
0.0535026626778065
-0.034283752896692866
0.007999510830976813
-0.01906954894752088
-0.10647081857315148
0.07838619149621213
0.11774674306413192
0.024265187528684774
-0.002665257063260696
-0.06232072280017103
0.10860092980447086
-0.06414875504901599
-0.05544749273667962
-0.10528854565162304
0.07640455537469944
0.0803793691560063
0.159674968397222
-0.050532924723114686
0.0543071432216616
0.02139943385281671
0.0029638076296423996
-0.038228047974810046
-0.02849100277105678
0.07668243681496753
-0.06554165307192794
-0.005476018962909803
-0.029669543151292367
-0.04712088411985571
-0.052531215054986216
-0.026447785645166012
-0.03015460846767227
-0.10024764656343835
0.06811270201224985
-0.1221743649425327
-0.04804246810483508
-0.20684561928214268
