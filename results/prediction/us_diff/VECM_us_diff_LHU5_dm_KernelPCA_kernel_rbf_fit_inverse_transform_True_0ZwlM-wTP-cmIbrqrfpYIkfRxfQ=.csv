# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; LHU5
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=13, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.1154038629869611
0.17096015642689655
-0.10541515117509537
0.07118082250732022
0.11165610722315472
-0.12806037960681313
0.0923471949636443
-0.05558036392403042
-0.09824199565123569
-0.053849224518892985
-0.08279188558284986
0.01014327948190625
0.016847730725947686
0.08744262446061854
-0.029093676205464714
-0.016135736498274437
0.04808663722421097
-0.017592626588554167
-0.0775715382763609
0.0039637662178180315
-0.09301833743260234
-0.06992767625845739
0.023905153678141174
0.06856341855020341
-0.013593488523332956
-0.011029509182469043
0.01152352537651806
0.05884330141002063
-0.053818958875052905
0.06549906676034367
0.0911654498747845
-0.05964971506498708
-0.030909365360956505
-0.06995006317689366
0.030283009464146405
0.03786124920576288
0.002813634544497525
-0.0020203699389233724
0.026121920234390063
-0.051773206643940306
-0.01612612278235017
-0.009149979765115186
-0.06738179005741202
0.06845750665944841
-0.09078505884217915
-0.03286964321964179
-0.014761370184048155
-0.06801568781701652
0.028842294214851794
-0.06658339380585734
0.02619624472466033
-0.026852693185940697
-0.0030321638723779035
-0.022002269957688874
-0.028233031074918015
0.04123954479775272
-0.04245109477093688
0.014920788607856443
-0.007628501573372659
0.012956020885997859
0.01438793927013319
0.01746683084592172
-0.016520447303434126
0.027806503287761485
-0.048209092943454566
0.02842274289281021
0.014522233098574026
0.030549903225355814
-0.0009652378380652065
-0.0036278967444432165
0.002329597127351785
-0.024476530854884694
0.010842334890912705
-0.009580668569520473
-0.07759326819767784
0.04145095096177776
0.04247301898690106
0.02883447634305745
0.02457884377824334
-0.031483670849377976
-0.03866425527957812
-0.030457936240053288
-0.04780468948801794
0.10236266548178366
-0.01414299089114348
0.010648746185573045
0.009581730374987702
0.004313968300481523
-0.038577022664873235
0.05622446289568535
-0.021427709278111464
0.028638522744979508
-0.011823911718294873
0.025121800894602805
-0.05069362286216571
-0.014337439752633251
-0.005306823730152933
0.047422275972898546
0.035843307560086614
0.038141708196899764
