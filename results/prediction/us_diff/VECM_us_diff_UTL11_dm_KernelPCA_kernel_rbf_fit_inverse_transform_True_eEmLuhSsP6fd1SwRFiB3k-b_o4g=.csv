# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; UTL11
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=4, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.14133631997664903
0.01913582869546132
0.06244793353989899
0.026645803310011217
-0.05503510499133007
0.001865385401902911
0.03663802596386803
-0.0008064211656472024
-0.026122290548313623
-0.006109378415389761
0.02999263823810651
0.011869261853017005
-0.047213705248445384
-0.05136934505676999
0.041686617748938726
0.03749822032597666
0.05885067150561819
0.04076958620337346
0.01355769760232325
0.043909019295865555
0.028133903207920377
0.07390474808260686
-0.008595909249705326
-0.04774748751623101
-0.054571768003530724
-0.015068103196407525
0.0013953505195176828
-0.021822238202801648
-0.07154665759144983
-0.10600401824424928
-0.055280777093926786
0.03676634305714653
0.07925410850452615
-0.007146645349463156
0.007798776756116791
0.013912188321020624
-0.022966901851311256
0.00853275716427233
0.009674701914676337
-0.0286269861305539
0.013239382115295888
0.06067758210774287
0.06934152945505605
0.0391070164506965
0.02480901720490758
0.05038133922995726
-0.03362978062134594
-0.033664078442809386
-0.0036381389972918966
-0.015111156098022115
-0.009911820908948021
0.018987927255297257
0.004654769402384568
0.040847378637421716
0.002752017048702607
0.02429051173174098
0.055601290279875225
0.01275445161149312
0.001606951639015937
-0.03703750592098949
-0.05074633516015752
-0.02813215791865767
0.004161242412700255
-0.01861210994118207
0.006149341274853478
0.03921522703334323
-0.0057918040829294404
-0.036825929167922315
-0.04963103260473341
-0.06326464473933895
-0.07602182976152051
-0.04572602885620478
-0.06525326504792243
-0.0990486149346929
0.014562714102201398
-0.0011018058984677871
-0.021063982814129262
-0.040829374377928926
-0.03339600569506948
0.013388001370658618
0.0802738497307569
0.10132608914470076
0.06819284659917964
-0.004280719397681976
-0.007113357014868975
0.004334150983685852
0.005900250124655893
-0.0027732850010087507
0.032421473990957364
0.030920450644582053
0.04070066776141271
-0.020003233374420427
-0.03638336877975578
-0.010375333316749233
0.013147844587653697
0.02504788233483308
-0.005176356667100824
-0.045165017017140194
-0.05900972635934356
-0.0340504320835663
