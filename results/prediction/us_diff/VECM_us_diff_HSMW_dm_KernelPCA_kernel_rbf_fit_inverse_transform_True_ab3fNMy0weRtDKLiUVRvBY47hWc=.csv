# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; HSMW
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=9, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.2150030142104885
0.09716726962454181
-0.1839228427001609
-0.12051974704034009
-0.11539574755917748
0.1582426130200728
0.016122201153690143
0.00753800896688887
0.020962926044195113
-0.046171966300223466
0.008431308682384467
0.05721503039179976
-0.02193286203571523
0.04942900949190933
-0.1532072233708133
0.04620448132124396
0.08352523722968064
-0.08010919177613231
0.0571070569227248
-0.1490802414758911
-0.014627324782334798
0.06785353393841521
-0.051156973485542614
0.07151440704801901
-0.08876783387395333
0.11172311037094221
0.0819157668123075
-0.11679208614785824
-0.06056129076075114
-0.00011031262905374095
-0.1441157638518181
0.15652964169253172
0.07100740550708501
0.04922750862580951
-0.033954279543191586
0.08844661548830414
-0.04514650659608263
-0.007891685584881587
0.08687653279261062
-0.09780411866869607
0.08061137086800724
0.005078880445588095
0.05473475238728171
-0.1514288633522922
0.1542199298056158
-0.16386992380309973
0.05726964551573197
-0.11843511262694996
0.016237255110995674
0.097142788831431
0.019110823699008763
0.07384654813637306
-0.019618541384046485
-0.055396818777724885
0.06436039737651625
-0.04890122524298671
0.03716405234939629
0.0016544768238052517
-0.034863995896423666
0.040556109157365516
0.030567544774091477
0.018631816878353295
0.07013237725699759
-0.09548910286086873
-0.02222498212806396
0.009479138902567671
-0.042394145824616
0.04254303314654175
-0.05717690835324166
0.12203676937937735
-0.05540302095662578
0.005344940428148136
-0.015011647031165396
-0.041705712329103584
0.20475002189391575
0.031166365335280145
-0.13384968790556678
0.028882401864395415
0.021106635410574164
-0.04956459368666674
-0.007605690120646838
0.10805755755006655
0.0880278626442389
-0.05517256327031249
-0.09985888165241738
-0.09611166991831432
-0.003422806699024313
0.05872622869656195
-0.021431151305143066
0.019163513207274295
-0.036262806533621676
-0.09722976545454244
-0.08579221502091364
-0.03396896876808084
-0.0038638932311726853
-0.053335444180135
-0.06590604961750812
-0.1599615438332019
0.09527610960800298
-0.05482059971222499
