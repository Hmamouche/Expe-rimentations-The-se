# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; PW561R
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=8, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.01333757995100223
-0.08651085183104187
-0.04155338436813276
0.014086059616384771
-0.022574426424503226
-0.02677772502918132
-0.0057829083740491005
0.0026522864532262744
-0.018357284404442085
0.005533371034793212
-0.05439894881585534
-0.12389654205280022
0.01200013512120519
-0.04049609780549464
-0.028365425702404058
-0.0174227561711654
-0.008972540111454127
0.0037725545959121885
-0.024615844247008696
0.026329921892238765
0.006938573579504647
-0.047789808525071246
0.033001099281013954
0.009638552460377352
-0.027980245771819713
0.01585557714775434
0.023039595011980364
-0.06705411036858003
0.014815131558143551
0.05737287663355472
-0.032005972562929315
0.07922081535816565
-0.002357122143026519
-0.004051688112801058
-0.08464468349737908
0.06014745904503846
-0.019522452049683284
-0.003230911363400469
-0.014908762296766302
-0.041455912179017035
-0.03737815579334594
0.004759687689416115
-0.007552285280946676
0.025314779181400286
-0.01896652527771862
-0.04097353567636761
-0.0012107806013270592
0.0012804386277508132
-0.03780947179201792
0.015302264944805164
0.028071150157275782
0.02859989215750707
0.03339624108108037
0.02751992501538515
-0.0345332900707739
-0.006225284390643767
-0.002644517178892627
-0.01712757864980847
-0.029563800743929423
0.014228996731207437
-0.018292726911211577
-0.012953070753995446
0.022034862592445248
0.03292477557129189
-0.00882623495696429
0.012848793569144121
0.0020066218809649157
-0.023101366129833094
0.02963978838627504
-0.018443264589462396
-0.027746465433147908
0.021265887498451774
0.01145528236000677
-0.04796862622070371
0.033145149465610924
0.02197095599435376
-0.0025709730360793746
-0.004715286291501892
0.0016209220783427389
-0.02492146645051094
0.04624400425004968
-0.018229507009931336
0.004353983784950159
0.010851064470152872
0.0180789832722347
-0.01058615503613243
0.002738855190976556
0.04019026204177675
0.051558521886462295
-0.006997712161225645
0.06538125716412439
0.026040544559576183
-0.007592885601120925
0.011009941827173048
0.010779687624533595
-0.040177804874068045
0.0298891436820493
0.0331100483293656
0.052229152982950625
0.20162666739838214
