# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; IPS306
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=13, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.037230085151642896
-0.022204297833994167
-0.0007933346719819143
-0.010345563724899778
0.03148393782847543
-0.03493265939725935
0.06548642041232935
-0.03513465462747823
-0.014416650375241785
-0.024329020295092513
0.05047288813768004
-0.08694890429467124
0.055625019188392896
0.003914171767415117
0.0011646346589512958
0.025463838693602674
-0.002347234047997273
0.057333556290012695
0.01780472332074369
0.010705507513055343
0.018593725738312722
-0.0033134729787925137
-0.03730310750887737
-0.015168054306727297
0.013332726974543546
-0.0159208170046447
0.010892573971784924
-0.009338389250315382
-0.011276496232689621
-0.018022871659160337
0.0749437327280298
-0.030925722185550515
0.005502860797907975
-0.027724541327169203
0.014828099263719023
0.014007147019190523
-0.027818930058345903
0.05785074547473673
0.012689920300608788
-0.04561585757824841
0.004148119961421537
-0.0017518504120758206
0.0041011799901425235
0.017462888853633448
0.014356628579335005
0.027536915579546874
0.023889041048396918
-0.012642799158428036
0.009244793156212213
-0.02405178889158334
-0.01744948646545833
0.014227221926360062
0.05469757991961579
-0.03336107840629229
0.04367074167939562
0.017786569716366902
0.008040604987837437
-0.00496792484015148
0.006102070393077285
-0.0444354035996338
-0.01954859029933038
-0.0008398437968422413
-0.010466584129003628
0.034429184453527974
0.03926345826027515
0.03651811214425105
0.001945290723614726
0.00425398190361852
-0.02092842160918233
0.02180274444378897
-0.012032006484000532
0.012494338386379814
0.007405820574410391
-0.03527126577857557
0.0184612026058652
0.03555134211052586
0.050007445558668054
-0.022175805224058084
-0.0001195377113055434
-0.030701875872751815
-0.028062945289463292
-0.010985445841769165
0.020093562520600045
-0.000512959076130106
0.022558781681981582
6.27609623674967e-05
0.02864781833901012
0.031187093339655903
-0.011315021181069102
-0.020535161885414585
0.015867839601779565
-0.012336385790236886
-0.0022661251666539206
0.023643779528336555
-0.046191599144182784
0.0015007560047073359
0.044806931608766795
-0.03282366197468247
0.025803192069659364
0.030570333708680567
