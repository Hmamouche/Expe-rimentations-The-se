# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; LHU15
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=8, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.011570422746574074
-0.14748291999477203
-0.10272292990265985
0.02902680161568709
-0.02361579640464414
0.014947550340534459
-0.04686588259284085
-0.008799285932739482
-0.01172770085291725
-0.006225733946666593
-0.021437592944445128
0.007728074321004097
0.022575790216149336
-0.019560548618495384
-0.012336092727784267
-0.025003330214346656
-0.07021613634828976
-0.021453444735758366
-0.029144571218255963
-0.023803986115431643
-0.031204048082682706
-0.05166228590000044
-0.0031471161398281934
0.008233089150892058
0.028487288473695537
0.01593106418907772
-0.005322525501350468
0.02737250014949711
0.043683568201482115
0.10220387370264686
0.07981598106831239
0.029184357132141788
-0.04166216413353474
0.06153554949863835
0.06876749899628412
0.05411038476784952
0.04331901492606851
-0.001440896780073652
-0.05995397816618142
-0.0008341694130899538
-0.012556246379264521
-0.01345392018679861
-0.018221126055536933
-0.06850257829278755
-0.002855993312356046
-0.038301145301448784
-0.049553992053911616
0.017949902264473072
-0.007179874235921105
0.05524253105392108
-0.006631447254601812
0.0011537235049944813
-0.05240453462999829
-0.04110064658823653
-0.039591771499769904
0.015129941057720538
-0.030850265922041983
-0.034600826190998704
0.001994355465595527
-0.044571602740279964
0.03698182298603019
-0.005071557273078654
-0.011685536236305586
-0.035845530853335084
-0.032646558750939504
-0.0189061545999144
-0.027429099344551157
0.01603120497882766
0.014232026580385911
0.013840316889908142
0.07460548325150775
0.04371780495385176
0.08852662898216802
0.08784591400174957
0.018016915654013373
0.013653776805816968
-0.015890002353101583
0.06418482126997983
0.025730203073919634
0.048972053178434866
0.017612529264239875
-0.015108755443773167
-0.031013135382762822
-0.023947848201311715
-0.02470927259095573
0.005178017318498533
0.0008186102419164634
-0.03597792634235833
-0.043210705292612714
-0.005238547716152503
-0.027382589586798785
-0.02044114411698448
0.01365281085851949
-0.04046197138123546
-0.013778797338267776
0.007276970474868684
0.0603781226937518
0.03405416678448389
0.03639747368391476
0.07069923113348683
