# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; UTL11
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=14, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.062383054483510426
0.019196252490181975
0.28647539577157916
-0.2222206269065926
0.019993664266614905
-0.00984390626198682
0.018139455539806913
0.0046435956982390884
0.08756384657034275
0.008436804344644208
0.13016448069575975
-0.043585628844519955
-0.07583635019551682
-0.011794872518052047
-0.008953869332777677
0.10315183524413363
0.04526108376164554
-0.04252487370290271
-0.007690400463182243
-0.0026165496970037283
0.0360724530636552
0.1168000252207625
-0.0065253176675158855
-0.02854856386478987
-0.03441537112359453
-0.053093842066723004
0.03198881169117318
0.004131459776081661
-0.11955276263662845
-0.08156697693538598
-0.0008834394193064973
0.0321164535226385
0.023750366689448847
0.028644470958980178
0.0055305336779654865
0.012467010756035669
-0.034387969467713095
0.07584202080440017
-0.0606672186771252
-0.01984205874029862
0.07580673999587016
0.02828687958205334
0.09982980862724161
-0.003963068250592842
-0.005580007450767997
0.005579079600878662
0.010031168987712146
-0.011207801032659956
-0.03102707614212246
0.0011658945774035388
-0.05342301350911224
0.04596650162428021
0.028903241773935016
0.006712242297830618
0.013995166168760422
0.0074316503896251565
0.0467236052571909
-0.01293628264511143
0.011593558156856509
-0.04728710287999349
-0.067159418098241
-0.05660780408049747
0.033018980818805
-0.033923778093690085
0.021799326715643438
0.026161479659965032
-0.059613213354747965
-0.01291954145519519
-0.06648541017508039
-0.051004508182890575
-0.07830765811508567
-0.024453528134411998
-0.08306600069647838
-0.07909757391965408
-0.004575445159638123
0.0693775010626024
-0.04151842507291715
-0.06512949175085135
-0.04706636310089027
0.1008118395161628
0.039003828448539585
0.0633371029220795
0.03893567063709396
0.006443530157840613
0.015378717150916849
-0.010373159317891747
-0.00804381824328083
0.010230515338169608
0.013663264154419152
0.03646054447556238
0.04302021810912503
-0.020655677238609574
-0.03267634909893584
0.04856436741258782
0.012794054982467742
-0.01329469468096695
0.026550067791034943
-0.08100849083969236
-0.06348523580378894
0.006324476805329913
