# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; GDP276_6
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=2, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.008021552630871405
0.006979971956665141
0.007955009051059756
0.007129175803317927
0.006296583930283406
0.005371119372134962
0.005602902013879144
0.004992893732955263
0.0041193262794921815
0.004218611322671818
0.0039986405507201365
0.003613021592618934
0.00331442897949285
0.004134321886050642
0.0031311623882645996
0.005889232649037579
0.006593203058987944
0.006749778220104028
0.008990800556206869
0.01015101840207124
0.010450144055548154
0.010770606258498702
0.010999020449050457
0.009785808729452552
0.010371896533696327
0.009935491083434944
0.010097245195665908
0.009994941867918508
0.010125195999332409
0.009682266119862547
0.007782402739242772
0.008340741692766471
0.007597495216201242
0.007830634726401674
0.008232380752763618
0.00876033988297488
0.00845933156374356
0.0077604590249913934
0.00827444477586056
0.007747493193294204
0.006961648060732721
0.006391619923650435
0.007221573193739536
0.006326829211319207
0.007091318548666383
0.007687678775897745
0.007612420167581268
0.005508517938268174
0.004813222788542166
0.0052053282418052516
0.0034831740754493502
0.005425125357577796
0.004850794674440797
0.005128457072552374
0.004554283631365651
0.004892887875833803
0.004561895803231068
0.0034870703716677115
0.004679783043565782
0.0057417019843907634
0.004774093212338037
0.004626623540173014
0.004555281175188728
0.004103555261685175
0.005267279380107075
0.005639845183998496
0.005258089160167665
0.00602535614924076
0.007015838810884861
0.006486089916623728
0.009391371014140386
0.007761035809262809
0.006828638253774061
0.0051033763023394935
0.0037739795626910147
0.005097326729374581
0.0059684628003888705
0.006948047221367631
0.008485926346931879
0.010154175145284187
0.009564152488541892
0.01058422467534245
0.010444405572184613
0.009063485101508654
0.008529877328858057
0.007140927983387053
0.008623277344879908
0.007974011772360848
0.00788428240044852
0.00848438931998551
0.005798315607909087
0.00733886096210586
0.007545455787103285
0.008208615573285264
0.01199316102642417
0.008676956193469584
0.009800527202954105
0.005215663216980464
0.00718085658004987
0.005756970096426856
