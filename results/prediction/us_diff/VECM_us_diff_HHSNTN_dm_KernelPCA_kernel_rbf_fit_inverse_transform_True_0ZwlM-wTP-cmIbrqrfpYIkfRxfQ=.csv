# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; HHSNTN
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=13, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.11801567381367259
-0.4478810822504499
0.018184384717316697
-0.16812296578056007
-0.024752706582663465
0.11320041960308523
-0.09330951207519492
0.03170646307902147
0.21808536423229022
0.2098704177829528
0.0009851640987269955
-0.08757981268705324
0.038421129660102624
0.018110431842706685
-0.013432206664280261
-0.08599311205437118
-0.11757989136475495
-0.06110084990602273
-0.1198293000585184
0.17491022212444743
0.032877736813275805
0.005295666110086557
-0.013991796922996982
0.05739792072279228
-0.012434550950695965
0.0016298648909222222
0.0809771805873657
-0.04216288213796146
-0.07444959765246338
-0.046582073225310563
-0.06943527121740534
-0.22842166086160948
0.06149204871199687
0.03925522988256518
0.1564171614397463
-0.10434719900111143
0.007314748210318101
0.08115243183266208
-0.0498113879582009
-0.019011534761340446
0.0067249836261425375
-0.04727996647681453
0.02133173458748076
-0.01338763555420426
0.04926397684311886
0.05972492091883787
-0.07505387383864233
0.04112585853486268
0.09476873158096087
0.024286777023129095
-0.09839150983805534
-0.06567977041978493
0.08091211481814872
0.043381809933131246
0.06472157176518877
-0.008172814568046888
0.02709142159540745
0.08927403703879659
0.045004644386012496
-0.014960737932391948
-0.022933001999772565
0.03940254025361032
0.01981096546298408
-0.15943527462893492
0.028176470742206805
0.10474288334429482
0.03395208078758492
0.024524137223805646
-0.0404985077080739
0.028420076326624806
-0.015249776297415746
0.008545671270976309
-0.09336016854774924
-0.01795760984086017
-0.02009043752171091
0.07475366823081056
0.05697995876801715
0.07035997624499112
-0.1813410622036837
-0.08934857481389266
-0.011919987453405582
-0.023490656781597317
-0.04255890124102711
0.006728946868454801
-0.005496904576269585
-0.029085327601125928
-0.027853512443064953
-0.0729850248390217
-0.02263652287453076
-0.05836008345612935
-0.05937759142629754
0.015018355184460335
0.03168106500663459
-0.017136100575717923
-0.042742738569826126
0.11927052332641962
0.08853105134564221
-0.04756061997978526
-0.024304442249972705
-0.05826357946936615
