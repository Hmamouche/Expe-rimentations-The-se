# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FSDXP
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=3, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.04629909486369111
0.07773083464148446
0.05283383638202555
0.10174254899105023
0.017861267433356656
-0.028511527346951324
-0.036366472826642876
-0.04529166863802671
-0.02806997822605559
-0.0512668861457094
-0.05866429031008717
-0.045075779820324895
-0.0031695238329173783
-0.018986545353454436
-0.04408448606303213
0.011619551228487488
0.023854260172908513
0.05847788090955067
0.039566773064808804
0.00015763602657087924
0.03685569662925153
0.01148269722787523
0.005771477824323081
-0.02419251907800837
-0.050813833259316196
-0.039486394460241166
-0.010463297929977437
-0.005375078422569571
0.0005343027380313248
0.0033111101980474954
-0.06434626629783485
-0.04365100817968742
0.009053123679806228
-0.023773187393130668
-0.01866351289359211
0.010462757260758869
-0.004413030864573565
-0.01352142308160211
-0.00551495755885819
-0.018095307501768042
-0.009936459208301829
-0.013590463571337794
0.027919528189803932
0.07378986186093178
0.038630960714417614
0.03436525717451103
0.01666033310883978
-0.04814311621136607
-0.046494341905655115
-0.039301395100025754
-0.037095191620066414
-0.002976256886253739
0.006091209426124348
0.004840269749665821
-0.002327231648460202
0.012677266901974484
-0.003708540903647842
-0.009057463609028609
-0.014669783975424672
-0.01813953308114481
-0.017759270315465463
-0.030271172245550204
-0.02174853973701454
0.006955647313625496
0.01750895300907135
0.015038725086688227
0.009296820003022711
0.0031065289796977707
-0.01892942533731431
-0.03968342250682093
-0.04690434435364841
-0.02324550679590541
-0.006121672977129051
-0.022928510158903487
0.012813382331239874
0.007988898110642658
0.026528812901790558
0.0006570807768310379
0.013496502656336274
-0.003361856913668546
-0.003807216544718669
0.030114466701947173
0.02479912827236102
0.03329168595622772
0.02610846025940386
-0.007467437245772007
-0.008951746769042904
0.0019803530031355567
-0.004912564892800789
0.008167345644491767
0.001408296901832844
0.019461708088578857
0.006310114638853314
-0.026558520204312094
-0.019687556000775017
0.002555432257298741
-0.007556814565913481
0.012618691906602579
-0.003342668585695203
-0.006986232899237032
