# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; GDP276_8
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=4, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.009954339137496467
0.001998733512912801
0.0035913722415583356
0.00454842959003645
0.0046573660952394695
0.003131357071347937
0.008033459017449543
0.005847391508867043
0.0017458436834477558
0.009021589990486911
0.006875684455488422
0.00870655461188747
0.012300022101429593
0.009167401902571402
0.003926132179864258
0.0010109378211933767
0.00570846496345096
0.0012541466624685289
0.008421600759225871
0.007961609221945389
0.007226792259992949
0.005836820741680855
0.0065775416819085115
0.003708901817286473
0.0023840169055649872
0.004059762193848467
0.004081511667056084
0.01216304742820347
0.00525493932549828
0.004201946867679453
0.010529280340646869
0.007071504409627671
0.009493469553733985
0.006789214932082192
0.012437355103323482
0.007474326756664186
0.009893401322885987
0.006968851686507831
0.0032820320956540973
0.005142187593077782
0.0018503972023639719
0.0028479944821943035
0.00299570852031983
0.0050166241398033095
0.0059309713876552475
0.0056593669827339575
0.004441573832384918
0.006268562810846025
0.0074756984073393665
0.005431979301160005
0.0055719145518984825
0.006781376446061932
0.006499216077252553
0.007637775463754022
0.008228419085772608
0.006765194284539915
0.005673435592601102
0.006335159005996428
0.00436354299404678
0.0025385362457982243
0.0025676500539550183
0.004494071725697782
0.003344431342367812
0.00678034627026374
0.0033135881874736197
0.00544862491460544
0.007798136527563057
0.0031948244082554977
0.0021586675604351453
0.0005061308624582188
0.005974765983199359
0.0024046345915451637
0.0018017679577256068
0.013941078666243063
0.007228598732537228
0.009552829968781805
0.005291912678823158
0.0064881004474818945
0.00588934277682872
0.006395238183927017
0.008701037522927724
0.007922773237834187
0.009292085221625822
0.00951163197245988
0.006436851244667309
0.008106498371643546
0.008383921720914194
0.009717267083636003
0.008081701482114012
0.009178026074884079
0.008658085239374355
0.011117828516975403
0.008117554035794658
0.009362686584155386
0.007345040255974629
0.01072932070497881
0.008473053729897612
0.011054752366963348
0.008086400544214395
0.011406178815857268
