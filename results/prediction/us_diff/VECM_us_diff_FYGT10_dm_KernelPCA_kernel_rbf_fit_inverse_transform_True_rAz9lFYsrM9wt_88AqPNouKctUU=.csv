# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FYGT10
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=5, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.04687311792279514
0.029814641319221166
0.11457763326701073
0.029224136785813752
0.011579320622066291
-0.04730785231500506
0.009439592227004412
-0.04566393889854392
-0.03174005929110753
-0.07277303953328838
-0.052909717894908274
-0.03825171937470123
-0.07463669163278672
-0.018113445936039602
-0.03756473690291387
-0.006883814388024481
0.07886757288093038
0.04790953830051894
0.04688794556495522
-0.041812836246042956
0.021043567578818617
0.04183941914028874
0.007220872258575297
-0.01565772290567679
-0.02718456667269676
-0.020045402645760928
-0.005498293275348393
0.007159922720566446
-0.008183097678441775
-0.04081621760585164
-0.009743452816347174
-0.042018863338088644
0.0048399513434948475
0.017952863089507515
0.026848409509361702
-0.017382275841312973
-0.05762792742117888
-0.020057753591089585
-0.04517531564680796
-0.05957997717794766
-0.015290846334996763
0.007095950990321696
0.027158221350261448
0.05895750758599351
0.059615435555816394
0.06582383882948857
-0.01418767222642672
-0.028125419698044586
-0.020346570556256348
-0.03119029718395215
-0.016246269123043812
0.009972407025170143
0.028511073220388025
0.0026654963817980652
-0.0006382103542611154
0.02344516794968069
0.03746998610400229
-0.01971704812847621
-0.0019287994879256497
-0.03623902521947539
-0.021433274196628722
-0.045985686351017706
0.028915287545474844
-0.00993398051004918
0.014750962732675328
0.06130055038369693
0.024751278221879697
-0.02236023210157242
-0.009142992863572463
-0.03482317824318778
-0.023538923843981985
-0.00409517307431817
-0.019434562684489065
-0.05599067761557672
0.00757005459321225
-0.003871426062305349
-0.04671213332205556
-0.01744113105104996
-0.0661962255563879
-0.024467975470264743
0.04249269129719916
0.033038705330629636
0.04892743502183176
-0.007050193062829238
-0.032323940054654186
0.010699489506869496
-0.024887451156841198
-0.02390677884009481
0.002051039257843041
0.005890848845327732
0.04326826400689689
0.012399620839323881
0.0024519233580973833
-0.0071303786468606585
-0.0017560777404830065
0.02612699837049496
0.0332302579367143
-0.04063658652537843
-0.05551767403333386
-0.012391885210797345
