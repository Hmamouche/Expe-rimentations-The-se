# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; EXRCAN
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=9, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.010117127989185945
0.009577469227046527
0.03619196010839361
0.04391757959546762
0.03164079471685785
0.02465235015126874
0.019198241585304743
0.023453191464358456
0.010651194829491803
0.017101164262022094
0.02428184008835582
0.0005127537884962331
-0.015936652092964132
0.007860329837834613
-0.04019844932795319
-0.016758951427955075
-0.02027479348884266
-0.0053915097087355295
-0.047965070375292934
-0.01618076788234601
-0.030972099863058752
-0.0016394156894051666
-0.033580760156622
-0.02572955159887789
-0.02227634798380319
-0.018874782010255237
-0.00722364571256295
-0.01946151303913452
-0.059843969399770734
-0.024108597905689255
-0.0016886507939030144
0.020002691145258142
-0.011137477987648766
0.0006806343853303781
0.0337242408477866
-0.01601761019596881
0.023658828025271462
0.05428241900178966
0.00502319825683042
0.043809890772892816
0.045870993473429746
0.03403176780878441
0.04233298936895324
0.0354486140811979
0.009787084191118768
0.03644241438357648
0.01755676750206837
-0.030463179172465264
0.03518932078237192
-0.02764375430756571
0.03340117342838027
-0.0030537474092568228
0.007290096105508681
-0.03255801442480165
-0.01015591867209964
0.006216047863149251
0.016091776918481734
0.033378610901879345
0.017955847713701907
0.0366793082409128
0.07533308391524068
0.0400969779374163
0.06477290327374759
-0.035024956157623036
-0.016641971248057726
-0.012193790704738559
-0.023107154769756243
0.007096079067945265
-0.018316088093757604
0.055816721287811316
0.005000341036043285
0.032921215334152557
0.013112867672623384
0.03356404445337194
0.028507769241591114
-0.009409822527181064
-0.007751558945537548
-0.012094781521907896
-0.011463354631944561
-0.04708256493645734
-0.03461351446271098
-0.13867071688683147
-0.05783984997085096
-0.06449285943830163
-0.03771950105215223
-0.04047218503939086
-0.013677435173903425
-0.03648726126776321
-0.04100100496772317
-0.034200388820913905
-0.04142408042832549
-0.050526731155977875
-0.027715957837865297
-0.021695098811011275
0.0268630531363022
-0.045385454062290324
0.005025806803244241
-0.0908757533873812
-0.028867650897439485
-0.0651208983505346
