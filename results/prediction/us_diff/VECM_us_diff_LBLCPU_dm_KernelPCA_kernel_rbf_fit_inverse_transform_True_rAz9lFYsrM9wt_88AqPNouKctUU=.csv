# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; LBLCPU
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=5, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.006674621329074355
0.006436101942302214
0.007821418197518783
0.0070943593363092215
0.006461012110988663
0.0014215088086410863
-0.002389781139338506
0.0049430294185257035
0.0022034734711956234
0.004721506131930957
0.0048828242632143965
0.0031348965150756816
0.0031451431367190068
0.00834575665308433
0.008227941725786144
0.01005691426866161
0.012329970385012864
0.0082704406903642
0.0026662225583201616
0.00434010910707747
0.006196371869651902
0.005657377187054474
0.006435611357330809
0.004499377116112106
0.0013291696931370988
0.002352966068709664
0.00653911769652186
0.01154094461502498
0.013119410807436265
0.0109803949272132
0.007062182880860902
0.008777611638569029
0.005592592958245487
0.004838494048700481
0.003211867029305937
0.004940786814449155
0.006497867473299248
-0.0003123662985856828
0.0020041236774272888
0.005483712803549273
0.0029416684639890023
0.0002635305054478293
0.0052318184882949054
0.006355160696868881
0.0022744168449962865
0.00226595909840508
0.0005376435519591153
0.0020619158279433785
-0.00032396011771661877
0.003870627441351895
0.0037192229792031364
0.0028602856292004736
0.0030535329586045684
0.003144079694490341
0.0030179778262817607
0.004310144118846704
0.001219763036402072
0.00395786320758099
0.004728248510504009
0.010338540323162788
0.00960645746835085
0.005762581916033746
0.00416903896479261
0.010654045342575965
0.004695705690101034
0.004957399045253296
0.01108841431264973
0.013537898672258606
0.005322478444164735
0.024731252966821448
-0.0028905742351652006
0.012568484476062562
0.0001555460958932108
0.008447143574072212
-0.006144308559617698
-0.0021680409027391976
0.0014167341449032506
-0.001861767340417742
0.003049136670686217
-0.004859197643846668
0.007499004597214595
-0.005373362069932665
0.0065101969735194285
0.000711732300951812
0.006735563613016673
0.0025429396182551777
0.006315794924953624
0.006703926457542381
0.00638991381809672
0.0016345268078893599
0.005530692983346622
0.01668342150631997
0.005163927037145475
0.007386910689309623
0.00821503454551082
0.012779743961396544
0.015383418077062543
0.013022763263648332
-0.0029392079235614874
-0.0018069029425403361
