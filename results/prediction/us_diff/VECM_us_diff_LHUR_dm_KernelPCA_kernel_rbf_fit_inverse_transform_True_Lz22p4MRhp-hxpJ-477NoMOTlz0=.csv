# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; LHUR
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=11, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.052780811903711186
-0.07857019564260581
-0.11539277987115494
0.06292098520169147
0.0177134140495724
-0.03201852360523367
-0.019523970660702054
-0.03153004447274278
0.00570445666771361
-0.050192483190125634
-0.024650642707617243
0.02251069357942576
0.013777075749929143
-0.0015415838750359734
-0.026560149593570823
-0.03048431411880074
-0.07583404866816834
0.013326294948855108
-0.05051920046137616
-0.04420927768674974
-0.03870717553349886
-0.0987970901575346
0.025998768796403303
0.021954461326743765
0.04390030331976441
-0.008415179516552093
0.013646208795874703
0.022507890460868138
0.034738365387545045
0.09815898058617584
0.10549823815496226
-0.007506522613089439
-0.049990258810502224
-0.022492986377209815
0.024706882794137908
0.025657049927205577
0.03708663947195273
-0.018949420574652282
-0.043219140064643266
0.05610659905880902
-0.07688348042251633
-0.020289270350541365
-0.04968752023441761
-0.033556561836594494
-0.05050200076393472
-0.0382143767029128
-0.03445700556305778
0.00943757037919828
0.0028151318620218933
0.015198294385367617
0.043000410054427424
-0.00982821235846942
-0.04176614677927936
-0.0483363272346987
-0.05736204864749591
-0.0057499363119210695
-0.03945637193445384
-0.016258008986484492
-0.0066147486502855495
-0.007571664884244854
0.04191406456582454
0.008655194298906316
-0.01495247579323529
-0.0032355988475610276
-0.03209819550971446
-0.05876523509415868
-0.001559317174205311
0.0033875819836510605
0.027622329219379804
0.00522815370261396
0.060040970224227684
0.0330601762723369
0.08057456848765339
0.07466511392020744
-0.06715470650578481
0.00870831023242621
-0.00942001267296749
0.012421432713791248
0.06254300223336064
0.021933093906241447
-0.014684013001471439
-0.06385472696471899
-0.020936646085026923
0.0096999182100224
-0.012009089848846032
0.0313937287295388
-0.003660962504709136
-0.021767607427165592
-0.04069010011954184
-0.0025871918168902976
-0.04716897833294146
0.005994039019949401
0.02190241698278837
-0.018722273541861154
-0.023544157634236586
0.00980193067654794
0.026481534572879353
0.04975399060339514
0.05591336027203326
0.06292768448177527
