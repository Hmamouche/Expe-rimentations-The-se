# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FM2
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=3, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.009701964554370893
0.00378526144955494
0.008128584087441798
0.0033077599204058214
0.0052553665089757175
0.005838787325334086
0.00743265311783046
0.007390245756545663
0.006447113798596411
0.006556891375984238
0.006880343293193518
0.007396555924447507
0.006447499651132874
0.006373427901994227
0.005264811998512412
0.004597037175813903
0.005716890455018674
0.0039635524387636265
0.006253612848610205
0.00520720129645919
0.0036568243582755407
0.004971768804929654
0.0035835341231377884
0.0050836127284884704
0.006793875419946253
0.005616271892400546
0.004195103169017765
0.003124367093288041
0.006522309702175769
0.00446020096744458
0.0063925448117169124
0.004554232918717583
0.0028671487356821393
0.0032866286588603567
0.004421701628623441
0.0011427595051834963
0.0017971225385558943
0.0015383525892825986
4.413373697626866e-05
0.002430074344592461
0.0004292724740616059
0.0031515001127206317
5.326519778604615e-05
0.001064324568317033
0.0008377451584421649
-0.00033205848999127584
0.0016869583594195815
0.004490113110499014
0.005236270374437053
0.0037499739579434127
0.004864307294825793
0.003475496938393976
0.0062161363382274605
0.0050131083450610636
0.005419227803479255
0.005784051407123991
0.007422300742905869
0.006862140077443302
0.0094251451978513
0.008562051139137857
0.009520473913006658
0.012480364399667305
0.009137220309847982
0.009122583370119921
0.008656368781278954
0.008682801635261514
0.009508080292148424
0.010190655264354132
0.008147287735473236
0.009464600777943836
0.015192118112662073
0.013368805691488348
0.01418494749785979
0.014422381378477487
0.012461509551637988
0.008938596085623154
0.015289492488715223
0.013659827952717644
0.012379616372149377
0.014259369815600722
0.012633511337722336
0.00400376355658357
0.010976318477969925
0.00845210075651838
0.00897842179525064
0.013173234468227428
0.00291669498544422
0.013206839153943776
0.00832041135705434
0.009162055072665466
0.011394781660562046
0.00611718943530205
0.012355906547374526
0.012462458936076667
0.011894199114826396
0.012431666460433356
0.01230951906421671
0.013794391472798883
0.017335895639312666
0.01166598486509151
