# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; Sfygm6
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=8, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.022511571199957114
0.13027552094887265
0.0941601740098007
0.09093265879263498
-0.037184934886381436
0.16498756719358768
-0.10705597430605775
-0.03874961201832471
-0.10314796646271226
-0.04766374813503584
-0.04949665060494726
-0.04415614735614728
-0.011301553481278419
0.1461004052184977
0.006129644723787283
0.05689701250275618
0.10474932716767132
0.009861377067836602
-0.04392173381009345
0.1197604604953557
-0.12347736176235322
0.018432696544315452
-0.027953471720183462
0.05439135638648266
-0.08377556683734122
-0.12155627677875638
-0.036083600634125815
0.10459981056181283
0.001506879919061066
-0.017611734767040775
0.11827367822814083
-0.07086983691325004
0.03676912520825602
0.09148026749250146
-0.04868373717133883
0.008694245207858435
-0.06737802634393199
0.04444882620650677
0.15773335854436174
-0.035138963095429986
0.02769483027316895
-0.03076337438257659
0.045665337642951
0.11936668997069774
0.014941516658847933
0.09373532302944064
0.045067424447643424
-0.10399783969103751
-0.06593046834134224
-0.08676082464849798
-0.056881254071425115
0.08840150985552897
0.13576421861868002
-0.10757651406368393
0.03459890344729
-0.0012841665386769709
-0.06320428744982098
0.009716528499943037
-0.0067893053215109765
-0.005430601485592505
-0.024370777427600583
-0.04432477041913083
0.07565184162871179
0.06011263269606482
-0.025814672419596514
0.05261367007382249
-0.040991485040261524
-0.04177120482285371
-0.025515294666668537
0.035587557715420826
0.009149149933370598
-0.00918009536393602
-0.01627759247794906
0.0035597986217574696
0.03233043767311917
0.01794620831142455
-0.18046789287292453
0.047803541778518865
-0.06292970405298841
0.05904518757714157
0.06598654960277979
0.039323736749290336
0.02976560429515565
-0.026088412387180734
-0.023814501630580867
-0.06188830485004736
0.055226702106153644
0.09015824384409656
-0.014534784123715387
0.01688703355032041
0.06080540065646204
-0.011671605586724275
-0.026673799944419402
0.02247649069937273
-0.11893361577383874
0.020997216767362756
1.045458606320146e-05
-0.03643560980449442
0.009830680872530837
0.021946015956139465
