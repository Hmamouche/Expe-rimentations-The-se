# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; LHU26
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=3, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.0523191163286385
-0.04544307054675101
-0.08171673170980188
-0.012373254748642403
0.004401750342882918
-0.021107759328110234
-0.004140643730016356
-0.00014059326104338618
0.00527994622141603
-0.03559204366135428
-0.0023552921102367183
-0.024299589031769347
0.03973367947269019
0.0075907013712676295
-0.004495741961471497
-0.0021292960303420775
-0.06735069308018296
-0.02368791496024133
0.016829576964318214
-0.050266194057999924
-0.0015855241510917776
-0.049835815673589585
-0.005302001247891485
0.022600511842672557
0.05532754554353964
0.003655889331321799
-0.005329868881962941
0.01680595428466463
0.0626506564179424
0.06565239440713817
0.08132136160179589
0.03522021481220439
-0.03937277070224442
0.04720475437961878
0.018823336507214905
0.04178602648658028
0.02386223660682606
0.017414708234737723
-0.02155337553401897
0.026433764561169186
-0.036981504593842966
-0.05412838675679047
-0.029628537629364918
-0.07383317895828058
-0.005977363312097319
-0.05839590834750188
0.013890560521221264
0.018490419703739013
-0.0054477101949913335
0.04035335026464812
-0.016116592732878788
-0.010130586459535833
-0.021447235403261544
-0.030616625563748542
-0.006643640631162238
-0.02127172026401192
-0.05285842782617699
-0.019824639352870668
-0.012179144451509614
-0.0025564724502621476
0.0290780427167349
-0.02787972637193405
0.0032384618374438224
-0.04112612292410925
-5.9176928476629975e-05
-0.04136991847514197
-0.014322032059763245
0.012471257912760134
0.01361907650753831
0.03170114742756412
0.05762546189406612
0.054876136473065155
0.05870548050107916
0.09655129445469002
0.03166633851521352
-0.0018165220960942438
0.032277511981210046
0.04647895016306094
0.0423550454675492
0.014324447091630516
-0.006984863851299128
-0.07189489556748793
-0.03736867627356737
-0.024763725367352062
0.0168705162376871
-0.02315092735041031
-0.015142394199793845
-0.020890644882261195
-0.020449437464732288
-0.017272480185851227
-0.022626305567929824
-0.027645520332648846
0.04151001864569593
0.00272296418904556
0.023934980826943418
-0.009159751774965537
0.017324088616991307
0.057961426153443
0.04262324658571089
0.07965219460280537
