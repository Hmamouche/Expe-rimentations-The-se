# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; GDP273A
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=9, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.005468697071149581
0.00409796904353856
0.006961896216490065
0.008377372367460802
0.004631791825645078
0.004423858233168168
0.007999427732541134
0.004160299437907896
0.00505983657052822
0.005656252579693405
0.004857883533330506
0.0003799670665200804
0.00393678821279981
0.0031264169592822256
0.006440860489299445
0.007058186852482616
0.008082220656311461
0.00723694195017282
0.005953268606659847
0.00678974340722858
0.007321033965473145
0.007808448798276355
0.00884668846029471
0.00982851448092576
0.006206400645092663
0.007162940005494311
0.008500809223470683
0.007905422680994437
0.010972837476747994
0.010689996063775891
0.006929293725929609
0.005880405009691647
0.005376531187590765
0.005829803193059875
0.00480340161465735
0.006601334737780722
0.005935565222555757
0.0044915257703208415
0.006590708496054484
0.0033312903267923373
0.0036431618612116703
0.003781739549728768
0.004265450803664611
0.0050397130380016725
0.006610534436612098
0.004391067258862237
0.006236234900789888
0.002863002543780184
0.003473471381700196
0.005259469406872678
0.004332027029950331
0.00618303665408161
0.004743387551347967
0.005240209726668047
0.005101097850024117
0.003864875906174959
0.0023688525102406304
0.00193068697408781
0.0013392449361851106
0.001478703592966336
0.002407625551680942
0.0026421089688598784
0.0040627658059740335
0.006074616506341655
0.0038671467601919885
0.006858341471647087
0.00608984176886887
0.005698723326745646
0.005438459635341666
0.0043455782463538315
0.005751684659561152
0.005441492772742472
0.005855224851983482
0.0015495514935439092
0.003150803782549949
0.0052172995000503465
0.00237623063320476
0.005725250922030266
0.005614415917352372
0.0029453094354815776
0.0061239800848388645
0.004334694728610229
0.007463081277904118
0.007763939392988877
0.00702200256637118
0.007797373213042493
0.005898552938891294
0.007693463443854663
0.009451887965992622
0.00952791328613532
0.008282626921979544
0.007814843266467045
0.00734476928450827
0.004126475413785599
0.008003591822800119
0.004002902044135103
0.00893835946894506
0.010481814598564829
0.008433117949799646
0.012266514787125494
