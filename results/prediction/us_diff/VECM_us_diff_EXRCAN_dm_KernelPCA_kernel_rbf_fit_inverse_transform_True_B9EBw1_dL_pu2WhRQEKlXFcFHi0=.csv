# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; EXRCAN
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=10, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.014407393512243998
0.010301066429737712
0.030291991958127713
0.07179205917429175
0.0278090785792504
0.020622257303933772
0.016641737522819743
0.02886049730420079
0.01574121614268935
0.00973148327288899
0.009598238964544405
0.006095178666491377
-0.014315845038318425
-0.002146603384862925
-0.04273386532371975
-0.005451039205032587
-0.03368227434429032
-0.0040626257831665995
-0.02960093772555738
-0.01671788530977683
-0.035746370953331545
-0.0006815509893583482
-0.03192226780387365
-0.02318730052748124
-0.026641445183533577
-0.011056611269974658
-0.019620937826213214
-0.022509724441132514
-0.03864526585907854
-0.035934481809507896
-0.02099503758802354
0.03457077656023856
-0.0038957940648395237
0.006274648856100698
0.029898581745196696
-0.009095614810535706
0.0085133221526733
0.047945975334813255
0.018732102657302196
0.050612672485995304
0.058335368858339016
0.026593351407248847
0.03663593841627691
0.04159897130415834
0.006963758656460037
0.024637039489718873
0.02689580205701238
-0.033740653839273556
0.03439643930180415
-0.031206742768193428
0.026340300109012615
-0.001594234047786064
0.015797982045386218
-0.027852481490444177
-0.020102177351656966
0.007315548489058873
0.0169061930704913
0.03360179856486558
0.01063999224843188
0.035752748724587516
0.0676685072702826
0.04876978178702206
0.06390745329603004
-0.03971157921811273
-0.007271479838234653
-0.011666807665666239
-0.029657030346429795
0.005342916970570615
-0.018256574727397255
0.05511509249323021
-0.0018646893574030387
0.034348018520992084
0.01573893134495573
0.04060585234116522
0.03312141725160132
-0.008389650295125225
-0.010525846061516271
-0.0181783305598014
-0.006498357016675723
-0.04182854051403388
-0.023658802274008926
-0.142040111919031
-0.056675945720212034
-0.07339820403540581
-0.042117080412651664
-0.03499712855354979
-0.01247365181251003
-0.0370693525809058
-0.0374916636251023
-0.028935317218128385
-0.04065008490768511
-0.044102348186184116
-0.02395766149384905
-0.02482836527509748
0.025903683762467412
-0.048772666273417614
0.003211905590344263
-0.09024960127217253
-0.038880878278967204
-0.07355997896552167
