# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; PMDEL
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=13, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.21029834233679903
0.18673761330685978
0.3343170836322904
-0.0761156719568223
-0.20376307100049115
0.026962313876906382
-0.05194281433330107
0.0019144256336309298
0.001623616100838339
0.3011587316963284
0.14504262138967067
-0.03067539349141995
-0.11791143711740863
0.0026387828440490035
-0.10419633684912155
0.08748619667255186
0.03546500689007274
-0.047658002661333396
0.048388838956871946
-0.026768980625740305
0.03427685657097217
0.04198962663893176
0.03221559944190692
-0.16221683666801773
0.08025081990505628
-0.09522086552544055
0.010328727285585659
0.036517318774835855
-0.11543717762544922
0.05712968311302997
-0.05682199029547665
0.14917176092414375
-0.03305008206372985
0.08250949304646467
-0.03201546492325589
-0.06044385425718948
0.05572856307860799
0.023018432860823355
-0.08205682928337264
0.006477604674361901
0.07512664772886929
-0.03933221629001871
0.11073747293052154
-0.0026156977719444634
0.04680559362084501
-0.024966570761778176
5.180652439287087e-05
-0.010206504522656708
-0.025184027950533018
-0.023570497892515647
-0.04989200022237468
0.0264976726389224
0.002223174160268307
0.03370979110256151
0.02608519935591594
-0.01753848690186107
0.002242057215784573
-0.0338977973897912
0.02061172083621262
-0.01172734293948621
-0.04114541403445246
-0.06938394530055828
0.12625065307149677
-0.09894687630821132
0.030500581015515883
0.04223051425998263
-0.03156975302288981
-0.0035784661112796302
-0.0658545073967713
-0.004219573108863964
-0.0846732455237584
0.11284003841783198
0.02139052018970066
0.04221027718892299
0.047137210179009834
0.014106463857494646
-0.01622865586731277
0.012360931493147147
-0.027332629482791606
0.04729396569170091
0.11202011269586606
0.09079624576102562
0.004729941389300992
-0.06381470579156101
-0.03329645204032045
-0.0773186137042657
-0.05445292978720136
0.054854242754397924
-0.025008185539522713
-0.013011546652001538
0.04467227545849609
-0.04237291135672014
-0.048380555943271744
0.10471303525305913
-0.03480724690332713
-0.03256460142566301
0.034942231322959275
-0.04267146973722418
-0.03036128819778637
-0.04790740750053515
