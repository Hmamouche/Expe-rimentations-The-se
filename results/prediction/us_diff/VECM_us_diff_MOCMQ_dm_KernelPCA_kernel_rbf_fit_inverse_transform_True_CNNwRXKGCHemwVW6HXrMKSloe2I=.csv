# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; MOCMQ
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=8, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.026093270933504775
0.015527080954847441
0.03308261070664822
-0.010187383761881353
0.002312651098602292
-0.004775921116179025
0.012632863201878296
0.0102062116851776
0.0018252345883185243
-0.00527254902420831
0.0352185618584408
-0.013189630629737978
-0.01800030188068069
-0.013143410389996866
0.018982474969006805
0.02060081874154418
0.022486231668293857
-0.0023383060929824803
0.015617056325151193
0.012702332608107937
0.020933596142719495
0.036168554191669644
-0.042107552961973736
8.768479909057256e-05
-0.01072098030572156
0.027288858965685482
-0.004625025035627367
-0.015583934616573099
-0.008425997560208311
-0.025691322001355137
-0.012759255189669845
0.028099692623959616
0.03416557845393758
0.016026041582181946
0.013788021839787654
-0.004571502634918087
-0.018451000992220978
0.03227267722906084
0.02909476983149191
0.0011673067124353502
0.03143882877496998
0.022608384598217145
0.013565379313769922
-0.00356258143292096
0.011609725755408955
0.0060472093912745705
-0.0034753540024646005
-0.0005187892921471142
-0.0026564728167137253
-0.012381365852670238
0.0056988394164940465
0.02420290720313443
0.01655931232343307
0.034666493003370345
0.0213905218899101
0.009034515293311852
0.02675851087731331
-0.0011688083311061845
0.015669709202025137
0.017254441620805167
-0.0023548093929878513
-0.008475957934559467
0.01409844271030793
0.014950779387806859
0.041871569350463926
0.02559527353897876
-0.01178867854405413
-0.02709302088562649
-0.01032376724701665
-0.0026404754763744397
-0.015495006281476311
0.0008285499575434851
-0.017207551594476594
-0.030958923377549852
0.03839500574122641
0.009713899087304498
-0.007097938804035928
-0.010680373971313016
-0.03376210004389081
0.015388566665326517
0.0269779778634827
0.01826650215938104
0.02289809869746283
0.00413708335328467
-0.018638378098726684
-0.02162840046047546
-0.030907913062341263
-0.0008151555826799014
0.014897523796188777
0.006843476582491333
0.034338530619126714
-0.018872647966018313
-0.01124655043613936
0.014933510341975719
0.003160690364032063
-0.009353612291885268
-0.00510236529087984
-0.04548113928998574
-0.02436851640237473
0.0027956824946529786
