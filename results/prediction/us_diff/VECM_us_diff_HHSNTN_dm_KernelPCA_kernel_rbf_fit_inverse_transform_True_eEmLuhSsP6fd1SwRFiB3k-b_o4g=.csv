# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; HHSNTN
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=4, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.05996448067596829
-0.09283372177144505
0.06752292910755563
-0.0031472476762608535
-0.045823282688961846
0.10211138275308076
0.08532425714624844
-2.961316287160287e-05
7.4881840852122905e-06
-0.061668939654240314
0.0005768018755332553
0.04110780256312534
-0.0010306643364641392
-0.050583499781579666
-0.054926707666760916
-0.06278028919456467
-0.06486800825667405
0.03378860822340172
-0.038624441455996535
0.02641941627235773
0.0039540290221241284
0.05412228521249162
0.02712975829582149
-0.006167609682372814
0.02425594178458601
0.006040802564025593
0.00017466385564533782
-0.04574389844533935
-0.06894207077398223
-0.12082504241239497
-0.0907201586021597
-0.04961517135837269
0.09170476657842755
0.031460878668161184
0.019309581024417644
-0.07153803767441173
-0.057343915775556975
0.07977952190072798
-0.012704247595591342
0.05355284144222143
-0.03217698267004398
0.016835047459884744
-0.001325635277248818
-0.04918374176439997
0.016734495108937583
0.06638626170020513
-0.005658936959546383
0.09483071732920975
0.04622937833099386
-0.0024525725249533725
0.02472871882385442
-0.042410629168405906
-0.02004860911665441
0.055498393353810724
0.0226447447892859
0.02828334260936256
0.07681370626511685
0.041419958865091824
0.049359410518686454
0.025516938462083624
-0.013310097918528607
0.007757840391234612
0.003503178421552209
-0.06107241325417794
-0.012919742670121196
0.014643383562403554
0.033060385477987904
-0.014405546281525593
0.03508219260908623
0.005856058832649847
-0.01161644716448082
-0.0077688575801768784
-0.09349349569960429
-0.013661740767381518
-0.02015783491299575
-0.014360783861256948
-0.051141709187797285
0.0032834090957904946
-0.12869927969877207
0.0018112569201541584
0.007371532501669751
-0.001137848850044556
0.015323020361243285
0.049738573682022745
-0.036336167216502055
0.008687443962020876
0.0017134883076701246
-0.07077653892045675
-0.0008895873540003887
-0.08998841497732953
-0.02320308236468827
-0.04017817788145529
-0.07763129409604029
0.05500575550901263
0.07098602539338941
0.024753814485790407
0.02469648551645128
-0.01626301126267988
-0.050686539884826666
-0.0952967140986274
