# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FYGT5
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=13, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.10299723619420849
0.1050398385999486
0.08413943208536515
-0.08359962810699662
-0.014147296168905456
-0.020824212633993016
-0.0280484791097923
-0.04732753772167306
-0.006607365848788474
0.06490704269535608
-0.07309963755531408
-0.09365601240512983
-0.08732931146500687
-0.019181008788821736
-0.03321750727261369
0.059823432207958716
0.0627110471936023
0.001550581194367258
0.033078129476588614
-0.06623463860717142
-0.03964145780201742
0.0902017983836944
-0.006054643271090961
-0.06626076734348745
-0.05570232590188971
0.016949443407702304
0.01200422949757677
0.01692931542761052
-0.040643285311915
0.03885417708196916
-0.07005860888370073
-0.1131966635900928
0.07413336403838186
-0.0169745414625415
0.007223632002151467
-0.010639046198963317
-0.061396955372260205
0.00033317497415365216
-0.01439570069335458
-0.0017017464246118323
0.06000775239262533
-0.035737313388460076
0.015999788147018613
0.040992595275685356
0.056076076026447784
0.11750113807433121
-0.040368103729065914
-0.03719609632012725
-0.043201019094863846
-0.05518460289682042
-0.04251683112556656
0.03285231711838648
0.056931038478294775
0.008452642099954808
0.003243005764005706
-0.005843205310241757
0.02231503191813595
-0.037920653591537706
-0.03445930777770152
-0.026543901293332478
-0.03304016889178711
-0.056221144900962076
0.0853785564908424
-0.011586687468663091
0.04446973432680835
0.06902875001328279
-0.014497317133956737
-0.05113057505329545
-0.023229229177546856
-0.03574190041179748
-0.044821248587842685
0.007288734252976293
-0.038251649728091884
-0.03544272871348119
0.055541392632625176
0.0001745395663626699
-0.04778460236359706
-0.027958721236956544
-0.06916932828765798
0.008549139551834822
0.019105180687859796
0.027718445896356286
0.05321472207287432
-0.04323135000098634
-0.06545478405567788
0.02658279162051137
-0.018261148829454284
-0.005713056102171982
0.038791810215855954
0.03924805534964935
0.03385806363087202
0.02914645302795425
-0.015463756558476913
-0.04197708976679089
-0.00839456168295212
-0.010438526972197916
0.007993983118273864
-0.06605388263779287
-0.07976786509026725
-0.0047738832858294986
