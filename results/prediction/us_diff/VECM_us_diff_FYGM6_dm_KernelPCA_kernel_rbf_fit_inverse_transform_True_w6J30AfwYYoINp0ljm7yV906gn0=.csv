# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FYGM6
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=14, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.03341669187938229
0.1035644564442604
0.1763204245624802
-0.06904419044173407
-0.062430303728874426
-0.11367278371870879
0.006491596750136566
-0.006704866755256625
0.07356799537245123
0.10394961125723363
-0.10143250257612378
-0.17518860339608866
-0.11697392232393504
-0.003538373952437057
0.05805526757660564
0.0818419439686202
0.02803865778611833
-0.09306856804206443
0.06956138860308206
-0.052035641124438266
-0.01258804186330089
0.16135907090264495
0.03183377239432917
-0.05846098535814988
-0.01663913225382638
0.05063638992757773
0.012059796430051981
-0.0316712020811514
-0.013969013439975884
0.049755326192438595
-0.16311414563139148
-0.13177615199334725
0.05250301374705231
-0.011310189413368178
0.011060716725802017
-0.02599511869440922
-0.1039050129458382
-0.08599223702319168
0.008133871704425128
-0.04693962717850388
0.08672217460466021
-0.0005065242503325632
-0.02073175452254242
0.02513899295711075
0.007615433787924895
0.09086794836042428
0.02619993830993856
0.04573333536742836
-0.014175159049002404
0.009299810648501178
-0.05521098565780667
0.003571378518172916
0.015889688716615512
0.019930478151911853
0.010776232128988025
-0.001775462657734428
0.0010185381772391776
0.010192681131725303
-0.020387465797463
-0.0197671714414297
-0.02888430391979112
-0.06746316821123288
0.042041317735923754
-0.008426029752251152
0.027810336018943643
0.07718030035009849
-0.03246840023840369
-0.05133979244104531
-0.029923523450573426
-0.003514349612877497
-0.04746778254401958
0.0010946110885116864
-0.058687993039785086
-0.12318049905568378
0.05187805957253988
-0.01160451821884732
-0.04919967126649619
-0.04218818930047924
-0.08876728737425796
0.039751270784320517
0.07446314380740801
0.06266615326985575
-0.0014372657352118446
-0.044031955389129906
-0.0033870857058569646
0.014069326771922006
0.02339645113433282
-0.012791590454829036
0.03571216489493588
0.023347149014548367
0.05104314482046349
0.03162767465665358
0.033746067171979685
0.006510301275969559
0.015331283862505953
0.027803706012682675
-0.010449784575900317
-0.010781671473549746
-0.0951966026869551
-0.011602653939504538
