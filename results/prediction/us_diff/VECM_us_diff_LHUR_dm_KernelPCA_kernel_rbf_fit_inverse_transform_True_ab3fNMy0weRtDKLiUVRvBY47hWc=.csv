# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; LHUR
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=9, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.024755041260525412
-0.06433166098125365
-0.12349056237439306
0.0371153673264315
0.02133701022707572
-0.017250692884185112
-0.038286583855021476
-0.021459355284165677
0.019459177773135036
-0.012586982146922733
-0.018165988836639068
-0.005003251755033779
0.029915535622801956
0.009843170430224646
-0.012091069919490847
-0.03063031092886446
-0.09367666062755288
-0.0032734081958978554
-0.049038842611692175
-0.03102977866403185
-0.04085248615452657
-0.08010548843081106
0.025630958292533028
0.014133588661694105
0.040477621566680305
-0.0017791525915818208
0.008337465661886258
0.008803265155738357
0.04128689073114316
0.1193961831938021
0.09106827151936969
-0.006038363178769433
-0.04931459148524005
-0.02776425320070555
0.036536859483554396
0.04175692674512147
0.03789720492368251
-0.029560697718875775
-0.043382210665263155
0.044345307933024825
-0.061459702328500995
-0.022027110865630924
-0.04921184845638523
-0.047665608032424936
-0.030458261056419248
-0.028541751971437213
-0.021622044648806064
0.015627385471900904
0.00901061184189915
0.022126201961404385
0.01894374661822277
-0.02475873240632495
-0.0604140039974419
-0.03505725404520092
-0.04779702247967673
-0.008695325044165782
-0.034601473887394006
-0.017862399615450147
-0.004684011201065754
-0.01736350838597403
0.03748137153566214
0.002898016418971001
-0.03171773454115079
-0.00774048641658716
-0.03724138324951015
-0.04530129991914844
0.014537648470634743
0.013586251210589225
0.024182711995809333
0.0029702427140578264
0.06443750910981302
0.032734367422378424
0.07329684148191795
0.07716449287822119
-0.060863871857666105
0.010660696229075601
-0.021543315436675724
0.02942647293522726
0.056768625111565905
0.016922001467516285
-0.01935308873828154
-0.05016449805413695
-0.029504685691790225
0.004499907078331972
-0.014371294849484106
0.026141238166025572
-0.001967694450688541
-0.010713025128556753
-0.021464799423996945
-0.0005779347155557275
-0.04898333500321553
0.016706128924365845
0.011501410396682673
-0.02466379146021549
-0.027178484569547644
0.001377858351133823
0.01767041553043345
0.05426755220385446
0.04638315363501246
0.04410221880946314
