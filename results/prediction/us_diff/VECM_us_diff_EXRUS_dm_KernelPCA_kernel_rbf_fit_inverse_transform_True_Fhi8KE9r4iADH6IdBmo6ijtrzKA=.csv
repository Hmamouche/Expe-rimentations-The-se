# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; EXRUS
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=12, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.0577776382095395
0.02679689443440122
0.021985217207755883
0.044284584702437464
0.10028240773431707
0.0379340766797434
0.08151973326308751
0.028969038480323388
-0.04248401124738904
-0.0544412257187534
-0.10969154750215036
-0.07581734988352953
-0.07049962630826162
-0.026771887421753696
-0.09410772341609575
-0.0276288490293447
0.0016393982298798498
-0.06645752417167942
-0.05777799687578111
0.06233509013055877
0.06064727197218561
-0.0352734051594152
0.012350585659586395
0.0341838846554855
0.015150624998244745
-0.016643034761737757
-0.05744480913656663
0.04413289609241181
-0.11992216582805397
-0.05657888693155916
-0.007440160494099516
0.0027215846192665152
-0.014405057063912584
-0.048957779092224464
0.07659067335720668
0.00859392857347006
-0.07370534320944798
0.06307200176169456
-0.021931820549554882
-0.04683032727168849
0.06133095352983125
-0.036018408383135296
0.03306279957173719
-0.00479592254480781
-0.05352017097912846
0.04429549495441637
-0.009285328683631109
-0.04647062913148072
0.009686147060887622
-0.02253151466033924
0.03751114602776523
-0.02074441799404245
-0.00191330961309767
0.04342032104053136
0.012236665974564638
0.02782459547420332
0.0331145427254387
0.027773532792521913
0.03302943926938882
0.004966845556067832
-0.013666769711347873
-0.03582981432203321
0.017232330055425394
-0.022041724399938623
-0.03175097601339333
0.024906869265658462
0.0006907826961007092
0.03228031892112124
0.019307439137178632
0.03007387870218288
-0.03499900156137634
0.004123501354794636
-0.02337543044808428
0.023141148004826022
0.043144008099580425
-0.025324410963226927
-0.014653130991026267
-0.01519142963915205
-0.11509014390817829
0.010712148120822456
-0.04042315951018529
-0.06803413314950249
-0.013993838146033112
-0.015613183857039761
-0.0034897535115260645
0.0028202425238958648
0.011103672947071715
-0.006541406041005676
-0.018804094502147508
0.015334411474919709
0.00829281003857097
0.016730722701131163
-0.013986694209798175
0.04786439476456681
-0.028196229761120906
-0.03795254397689428
-0.007426786599275759
-0.00915274933554928
-0.040336181610872074
-0.05536558771505631
