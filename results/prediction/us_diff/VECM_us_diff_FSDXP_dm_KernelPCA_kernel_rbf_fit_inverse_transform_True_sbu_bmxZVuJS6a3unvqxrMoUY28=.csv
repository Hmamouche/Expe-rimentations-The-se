# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FSDXP
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=1, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.019788718658882855
0.08289807145792392
0.0398357720507845
0.05469815189027082
-0.0019264655006913274
-0.010684115812944057
-0.01907288628275928
-0.03228202987886328
-0.008059822881774535
-0.014925889006940436
-0.0305734352278651
-0.028854448784790116
0.002682396188451929
-0.0031929639826939445
-0.03395824214558516
-0.0014045941429493758
0.005608834061507565
0.04592570022331209
0.031387603045802064
-0.011141863169395274
0.047492416275951084
-0.006830844616776659
-0.0003372592525405643
-0.01699178289475959
-0.03450998222126557
-0.014464253714984537
-0.004751307299421564
-0.02836258000339098
-0.0033943475973385183
0.015464210451681034
-0.04846813390099087
-0.02773886961313311
0.003688307286411877
-0.027379910522221038
0.003912261341445562
0.022655139885209355
0.0017046469252622
-0.002009864533240814
0.005003038542186656
-0.011056852471595007
0.0059200045221745965
0.004761948709349542
0.010327476933215726
0.026966638290587437
0.01240110937382413
0.01958942342965455
0.007582349657190424
-0.03144979778208758
-0.011792945065485665
-0.028873574740227722
-0.03397603672531929
-5.065379243950931e-05
-0.00025901682533959
-0.006436249540800033
0.0028803098762251646
0.01225124580307738
-0.010674167053548393
-0.0006084815281385478
-0.0022928901447569353
-0.020243237349365503
-0.005537527445144957
-0.013105502625231958
-0.016647566683287235
-0.005064365334326139
0.006051551890424115
0.010134345147857035
0.007242771748991806
-0.005901444531589228
-0.010255799333171906
-0.014937315021831371
-0.02735187665995282
-0.022013049445823496
-0.013568030645739653
-0.025935577684319124
0.01119215685222521
0.013730063747573419
0.015638978244069707
0.015959478881314437
0.014628047069484628
-0.003930254664533348
-0.00014165651455819075
0.02430525147544918
0.004822463075842977
0.02350904082893809
0.021884284630034948
0.002328982967063844
0.002055416072438465
-0.0018892015800459428
7.7762645637086e-05
0.006704442118474317
0.004050272289246806
-0.0003298846289054749
0.0008718800432048718
-0.02066315900145037
-0.010449797274524341
-0.004070603510725246
-0.02077156948683375
-0.007137916563301974
0.00013972557512930415
-0.011950360149885111
