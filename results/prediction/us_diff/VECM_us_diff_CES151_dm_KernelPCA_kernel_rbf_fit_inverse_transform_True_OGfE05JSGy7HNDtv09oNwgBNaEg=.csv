# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; CES151
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=7, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.021824656248929764
0.09686008785925715
0.023589931236104054
-0.05765239809621634
-0.05809109380096886
-0.07057745816709853
0.094329255095498
-0.07137600073176703
0.07130770467007244
0.0394065995424425
0.11164458946267206
0.023426484658942495
-0.06532263718370522
-0.019699152286293507
-0.04791992787779357
0.12367515291355023
0.05930067477785157
-0.015303881024507335
-0.03603709762244851
0.018053632925622655
0.04405610182916989
0.13155461629692905
-0.042908771227382896
-0.048638137337362636
-0.08445475511158222
0.023621754453412254
0.035975446445543624
0.005648098063260096
-0.11025978833528527
-0.12612719321730229
0.12088954164069296
-0.08076625809029542
0.07084705609118133
0.0540704604606816
0.026270585974684946
0.03245905865821656
-0.02298514933373102
0.11775560930796435
-0.07117514866341154
-0.06462293028233532
0.05994221945080574
0.11902366270414791
0.038493492987165256
0.06145302727951015
0.010374778962077287
0.0024788869339831612
0.02094280193232717
-0.032076903170916354
-0.011631787344796566
-0.02993624652145884
-0.03254744391559357
-0.008267494007929775
-0.063763060409598
0.05747426669762057
0.07904059984069944
0.06867325847107222
0.025102014618666117
0.05130582991809909
-0.04105311196200086
-0.000651565877611773
-0.08768843594461095
-0.05922331128793339
0.06573846985326459
-0.04123462506552407
0.0003930983743171586
0.07277022358545808
-0.025018940707903795
-0.062099653276946984
-0.03086409734893132
-0.057993316961596826
-0.0638624279971907
-0.014006491774459347
-0.06162837922068043
-0.07231217684713374
0.11498935614738093
0.02063553958848499
-0.12258625868195898
0.019701368078763586
-0.1695453462296486
0.05700509337705609
0.024344175799418704
0.06515493176137645
0.011561229501642196
0.01223091056359938
-0.04426136973182344
-0.048080200296679546
0.018219325128135337
-0.07058111742339365
-0.0012192996296162065
-0.007283229354158418
0.06093431192539782
-0.02166741397558552
0.0426813142632618
0.03207664546264499
0.05065245212467089
0.08643910913772723
-0.017706667593740177
-0.06944613220730171
-0.034233625511600946
0.045657741737021834
