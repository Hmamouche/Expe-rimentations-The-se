# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; LHU15
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=1, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.08629754304097768
-0.07197646880781966
-0.06747595928818767
-0.042374177967138486
-0.01426606296071587
-0.009324717605709743
-0.0072702780978281915
0.015383742827369387
-0.0023404806625165267
-0.013313101245271262
-0.010429983247817399
0.002771631008622927
0.005481616858231623
-0.01455264824350311
-0.017521959811634603
-0.017149638118014034
-0.04386582063118631
-0.026879153045943308
-0.008805332408706783
-0.021856415563907824
-0.004018370278870913
-0.024835091653326383
-0.0064308226403377265
0.007390199520413976
0.03741173138994831
0.022484968706172087
-0.005861572605930565
0.013899003208057576
0.0427417364498157
0.0473843723774415
0.061945288279675
0.048760857346983724
0.002335913933946613
0.04935138751063428
0.03413385008177058
0.04340541765559453
0.016757510048550982
0.0012032209900643513
-0.03351202085606787
0.015583360485424394
0.0020508959045103248
-0.016163655816593972
-0.023229484194804043
-0.07615656816084769
-0.023862472237830813
-0.04697067426333438
-0.027074409517601178
0.026397158559843915
-0.007105480300831433
0.043051062951128455
-0.012077298856526576
0.011295588114006905
-0.0340401726954173
-0.02428707011429242
-0.019004715925429603
-0.009597651851476922
-0.0290997408519911
-0.03535005144559261
-0.013279295405118664
-0.025358726518383917
0.026846069075425822
-0.0037579496862827147
-0.0023273032702106327
-0.035817113107355136
-0.007652566699552717
-0.030298777161027392
-0.03172369708816485
0.005652602207905676
0.01984281122706986
0.019936077084977227
0.05584816038581583
0.031114901897962986
0.06528294919673565
0.08268516398446356
0.04229022484589272
0.01251641269592242
0.011586533794705138
0.04924490514928766
0.017330743226537486
0.059464925314958006
0.003270282803660629
-0.03248057342365082
-0.05119526698965508
-0.03427220200631502
-0.015415311445957947
-0.007623164182873425
-0.008351026661210336
-0.026040163907900576
-0.03454246848552831
-0.005930375036158401
-0.01632750026894629
-0.02489984111811872
0.005859848519065026
-0.00237643926536158
0.01747665770674208
0.0015497410851449817
0.0421288698451908
0.012442841618905832
0.032291739948617684
0.06423752154717507
