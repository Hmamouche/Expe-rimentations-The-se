# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; IPS306
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=12, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.018798091359751017
-0.013029202446798498
0.025038242914002647
-0.014759446913287788
0.013474559101847854
-0.022037108096991137
0.06613202675772825
-0.044229876837640944
0.005386586186206042
-0.029495893526256294
0.03477405054382271
-0.07446886748155063
0.03907792760099396
0.005467043089930361
0.0041887397797670724
0.024896108672591767
-0.0035532233863286543
0.06024243773435252
0.02368460604050373
0.005464939641642451
0.012920399780224119
0.002357076996264181
-0.035569633449408534
-0.017267907430169392
0.01667425287757631
-0.01838169071304837
0.012541412814425197
-0.008866211178382431
-0.0060021985924614465
-0.01956684273804158
0.07997445674989957
-0.03583604667965796
-0.006400599475696022
-0.019629360870200896
-0.003020206823886675
0.008535648383866778
-0.0018498357381444772
0.05564705521333188
0.017911606026344288
-0.030806986852637667
0.026721679302540987
-0.0036489197467717803
0.01215839247753949
0.005275949591270286
0.030051871240226027
0.03074881378203037
0.03674309707856102
-0.002451242365668337
0.022507253815230867
-0.027170711518043025
-0.0368146166061978
-0.004705624307818118
0.035459484076755726
-0.04139398959660985
0.04082413438227748
0.021028566737789342
0.006953601933618412
-0.00478572459064099
0.007784431927403185
-0.037702182149620525
-0.010849554433686005
0.0013410607769301398
-0.020017820096562746
0.024552119816245693
0.026046573002989716
0.03189658124012367
0.007256616514821332
0.01203429570151215
-0.028315860071535477
0.0251150796874929
-0.013914010049956946
0.013668780316472236
0.0022052573820443237
-0.0382798106867415
0.017325072771225934
0.01797488081439519
0.04528426843493436
-0.019575872996764693
0.006243779934934584
-0.018055618822320194
-0.008100303138955028
-0.004125156744509452
0.02746102441072077
0.010683579873643934
0.027011866675278964
-0.004553022391020373
0.019660506147507487
0.01622515623884801
-0.005521038151004615
-0.022711290008192383
0.020848954508164232
-0.011085318395583792
-0.001459630237145501
0.027175846819989725
-0.04891292375246723
0.0013436048003687955
0.031784810871416956
-0.03949879065591119
0.01792664923569424
0.025950482522350844
