# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; PMNO
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=9, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.1476098501131798
-0.061248272400803
0.13678832241328828
-0.17622680174651284
-0.06794186486139989
-0.01133327944368999
0.027036565575887753
0.07922058957929562
-0.08419081596100446
0.019763356213768016
0.08881979488873418
0.09983576514667657
-0.04701082974537113
-0.07547549138456175
-0.04964040919660366
0.03088529607422423
-0.02421183859293206
-0.17722693046796048
0.07450006634887829
-0.01681186958758514
0.03678279404794452
0.138915987358809
-0.19838710084947056
-0.02217673667590584
-0.011376816394793179
-0.011743136822944839
0.019430421076679916
-0.03612683016664819
0.04589966662447058
0.021916174815997752
0.1357634365565432
0.0752857771596045
0.06669415417204705
-0.018289450146025697
0.0032716193425980644
0.06377466146763106
-0.019628319630400785
0.08852711205346415
-0.08095464939576545
-0.06301384725091316
0.01424860422726655
0.03763105542943673
0.12689759787983956
-0.08770600446913156
0.0347175556442238
0.04649049434388814
-0.070505858297556
0.10540776684765207
-0.008229250581499082
-0.09217640273928195
-0.05063355301888896
-0.029290734870514555
0.02898368855554356
0.050885460205955114
0.038493402342124804
0.05195385371518716
0.09968656278127092
-0.05712049548112805
0.030042161956124313
-0.01397594406637677
-0.11430363126908441
-0.016531130133017317
0.03234975115243268
-0.030567230554527675
0.08602212414878753
-0.012206032060904565
-0.02779774638322013
-0.14265406456617347
-0.044060710269154396
-0.00566615541161003
-0.03415014715852135
0.0625884700712963
0.053879130848223114
0.1407307655704268
0.19228280338039289
0.06153021365999764
-9.70990356046822e-05
-0.1308093549916616
-0.1482495715112952
0.1559006130022779
0.020090645831739418
-0.05205469840109646
-0.0524758399214416
-0.03605210272369548
-0.07663291319607353
-0.05203130851971513
-0.05666607562058899
-0.06537212337446648
-0.009058943644214654
-0.07150248868570179
0.12111181397286622
-0.1250385518851546
-0.07613884980118912
0.08643902123564404
0.026313054815938833
-0.04646525912421249
0.07674279965168453
-0.06773755605658442
-0.14406847456076016
0.08518229621855498
