# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; CPIAUCSL
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=7, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.0011768290775672837
0.00719452333446992
0.007656613440587436
0.007687680566724046
0.006409037101762788
0.003484227920602792
0.005380201840359339
0.004338967114818219
0.004611017291957967
0.005117204646573489
0.0033919375433864203
-0.0033624768602989108
0.0026942019781539006
-0.00010948930379847525
0.008099556616939993
0.004670653582807406
0.007317716538032583
0.006199251954476241
0.005472527015104394
0.007308803743522895
0.007032552026671131
0.008324145619970007
0.007681663225695307
0.009261211437803453
0.005709636213578136
0.007332629956157629
0.007524041823661898
0.007703150703723119
0.011246523011973118
0.008811431789268985
0.009042107972883504
0.005298683445978152
0.006190555364257502
0.006934890177427856
0.004544855359768346
0.005137948396921408
0.00582481960170499
0.007809147556038653
0.00593528859444495
0.004189540325165273
0.003307142982870193
0.005051865638508239
0.004961764475506467
0.008463420567083269
0.005506501240981503
0.00502823690274357
0.0070132887558671505
0.004652926531060397
0.005623427603865032
0.0040172416389431
0.004783487652313132
0.007725468026266344
0.005266847880289329
0.006870865903354422
0.006143884727601282
0.004685605267334418
0.0036770923628637407
0.003804831004025514
0.0034668341733345267
0.002944259829970551
0.002871432494674776
0.0035803135974556205
0.0043078730451342456
0.005646652221464964
0.0057789612813901135
0.00792319274018819
0.007144886216016081
0.0072420913371905645
0.009276876364110714
0.00614414425319478
0.007460360042294677
0.004994353697419022
0.006278176168506221
0.0004747021774560647
0.0042395433129595065
0.003326442619599202
0.005256606047912377
0.007017135742293061
0.006630735845472541
0.002044803404944231
0.008281364608048106
0.0011989962013786589
0.008354943059389263
0.007290894503893037
0.007297641474849673
0.008568551604506317
0.007591185651584677
0.008061123826938325
0.009104418908724926
0.010624251872780783
0.010725261464428107
0.008827180665192145
0.008843947963714155
0.0035489339796115887
0.00697309542654395
0.00307024347054387
0.009233153017480783
0.010651713781396729
0.008911479164057951
0.014198778141156896
