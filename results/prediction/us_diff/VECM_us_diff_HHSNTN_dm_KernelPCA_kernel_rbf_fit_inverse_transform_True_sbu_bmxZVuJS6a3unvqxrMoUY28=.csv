# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; HHSNTN
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=1, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.06651464195039622
-0.08233373090264766
0.026119772779887182
-0.03344516374935352
0.0069145892455097085
0.04214885595336064
0.012555384066615303
0.0021973059585841384
0.01295660788917539
-0.021893782952498688
-0.022618718170347083
0.013446233198031267
0.012716120850303384
-0.014096280894246669
-0.0437273319782514
-0.05270621022969396
-0.022266931038128455
-0.037115007645791134
-0.03323864870179297
-0.006343228507344382
0.014376659536810672
0.042108578299879254
0.021427286558335173
0.02133340415801621
0.01913572784132156
0.013241041214710227
0.013824114051032527
0.002289439241190794
-0.04610275502714277
-0.15065867956751838
-0.10670473440319142
-0.0035709759766137635
0.04848376582684915
0.013805448176428717
0.021078153645717468
-0.05028700644808059
-0.01993073998294704
0.0349446276319374
0.034756462314512925
0.04542194841879815
-0.03060625626194297
-0.017659507926820153
0.016850659914448535
-0.016799893102323982
-0.006035414375784292
0.07493227306501504
0.01666734406645846
0.03530452879571234
0.029164668226043335
0.008336005621921916
0.03399778365383252
-0.021119618317421915
-0.002184793335201522
0.03690459206952268
0.01452201638760594
0.03943950535474517
0.06354882949529721
0.03038426794951738
0.04541283606224339
0.03771258188101397
0.01487931896496876
-0.027965765439366054
0.009985752108654397
-0.04287823854711954
-0.0066195389313940395
-0.022416100290546248
0.03403101886663963
0.013606760282592311
0.04070440715210685
0.00830372256651913
-0.0320155899861301
-0.028697798305999715
-0.0724606918732689
-0.05846757250030574
0.0010904357842398751
-0.03533133898516024
0.019983241299856652
-0.022087920458928677
-0.06977511325265716
0.018126673529018894
-0.049123768761182726
-0.008116770992323859
0.005064079024859201
0.04910018916190073
-0.019854187280345395
-0.0055034217207800065
0.0031961375828903106
-0.05452052644143755
-0.02133829702466146
-0.09432500058223342
-0.03698906639771121
-0.052677410793999915
-0.012734649089407097
0.03323150514529288
0.078850793791257
0.012496766617441965
0.023980091032776346
0.0044614521079216485
-0.05780984251378719
-0.11479874837293333
