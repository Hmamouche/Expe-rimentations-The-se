# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FSDXP
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=14, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.031064356204180747
0.1853603104158199
0.06178847656991074
0.12368702536856246
-0.06610436106919611
0.019690227736022123
-0.04456579943341283
0.12314662391373986
-0.08425472321328108
-0.21660710868727695
-0.23069661465812802
-0.13398144545856183
-0.057682472716444486
0.029253197240681386
0.007843885805077819
-0.02825290411933508
0.0713416949927856
0.10965252473675637
0.04147957536599642
-0.013538038762374176
-0.012781484928827056
0.0833133586369649
-0.025377551585882126
0.020540239359714343
-0.009354184644423481
-0.052082470539637854
-0.02522411219086226
0.03691853243876031
-0.02218128421860744
-0.032690312327256194
-0.11516013076950499
0.005308363014886193
0.017163503140651612
-0.007290067253876403
-0.04434171758793909
-0.012381366419481097
-0.05690074950546417
0.012227613068757226
-0.01746920507287933
0.023320999164148357
0.01829432146492162
0.0050566232110051505
0.05777280918969116
0.04252808185106001
-0.03839437913560051
0.03739471223386
0.029226983862784724
-0.029979166983764616
-0.07459530893007972
-0.053598714487910926
-0.013473838858098858
0.03845456515853844
-0.027410646364799368
0.01751448293299364
-0.09344899535728775
0.02066950749695099
-0.0242967568260862
-0.051184547851755705
-0.022378873959573612
-0.059495189697984584
-0.0042467367662325835
-0.05015354246838123
-0.030033216934660344
0.060652645191107676
-0.030095829455531783
-0.016819248281759208
-0.06110361712583364
0.0002822415438554376
-4.3074851683413615e-05
-0.06495393809590341
-7.650017383340271e-05
-0.013858807525826813
0.05681036796610467
-0.09301171940459413
-0.055965328796515695
-0.023055847113508127
-0.04412891530656539
0.04452693084526809
0.04611300872911472
0.05501502728736118
0.007486780817154827
0.03580234691257239
0.06449424974945128
0.01971174920365085
0.039661540778733254
-0.027385463741898763
-0.0414103925262326
-0.0036095025401016168
0.033860206713686364
0.02425819490933825
-0.0001234539772805327
0.01428683904707638
-0.03640291959746442
0.006967058523192199
0.011628031027651427
-0.009158536249148617
-0.0040608497100526435
0.01916064318262036
0.006492710495678795
0.00160717651090853
