# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; LHU680
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=4, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.0006667971632437009
-0.07534891484284621
-0.0622322237176792
-0.10017196540858636
-0.014553899350109194
0.01441287429062944
0.001521978412423101
-0.004416066315134236
0.0005734104126637561
-0.007862254723068694
-0.03769437791673573
-0.007130156159830784
0.009197180133779941
-0.0085347518958708
-0.008846320178274242
-0.027266948570705243
-0.033902735523501745
-0.0484672879782786
-0.02011691230559532
-0.03872273603984627
-0.02064798379121765
-0.020666501223610807
-0.02963706575345401
-0.01957929864809254
0.009070454182112667
0.0063365698141058194
-0.0026592999820167114
0.011030684327656946
0.031043175066886947
0.021904088891373186
0.06808745156299964
0.08070342424883432
0.03449569265553934
0.03425130652562226
0.03699230340284294
0.05060498130940673
0.05768615081215574
0.07287045695039021
-0.009797170621609848
-0.0040448858827957825
-0.01745277381292518
-0.0010551775348451964
0.017089142636177897
-0.004945094747306315
-0.01698854790833241
-0.02890372062917223
-0.059971430214283625
0.011420146076611156
-0.03265647447356314
-0.0015372208147644987
0.017766951366937716
0.03694059487043655
-0.024119103266298805
-0.028603866607706065
-0.03571934766711518
-0.043599392390169314
0.004398214650515642
-0.046323477510865546
-0.024190323075802708
-0.02895544435159665
-0.005659887391816012
0.005354084534483262
-0.03352909938993616
-0.003067832477978176
-0.04317978791507003
-0.03849405741302076
-0.05487118574666077
-0.01098012039877634
0.01365875454597536
-0.002616712353608846
0.04115987724909408
0.03907921751559679
0.08701717880890746
0.0795659558767916
0.0524420000877158
0.03531934901937965
0.024405684206784413
0.05679355622625364
0.022180633376972674
0.0892933025321134
0.04790472897813274
0.003317329874848959
-0.0010067366282788068
-0.012169978906192126
-0.05150254637905533
-0.0031019421500498388
0.0028033273318427664
-0.001304977907694141
-0.030837313265097623
-0.0309511524101102
-0.035248491379811266
-0.033010615323227466
-0.009458129853020142
-0.014377324459442998
-0.0011469366081446365
0.003952454513776097
0.022308561016589526
0.007054920271565323
0.0159892545785353
0.02607985357946677
