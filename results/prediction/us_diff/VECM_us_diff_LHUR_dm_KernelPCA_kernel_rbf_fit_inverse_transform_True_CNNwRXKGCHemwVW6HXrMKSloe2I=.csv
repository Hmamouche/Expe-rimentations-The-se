# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; LHUR
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=8, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.0025991062337469215
-0.06023049678004501
-0.12814965648291232
0.025145771220298742
0.02059435817440118
-0.015935880390354894
-0.03919419854183251
-0.02071804206749265
0.018909495244915482
-0.010008225698036362
-0.014902450350739053
-0.00685652006471181
0.03161366487828912
0.01639982976511624
-0.015654484466620655
-0.03386298954733891
-0.09354738572247737
0.005738624770470981
-0.052863941504731435
-0.030941204447912186
-0.04618941815509836
-0.06701841913574105
0.01997276991149087
0.020512469861387064
0.03514935158497337
0.003245539966263219
0.006719219559603402
0.006300713699204045
0.044860693117303396
0.11253251288741153
0.09052434430337794
-0.0027290640077133584
-0.053353372336937765
-0.0277254473105093
0.035780587661936064
0.04917559382111575
0.0384992319982703
-0.03165054860780885
-0.04332215036455675
0.03768183130792941
-0.06600483045168351
-0.02619200723812931
-0.04559133309696771
-0.04543478572651816
-0.02385941980789279
-0.030437395140950813
-0.01854469198439325
0.013679134553316112
0.007903266747081434
0.017289734464931754
0.016729423441157854
-0.023985814721020828
-0.05421049259655987
-0.031239872192418992
-0.05248391016337332
-0.010445095840258382
-0.03194837184971505
-0.01274513374626568
-0.0040711719948073564
-0.015375157983434477
0.03714753320046238
0.008029174346353973
-0.031357994750662206
-0.009607114696414933
-0.03587043613807437
-0.039629935449397566
0.005984208342434178
0.006534554344136457
0.025213660646620232
0.0021958783008674725
0.0650233484839008
0.036488341677350565
0.07393029679535006
0.0829133020035892
-0.054410294612410044
0.01226113117343066
-0.02008057536454187
0.035310721244823026
0.04817262955572432
0.012977928887260324
-0.021247565051171768
-0.04641668270413218
-0.03259718936313601
0.0011239552263833854
-0.01487157089858548
0.024231209778658738
-0.00023951581421691182
-0.009027447777901498
-0.02000696413510185
-0.0026098444468770697
-0.04783843608870042
0.014458087503189509
0.011584509208646936
-0.020874582304163537
-0.0269570787936163
-0.0006999668775116135
0.01913843982095048
0.05541650591611724
0.046884676810156754
0.048164430353613265
