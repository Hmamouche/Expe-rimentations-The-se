# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FYGT5
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=14, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.00984880632561562
0.12078976689125605
0.15566799738913334
-0.13454376237966076
-0.03140660776305686
-0.037461198438208804
-0.020540147079709442
-0.022706041126907037
-0.03229183730834928
0.09480638685938947
-0.12123485150670767
-0.13889813001260587
-0.08864087974487868
0.001063062782716978
0.0027000178471110955
0.08536992796901614
0.07261800748035914
-0.0388414605545986
-0.016192582517936267
-0.0878367719423673
-0.05270805066323463
0.09491084485459966
0.022588265045634666
-0.04768105182248332
-0.045579972302093624
0.007660351231573907
-0.002631130100781462
0.0337132273387881
-0.02413736181119298
0.027237115859260988
-0.057152323229456715
-0.09892951862199142
0.05142949265772064
0.00019270590575263397
-0.013028050564715461
0.007926870569650711
-0.094251171225627
-0.01786286917321539
-0.023967275498190187
-0.02407472354526883
0.04935665906578529
-0.026798792680568617
0.004866250013947061
0.03842229464323782
0.055159257243855356
0.10298785903107523
-0.017717564695365142
-0.019867285102450477
-0.05327699440314157
-0.022196288738033056
-0.0550474228175569
0.041199985481511964
0.05312633297243929
0.019671811555269026
0.0011314458251593905
-0.015951584529427458
0.017210108160351685
-0.034527796076379576
-0.03935877306397514
-0.0160673118492389
-0.04668130558979869
-0.06629690064838864
0.08806620931702729
-0.013199328473975525
0.04797142785066015
0.07094019659986578
-0.013104366563755434
-0.06004773789620349
-0.023939944869176746
-0.04927242167433653
-0.04948640130639467
0.027898743971470907
-0.032322105141482024
-0.03874506148354578
0.04287230344176155
-0.001334623579165856
-0.06146712353110257
-0.053220679709014035
-0.07704447354828567
0.015032577868838562
0.043297626092597026
0.05185975634048272
0.055166515135164684
-0.032151023740500745
-0.0580519384658197
0.02592361838050158
-0.01654110888940831
0.003597473544616296
0.03597497029865823
0.03649456736054029
0.02241138942602372
0.0369905744348958
-0.003957420420717015
-0.036310499601863534
-0.0003599643524773849
0.003744397265661995
0.03466943572536179
-0.05394700979951837
-0.0780410496642275
-0.012271795141747886
