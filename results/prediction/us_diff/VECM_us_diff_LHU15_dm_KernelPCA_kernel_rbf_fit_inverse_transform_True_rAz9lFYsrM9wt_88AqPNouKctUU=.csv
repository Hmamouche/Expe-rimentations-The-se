# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; LHU15
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=5, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.034539183535589234
-0.105556463507543
-0.09285561626250746
-0.024677965034745526
-0.023788360928541863
-0.02448473505297498
-0.02512045095159023
-0.005585940917881933
0.0016405515791401348
-0.022025503664246795
-0.023323351060947656
-0.007087851977926012
0.021117747661953727
0.0022714426446742023
-0.015769350527627464
-0.007387121229377773
-0.06716045206440069
-0.028121015610937578
-0.01343446448092293
-0.027892001109607687
-0.02038636827624906
-0.043749208393948816
-0.02690357765873684
0.0009790057082386741
0.030490604546571657
0.013181014042890415
-0.009957707163522784
0.014826791639786377
0.05488399799746631
0.06821102385274495
0.07886272265843236
0.04914409901945612
-0.025158913756863653
0.062437293604514214
0.0597316631394052
0.06463680449910916
0.030202395806901365
0.025728120986070406
-0.041993963815635624
0.017315944344076708
0.0017360769715283782
-0.026406020521720516
-0.029916480901096915
-0.07761909712224288
-0.011358334053353545
-0.041289847730760554
-0.037349616144403255
0.01765623396051344
-0.022766440639048816
0.030084726282880097
-0.019088494432233313
0.018598468409486625
-0.03835223714507768
-0.030506450317769263
-0.02128829053496733
-0.006013855525251616
-0.03988748058806699
-0.04726056170738638
-0.017302739930524266
-0.02729688955924807
0.02275278832638626
-0.005818349555468647
-0.0020924869287723205
-0.03444365789619734
-0.009554568179685653
-0.03306305726495176
-0.034075770375752094
0.0034319363774996943
0.015049232822305424
0.01348316877956616
0.05709345904010171
0.04357493983415249
0.07000955492503358
0.09434151704570548
0.05687776618007136
0.025048186058900242
0.006075964767627767
0.06462265556425663
0.026074702441210375
0.05978092727296608
0.005293944106473621
-0.0340141060710827
-0.04732481260990748
-0.02268771426457176
-0.016499727023243017
-0.015159965547093788
-0.012811872567763626
-0.024978370807584308
-0.0381400011495831
-0.005738356856748431
-0.023605450328554635
-0.0231343373658203
0.0032796849798307744
-0.017690101788952636
-0.002581262763644212
-0.013415930195196914
0.043213098555863916
0.023183480070451533
0.039273424450252774
0.0755594440419316
