# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; Sfygt10
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=11, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.031566176424014075
0.0013814065090457656
0.0843117591796151
-0.030762132427035155
0.10987520947546384
0.10562329916950203
0.000885134225094994
0.14920671422025172
-0.2129294779391589
-0.10458849829729533
0.09101083248876118
0.020125297258730284
-0.0028938203996382306
0.0036406880026958466
-0.12422604135988977
-0.08412379250378851
0.16087888374765055
0.222916746249345
-0.023287809667124237
0.08587162036349347
-0.025513367194727823
-0.055451810728994085
0.13872020117998524
-0.12534200408284074
-0.04814577610946234
-0.19208555856463538
-0.1468880453593764
0.2052196822629939
0.005458618853821134
-0.12184222845587075
0.21060887676156567
0.12277973522918634
0.0803843994725259
0.045872546441968504
-0.1030726404361206
0.10179434771008566
-0.012955845384737555
0.12006075651685427
0.07848698041698737
-0.004820683478073214
-0.1722701704134636
0.01059640383042735
0.010092953553054944
0.03735238937684196
0.013193061021705567
0.006924750263230896
-0.07538077426323761
-0.029140808850300397
-0.24849963202685146
-0.04571455918281609
-0.03930514826427774
0.12859868369702773
0.045126723858782566
0.0036839160925435637
-0.008127296921631097
0.0282620730069538
-0.04472544018488888
-0.05930630162058116
-0.02900273187919139
-0.028021062761628402
0.029824658929969963
-0.10570930392618501
0.024258750256063524
0.128675000879858
-0.12407176219545912
0.03463009913651432
0.029098931483961803
-0.06058380947465891
0.058030557030583525
-0.08415879068089469
-0.023433692527811436
0.03597125798824088
0.13701939821736397
0.22002387609445231
0.001590665240575967
0.1652064472597788
-0.2901708821549549
0.2105785371420933
-0.036082635736767496
0.04068761336646736
-0.12447787074985192
-0.0061181997139519075
0.05155605159928634
-0.03656297046788667
-0.04437218620268034
-0.03975156910229042
-0.05297041733896466
0.04885078858783215
-0.0627854984453653
-0.05567995088999012
-0.03203141375855596
-0.047439738345577714
-0.0897675244081705
0.009228227634152049
-0.06097699808206993
-0.07682428168075514
0.0089987092164513
0.05556678428965222
0.09156405122923739
0.029736905944640058
