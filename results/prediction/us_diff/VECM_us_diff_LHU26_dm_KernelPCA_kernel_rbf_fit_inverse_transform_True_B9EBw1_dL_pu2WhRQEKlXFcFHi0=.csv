# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; LHU26
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=10, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.046816618151722084
-0.1512828868911543
-0.0643152689069296
0.060917854184133285
-0.04400060041841446
0.0328794867183034
-0.06456321567927419
-0.012407013647591943
-0.01593430801360155
0.006293128353474419
-0.01078479496827934
0.027358223638317223
0.015962854255219296
-0.028024717541722168
0.016104242998191635
-0.008057500285043432
-0.06611902391265922
-0.028572735735753287
-0.004377385099191718
-0.06514869439125952
0.002453259100577955
-0.07199687512751897
0.04979797974708118
-0.023032599924108894
0.059784401190093345
0.008597108584409714
-0.012556318327630803
0.05732072825242947
0.003167214356539048
0.11271077325405338
0.08815877996907434
-0.008037586093935765
-0.06741943803028941
0.058980308781425364
0.0679203097571986
0.017731746219596927
0.04366077003041386
-0.015963430000318763
-0.05288551220938549
0.0022632944983357675
-0.022960146506099375
-0.015556793078186564
-0.017671781892121957
-0.056653496032509625
-0.03131021676359169
-0.05772116219394177
0.002191195760813988
-0.002451682559565467
0.016250034870889767
0.07321480993848614
0.011588706302493527
-0.00839421074560782
-0.004014263906181148
-0.07929882261860745
-0.042165860307485994
-0.005212007140751408
-0.05168607267610881
-0.027393626565375157
0.027536523366234317
-0.018926489074602537
0.036212007453123266
-0.024669162146422702
0.02085997882364008
-0.04860041661919884
-0.028099906517764086
-0.035993749796689636
-0.025617274329024754
-0.0035167211962332553
0.013336367645042035
-0.0009551022495211114
0.057603620826850825
0.08779656499210076
0.10308734902120398
0.12492046100730203
-0.009359956920946839
-0.008789355688327975
-0.028383161009479567
0.048253377245157214
0.0602726862874329
0.009917238326756457
0.006622868439448198
-0.05419726370656957
-0.014225492438494792
-0.050874690975460254
-0.007104126798760529
0.0005983280878821898
0.012171310399465222
-0.024416303509323216
-0.014835812706779462
-0.014394168738498038
-0.022896344055570818
-0.020766299468072975
0.06535459166126939
-0.0638590325011916
0.00633997678479929
-0.011506332836314873
0.03647349399949181
0.06192635303842031
-0.010622499160697572
0.07833107174445503
