# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; HHSNTN
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=2, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.08778452760588903
-0.05402104445305886
0.023577570635470703
-0.01860625072471058
-0.011156752028684106
0.060983773178099686
0.002495739135326063
0.00630652797105454
0.01482159658983248
-0.02978991007694356
-0.023663589366996418
0.01993318707455836
0.009644167521515349
-0.013291892760322995
-0.05026941374074238
-0.04890109947827101
-0.03276318283316109
-0.02893496779181748
-0.04030199148283754
-0.009815881514188078
0.01170954606172895
0.02673122608706635
0.02516707671828839
0.016771182397679815
0.021175095017782912
0.014179496954225834
0.01157870777768865
-0.0014707737765398687
-0.03394769086387138
-0.12416274845165413
-0.07982651537903558
-0.008222742812268556
0.04588896989460371
0.032615277900268316
0.03383080078932536
-0.06023648677491956
-0.02514894578112932
0.033040421957410554
0.0150717796947386
0.037184609302934726
-0.022892309528311804
-0.004232368062713151
-0.0026338027153327568
-0.028013543709089466
-0.005133243131242686
0.03412302158301949
-0.007811374788831135
0.02731813086619122
0.039428596864414064
-0.0014300390686557418
0.05215401331654207
0.0019493063290831926
-0.00921537385549656
0.02550042032075042
0.018084868286975538
0.024658488796388187
0.045092405255278176
0.025913802654822085
0.0393918854317668
0.025561926742580697
0.01600695401340355
-0.01906939321379743
0.015366075394884453
-0.031577567880133064
-0.010478831125341054
-0.018851477268012336
0.02513203507224942
-0.00929831271720089
0.02611731999560017
0.009088979209829274
-0.007884567026960117
0.020009893635664952
-0.05093375663513181
-0.022529799200351468
-0.005816117129884845
-0.03429482655462728
-0.049838515538618
0.009121674403836174
-0.1189203324889409
0.021233476574050402
-0.016431750564222743
-0.0036576091563266834
0.003838290346054596
0.051830642320357105
-0.04356103461849618
-0.017052226385930118
0.0028373068075711015
-0.056496854211725483
-0.009438252081644963
-0.08144030211400556
-0.031084531287062574
-0.060045951418057575
-0.02245425786811003
0.028501536466938975
0.06776343100492967
0.03754846179506938
0.033282583344099254
-0.0025573904089864277
-0.03663109299479642
-0.09800379332928871
