# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; HHSNTN
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=9, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.00665870997337017
-0.23894226318146583
0.09383578615812614
-0.014158253218419767
-0.030558342296433224
0.1461804365262328
0.06907873447118282
0.011143556843331813
0.04395750661687038
-0.039843473354430226
0.010553732442245018
0.034621060646609955
0.019897670988569625
-0.057282463828392194
-0.07492085351926484
-0.07155216868621718
-0.09782239554864157
0.023948272573469535
-0.0424922469538638
0.0427078349376672
0.013094537535043804
-0.0049524003504165335
-0.02033418019319967
0.02719084500693117
0.026500146113909318
0.028560643974153175
0.01705028494359996
-0.03489085189992245
-0.07913811421835842
-0.023277318906344706
-0.08353194645457392
-0.12865587664820033
0.06902331175047721
-0.011074637368329598
0.04945680434598504
-0.12120140130738442
-0.0019101681068142383
0.10745967653947883
-0.024771352713651773
0.029392823664749684
-0.007313642230993782
-0.022489377785769187
0.009355248708760335
-0.030278375090174947
0.04144969501751379
0.08704378056607184
-0.014447375495403543
0.11294907965987307
0.0839688223401294
-0.08100967441376146
-0.059302770261145665
-0.031310765510250776
0.044465880342978756
0.045459302517789235
0.038462545240914414
0.03531810450608369
0.056798686472909546
0.07159899277481824
0.058195023573322846
-0.0020140313283375164
-0.034673697500279634
0.010649530398158835
0.010633477341197297
-0.0824546797729876
0.05548579332755103
0.08003651333114103
-0.019407156882352734
0.0027280601453209194
-0.0067449116458539946
0.031734405412475734
-0.03740121666518911
0.0034063133024815964
-0.14347219965560995
-0.06306174081451538
0.021619373275606465
0.020983332898070556
0.08549382627738594
-0.0015583512701077898
-0.23282767776451357
-0.04447818172685946
0.049671228430480094
-0.00824305738257157
-0.020970236325200283
0.08092841118948986
0.006973693138947808
-0.0016260917907317957
-0.015739497764423724
-0.08375260656799195
-0.041696684150852474
-0.027930574528777743
-0.029334762520981536
-0.019806221471128684
0.009140586716432843
0.017064528648659212
-0.032643915057811695
0.11467525579810488
0.09614679892258589
-0.06851598129299342
0.02940705772494672
-0.08464231795495468
