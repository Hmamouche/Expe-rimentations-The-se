# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FYFF
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=9, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.025570630501693812
-0.026834704771014524
0.058901511426286156
-0.04302660486461126
-0.04660712166618679
-0.08877515417422409
0.036013097106870645
-0.03904540111323896
-0.039674162103593896
-0.04486361027167793
-0.009268774881257227
-0.019072839518855484
-0.04983866890820719
-0.03667410648934673
0.026684060567323618
0.010702413415503091
-0.006427511056779787
-0.029579744533474124
0.026897010885324334
-0.00485584292020851
0.03176145185199712
0.09610704416220882
-0.025867372270798897
-0.01988411993652312
-0.022251319627048974
0.014767725479151016
0.04037662684590938
0.03327225380440832
-0.101915223930458
-0.013842555648497828
-0.08444084718829503
-0.0927076698613262
0.0013789138305987463
-0.003241039149721093
0.03488474122506975
-0.05831925977886389
-0.06318431411766307
0.0304477803143535
-0.038377922982230264
0.017085408626924446
0.09964353934762782
0.0013354469457702483
0.016233907948761013
-0.044444420241329136
-0.012931751000306186
0.08395996216528111
0.029328090555058695
0.004047529433737724
0.022625089144626385
-0.022774461649184934
-0.01577593089445793
-0.0014796616543292567
-0.020495730483597957
0.0031566634521383845
0.006319910402995826
0.02936632499591626
0.0026655958095479076
0.002129903909702756
0.013197681925842189
-0.017985579734211252
-0.04462540086412866
-0.038328290936211554
0.04297724185557445
-0.05419229970557978
0.03424235527434542
0.0426457650124573
0.021903655630838403
-0.0024786362448619573
-0.014902922154174493
-0.029426434685034898
-0.0015874213423187448
-0.004582116530446961
-0.09180187161425302
-0.14923067325196468
0.03748725038118783
-0.06075720423721214
0.046120717561781016
-0.014555490745693357
0.015608030088715108
0.01210062878416504
-0.02481314086088373
0.022865374250835985
0.04258837294955079
0.020110865779677307
0.012944532087315209
0.0014080276214319038
-0.005469423792459845
-0.013166539008313296
0.036904347688717504
0.02158035055840348
0.03683350288714623
0.015624105082641374
-0.0030780320755301287
-0.009104461354237148
0.04350034060687276
-0.007371038667679238
-0.008009367568546347
-0.06501271887752562
-0.05959021308108691
0.0558485266515652
