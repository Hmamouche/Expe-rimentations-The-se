# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; HSSOU
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=7, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.08934850400447593
-0.009405405422690773
-0.004851260308280016
-0.10888161193025966
-0.044624107845833295
0.03276760193692197
-0.04401234531659316
0.054287588791764076
-0.028622921434212925
0.015676174970840222
0.024747741034039282
0.04151478002106247
-0.09986206385754542
-0.02021007330981628
-0.10780433635617626
-0.15326829928199975
0.0778026314434745
-0.02750802569679936
-0.015000976422294005
-0.1020981349632063
0.006978032849990571
0.031974923924386375
-0.05805793368865297
0.03721930917217231
-0.011016769500815982
0.0618400662679961
-0.009526948949949203
-0.03407703585705727
-0.01667701805645736
-0.05330909917520558
0.036388045489760656
0.0525944280992475
0.050651727294963284
0.0220995022669309
-0.04533547149316629
-0.02741628576534321
0.0029791354250031238
-0.023434867121194622
0.106318618809803
-0.04181046538742581
0.07918126921701046
0.042699444114267966
-0.04019796899848657
-0.025042294198796073
0.04101047385581869
-0.12571550723506086
0.02792240676845462
-0.001094203231078528
0.06343523724355624
0.08646664201977611
0.03423600396683003
-0.0033247853366754933
0.009385598047151819
-0.05059904746878327
-0.003765594200225237
0.0037441160874260165
0.020367328893860802
-0.013344782137442685
0.014006149696523664
-0.008789216839801762
0.07501504167567476
0.07835322916746956
0.018936988254591672
-0.0035451988629875792
-0.04144161130370913
-0.05072812382311361
-0.06672994112487453
0.003427378245230154
9.454906521890252e-05
0.022470438028084117
0.08015210400664818
-0.017943064965883507
0.09820915435797702
0.004608922602521607
0.06351099050324167
0.03218742257205068
-0.09219547034058134
-0.011660348311464442
-0.03603841238591126
0.015580599954343499
0.04699460634697563
0.029322600109952367
-0.001825999684772199
-0.053614641531865063
0.02312467057938347
-0.011718543117690496
0.022452133611908908
0.04852211495263
0.03758536219970696
-0.023185225703442426
0.03715978152386275
-0.02043127617517239
-0.04363021969459894
-0.02102353607304602
0.0009767084864770494
-0.060135117097748575
-0.08614676581328745
-0.15329607430529757
-0.004510071149955245
-0.046707307968788546
