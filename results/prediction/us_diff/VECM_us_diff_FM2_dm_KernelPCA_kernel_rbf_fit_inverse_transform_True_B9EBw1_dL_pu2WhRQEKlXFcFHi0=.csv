# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FM2
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=10, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.01259655404166582
0.007363096348124299
0.0055825575257415555
0.003706371454300636
0.0059873596920863
0.005528375592093994
0.005830725869547335
0.008321653843372148
0.005797395076127033
0.0076436667126347935
0.006619040618587903
0.007403035343028143
0.006103413159106801
0.0058346985930304105
0.0051146222539243886
0.005657688858579409
0.006988333235646635
0.003980003256924567
0.005883430557943395
0.004773137491620382
0.0030778808611261637
0.00566336308257826
0.0037189172096101005
0.005952506716694011
0.006378978237208685
0.00553858147622036
0.0036611032716492788
0.00314106605020078
0.0064384342894294126
0.0035649657700952105
0.00698474246647712
0.0043372095540768095
0.0038614446159288694
0.0044792661640623375
0.00427685482858863
0.0017384921367122135
0.0014433522567725648
0.0015585053540622064
0.0004190201432910006
0.0011191653789564978
0.0008634792790616932
0.0029630924540912096
-7.811607479219385e-05
0.0009348335978910398
0.0019303034214772415
-0.000899597649880887
0.002548141512975644
0.003675569121683775
0.005065844541755921
0.004052496842857018
0.004692036794535996
0.0032966483683782985
0.005956361533311898
0.006196471056012517
0.0045048042650071875
0.006101537928211429
0.007696860916226852
0.006726907579972111
0.009721368741630267
0.008316308856173772
0.009841887588843957
0.012744856541020954
0.00951349132060723
0.008790644778313747
0.007632391709716319
0.009210514581705721
0.008580427877493824
0.011158813425150738
0.008507325409053395
0.008730168091627452
0.015479501189713063
0.012724267778717463
0.013831231771261398
0.013324639936358108
0.012113237246780238
0.008906829704128299
0.01554667647937562
0.014606782052896373
0.012408963803449085
0.012347255643456527
0.012192310752863433
0.006703827530956683
0.013662785549725245
0.008889796323897588
0.00853776424878359
0.012784348995421317
0.00301241973867553
0.012586656927550887
0.00836035972365979
0.010405131770431671
0.011086152509269576
0.005989911819739469
0.011327219996896138
0.01140161697643462
0.011115045073143742
0.014243274585344587
0.012916579446930192
0.014692090331895884
0.01964033216536308
0.011819205684568979
