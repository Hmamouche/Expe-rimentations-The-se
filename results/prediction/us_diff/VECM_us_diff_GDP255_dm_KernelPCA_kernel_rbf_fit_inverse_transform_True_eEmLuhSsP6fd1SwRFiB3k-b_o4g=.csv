# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; GDP255
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=4, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.006160858937112476
0.0068507052586672945
0.0061097744812782595
0.004606717373963324
0.003882493168370091
0.006323425288428322
0.01076087354180632
0.00932547886670119
0.008184925248843141
0.007304918667428043
0.004592065551802743
0.005227283717174085
0.004110256802869349
0.00409161263594638
0.006011647540783652
0.005957822804628907
0.006819078056136951
0.006886758200305342
0.008283756047932793
0.006057091057584439
0.006433658973701589
0.00739982419160103
0.005683859915856701
0.004838472865926315
0.004906362609622659
0.0058744858281926165
0.003896610012989347
0.005774576479148656
0.007060894049620407
0.0034405355292773713
0.0010176459651090224
0.005168666233567242
0.0025931876436855947
0.0057337842261426545
0.006965134599032963
0.0043596285809363
0.007667092360334516
0.006565673100249035
0.003805021047976898
0.004300813630812745
0.005759304584653739
0.0051620737952242195
0.007393653437338795
0.005997937444145192
0.0049967418896029
0.003778686370861293
0.004305208110585345
0.007669480408877241
0.0062875278404080645
0.006227910875956546
0.008439005800129079
0.004187172133725468
0.004949829432468603
0.00557077051063097
0.0062230517478142735
0.00728565386540296
0.008289442245785643
0.009173473841787589
0.010447345966828388
0.010352611742320626
0.00972211234263821
0.007188772907644064
0.008477750887030953
0.00889427728573075
0.009154993148291681
0.008888293023133851
0.01201282591188385
0.01131850097529179
0.010678174217459849
0.011091640855525373
0.008611797035938767
0.006981773792826473
0.0036133168261105945
0.006098206623701632
0.003631243725485235
0.004421110895617349
0.004713238066118744
0.00493249671872855
0.00263092286774132
0.004671272060832506
0.006174065040980752
0.006757479032982179
0.009608144758073848
0.008875524533971895
0.007152717795656966
0.0088880252781926
0.00670340693653945
0.005910577882909259
0.007498211397853548
0.006294523783326642
0.006909783784703147
0.007175347237241689
0.005936153366172207
0.007916839434701207
0.008968081044513855
0.008361164335711099
0.007448811769280029
0.005906852208413269
0.005558102948862374
0.0041717038060589675
