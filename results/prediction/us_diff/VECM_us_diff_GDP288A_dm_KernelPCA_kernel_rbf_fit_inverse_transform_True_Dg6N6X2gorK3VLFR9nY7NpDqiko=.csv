# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; GDP288A
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=3, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.003652992821235463
0.005044417177384153
0.007953235081191688
0.006710714437594245
0.0040520125379093315
0.004657871837957824
0.005255310462231638
0.004036878468382316
0.003696198529375593
0.004661388069911136
0.0030715590024816015
0.0017852288210046556
0.003522957737683761
0.006486990394854893
0.006595285820121712
0.005477546295014472
0.0053251508135029706
0.004081191562090886
0.00346969848925499
0.005360303355763493
0.0043615354332434296
0.003384712170135541
0.005323725227440892
0.004925416084044472
0.0034361387654552066
0.005921672996993091
0.007946541444722729
0.005584120474239937
0.006935699458882095
0.007898050269817833
0.004526633605419355
0.0033490142570127114
0.004112973508467752
0.0018566144227736355
0.002726637213881833
0.004089759543687369
0.0030856950312407472
0.0034739268865879203
0.004584553006615388
0.003455221926598836
0.0030625178198001018
0.0033877962421320915
0.005908222456050213
0.0033018331739294387
0.006412411756319444
0.004323708491389484
0.00514995887906963
0.004643958256845281
0.0036191201152603633
0.0027686278308036343
0.005323908483488373
0.0023610303366027457
0.004078998953954625
0.004727012764433953
0.0032629717748571636
0.0038264816473841377
0.002646457842289301
0.0037960446831920916
0.0013846437180420489
0.0031134602126041038
0.0037720450331404664
0.003137397498641328
0.004059136144033236
0.005968834197979785
0.006629410106342383
0.008258419230472526
0.010899217646686402
0.007707495238002855
0.008336883500934283
0.007108491433332497
0.00551713006555242
0.006217085506340415
0.004378792763967065
0.0027908561716441394
0.0031642079022337535
0.0054733734768638905
0.005772719556610171
0.00683467991870062
0.011734853767401777
0.0075954435059652796
0.0089110657907112
0.00821726751004452
0.005160290748018218
0.010022691723123898
0.01064750469703364
0.014738098549715742
0.013281854855642202
0.01294899347116338
0.01685436030281385
0.016133820513684633
0.013144553911163176
0.015351459896614288
0.009081913588813592
0.007535411770632992
0.012297379285035485
0.012236632740410166
0.012806700350640845
0.016790984361163032
0.015980489153244345
0.01802753867898569
