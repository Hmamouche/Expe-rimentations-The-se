# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; CES006
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=8, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.0445716138250695
0.0396204411900426
-0.024569578256079236
-0.008002118172913077
-0.0042457857675645885
0.02304228401202554
-0.008719068872912891
-0.04388171876196048
-0.0015963435841509638
-0.04040442030452168
-0.048208596286377796
-0.04999567663860845
-0.04829736760395792
-0.02136102648706984
-0.05459304737364284
-0.04476417818871099
0.01118729482745258
0.014885607876444731
-0.03889573154605798
0.020321560614662375
-0.037478786600835215
0.033779395043824594
-0.005870296605655755
0.0016244528842446898
-0.01691023169042845
-0.01768558038913683
0.00637096888773005
0.020026435365896517
-0.017181896384756187
-0.036395263013197496
0.0277311601119378
-0.05249494964360198
0.007636282530739252
-0.003976756720869196
-0.035837248838655786
-0.0037635226338491416
-0.047964696344374266
0.05303068942221504
0.014711227656474878
-0.020839645941004178
0.02655398831670315
-0.04600089386391925
0.00011464553470106817
0.033667137930282316
-0.012570285396351081
0.013680609705155312
0.014092473375378866
-0.03320870920334393
0.008569359861390456
-0.018418298815016183
-0.025378934440534087
0.03277267862746348
-0.02552383683690445
-0.029308884314677965
0.03885060135911538
0.014282561095975085
-0.0034995524313303564
0.0037159280179552092
-0.0074761676809221985
0.014309092888612153
-0.018280941817258995
-0.02213071947747529
-0.012003021899877451
-0.0007439026664623092
-0.019341485635448585
0.006109266716213375
-0.024147609782797473
-0.002206763359222961
-0.0005908413908932396
0.012599432597765569
-0.014892750596299322
-0.0022752778010579973
-0.029293912718979874
-0.03650183471398099
0.06908592049646332
0.020036999391305536
-0.044960074080927996
0.0004261813061987793
0.0028721108576612933
-0.0028230668922260517
-0.009673231440272625
-0.011115716364819924
0.04389367282455247
0.024955116187081128
0.00648573520461174
-0.03238231082360882
0.03355503661633673
0.021285850601485058
0.01221887820758803
0.02174793847639367
0.015366828562288512
0.033636141436798585
0.03277344019129259
0.05022580802470378
-0.012561000716110712
0.030029057675706197
-0.003224705642397735
-0.006256508322206308
0.019446740650373027
0.004611751619563346
