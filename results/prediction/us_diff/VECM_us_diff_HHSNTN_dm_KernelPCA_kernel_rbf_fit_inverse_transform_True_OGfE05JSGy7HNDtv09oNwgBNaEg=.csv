# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; HHSNTN
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=7, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.016018965084258114
-0.25863320907677206
0.057004965557205564
0.01785522880054912
-0.004021224177841916
0.11617274878389552
0.11162265397175221
-0.020336561748581963
0.03267472210485989
-0.04743948509480469
0.014212299040948407
0.013201351118259468
0.024180214035644458
-0.015990592826986665
-0.05460114198766386
-0.07333834868744278
-0.08634419322826112
0.031290724889126674
-0.024647772403259795
0.022716390644847713
0.007388878118650093
-0.03482928096531812
-0.00614179520151544
0.026965471742568088
0.023273129505549453
0.005795640213163939
0.03202610279595332
-0.029448939668817824
-0.08175300005209903
-0.09063702441214427
-0.0380154845715937
-0.14052560305812362
0.052025610775622136
0.007316784685787673
0.03088979573811363
-0.10448457736965087
-0.024893626028846105
0.13789089318240055
-0.016950347170419326
0.09339086948848968
-0.06279156085389634
-0.05625776962252677
-0.01224796999081637
0.01747926660546636
0.010110707277294097
0.06391876205040438
0.02415920789462914
0.05473449240747894
0.030780967052326216
-0.06552198678103277
-0.015040282163463786
0.01798201268305486
0.015386808464833009
0.0598630860626625
0.043334647735278675
0.02641924282042612
0.03587009436001644
0.04814054256680878
0.06202546200105724
-0.006640240186669644
-0.033953977572664246
0.020958770495648614
0.031177540132722725
-0.042945754155554426
0.04665281250500837
0.005054394999462308
0.03910517651192666
-0.024282455597296637
0.0008816664352534636
0.0009914952550545931
-0.009629435972903035
-0.004845896348934637
-0.1336373960208697
-0.046503549970126595
0.004107852216830854
0.04236200991499253
0.02804424600605385
-0.00899134765763502
-0.17137058914750208
-0.011376520221535683
-0.01133648148693344
-0.02597410858784279
-0.0030546951724586824
0.11218607945858144
-0.019481348114468527
-0.03415579367641121
-0.021546853830451844
-0.0742153615406673
-0.03297222336197632
-0.05175524689786955
0.012848426756564078
-0.05866272832771123
-0.01438561784455457
0.04601170042140658
0.004461129644159036
0.08925377607909668
0.06307781022255694
-0.02781303615748921
0.0068324120563050365
-0.01720133555109265
