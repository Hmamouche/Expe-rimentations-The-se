# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; CES151
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=6, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.018229898068897715
0.019151849085451436
0.03097232342047227
-0.029167709292479808
-0.03511017295679335
-0.026614687358719885
0.13278088688685954
-0.08465970142273747
0.029228436371447945
0.05632155719503773
0.13128816769334134
0.051129768521247065
-0.09336034556713986
-0.022870728477695303
0.0016890394469994694
0.09811192988462256
0.023493801357301548
0.012246829544972877
-0.043022704944320825
0.014860153618704457
0.023651561170257688
0.06572027973911836
-0.050811817312287705
-0.048322390275902134
-0.0353565819617384
-0.010867456661311671
0.06550095366926492
0.019054653306380433
-0.13467790984679745
-0.11363725480191633
0.1533828257786126
-0.07872960179455388
0.0795777350590511
-0.017505881228990258
0.019837269336666154
0.025299178100013895
0.0005164685209867667
0.15900204591312495
-0.03709464551722828
-0.02522009093539164
0.01358400734532966
0.10408530345237832
0.0586808244614277
0.02989249386866156
-0.02809783689198762
0.0504725271971652
0.016567267749792697
-0.008679163490421617
-0.012333986165595622
-0.05581290846311202
-0.004427667405904164
-0.02754899914776361
-0.04106690990771437
0.02438704614349782
0.08058074739350818
0.054989874246079845
0.01640202179183311
0.03717360217643974
-0.020228907279092753
-0.006319691560887582
-0.09530721572027662
-0.04311245424007468
0.0774121792355564
-0.027933470358516288
-0.0161390671608588
0.02555922516029603
-0.02865138161143748
-0.048897570186826556
-0.021939877471712874
-0.036357723344721635
-0.09150281775318282
0.000403374398547934
-0.08865553633121762
-0.04887506560188101
0.08640567638614191
0.0021240655583437094
-0.0758708827906014
-0.0009690418527841596
-0.1301717244558501
0.035595337971361614
0.009718515884960303
0.06654218886609037
0.0025355894389948383
-0.002669569376374019
-0.02823750857872271
-0.03287945774077349
0.030351434828397614
-0.07355518953412835
-0.0013383636029273178
-0.000982634218981029
0.05062592849829169
-0.027472064047200633
0.060542721737820424
0.050710279451797256
0.022090169541866385
0.10196988533284375
-0.031074185162980484
-0.058554671561843186
-0.02127075166908817
0.049254845498548125
