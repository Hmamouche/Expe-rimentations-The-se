# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; EXRUK
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=4, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.048333362298389464
0.01779425916146521
0.048169297265415564
-0.04139365261468846
-0.03678579378475024
-0.01466645784880544
-0.03778562119837424
-0.0075501590935305955
-0.013747558690063236
0.029706228247562937
0.02970915108901622
0.04436102734027538
0.0009068476051769317
-0.0033044480621473495
0.05927874377757731
0.01915650607494065
-0.0036474544419582075
0.08404003467792612
0.045559073145686586
0.03445044629304371
-0.03011365204191812
0.02078527845297178
-0.011638841891390111
-0.038750311847535066
-0.01410825172201522
-0.014976345689360234
-0.0065238510675002925
-0.02566680519974053
0.09157946977272426
0.06297584221911752
0.03630172935827058
-0.043712293789827814
0.00609990861678408
0.010088185824081894
-0.05563280628416814
0.0005312119474702223
0.0559804668341809
-0.07981578332987435
-0.025020732217005806
-0.04719592873015613
-0.050400054811807954
-0.02230070144355792
-0.02826832152681534
0.021572097171368536
0.018888929267364164
0.016496761293443143
0.0030729646346701472
0.005645578344397436
-0.00764078257300122
0.0007310149461042985
-0.031785336909781414
0.0029365501527539688
0.0304433066370367
0.011861978116633441
0.015550967228818602
0.007652062159587348
0.0005813985693526125
0.009620299476285852
-0.011353249713288096
-0.008477180373166245
0.0028776589593826262
0.011385935903454976
-0.013699827799962247
-0.0037260770924843816
0.01102730656650241
0.020910545987949763
-0.014615410828112768
-0.037021094949177824
-0.024713422026282832
-0.02160663782872579
-0.013566544270795806
-0.03087253410777956
0.0049162055893679285
0.00738374719993189
0.002115585932693991
0.02347219900746658
0.01480972612819619
0.007953746945257054
0.019535106729079595
0.01254032048724478
0.010713474149703266
0.03999757928237973
0.023535321380022876
0.008719190158776216
0.03231273761307605
0.01848171018964089
0.005344749625764858
-0.0021531174598820356
-0.009148870876442252
-0.00712434107122099
0.0009302647724042613
-0.005776317896011659
0.025015394469580135
0.0155729191553114
0.04231729587385251
0.016171167163585888
0.017295984950606996
0.01775876281401028
-0.002592408438723357
0.008111749863165072
