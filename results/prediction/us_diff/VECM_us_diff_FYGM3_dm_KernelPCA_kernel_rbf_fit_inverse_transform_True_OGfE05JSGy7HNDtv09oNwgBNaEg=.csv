# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FYGM3
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=7, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.10274174955149405
-0.05029266849021205
0.11135945567341521
-0.010752149933404146
-0.03181007335749015
-0.10661058074470162
0.05551279406682625
-0.04864257984395936
-0.010219692288256541
-0.050475257884981556
-0.015901324483388435
0.014726937766034235
-0.07424621833785645
-0.042725698119004986
-0.025867040758883124
-0.007878980785735854
0.029190529178103213
-0.028387704502870627
0.043324309942096156
-0.027954481344373082
0.03908341088578472
0.04830755056186224
0.04542716027185341
-0.028119391601158977
-0.01651361002229853
0.03698934809107138
0.027430259733162148
0.009799605160123858
-0.08697418723486952
-0.023730370711455392
-0.05737859244277168
-0.07167067398420798
0.0020421524319996463
-0.0052790531761930486
0.026345990633688695
-0.019988080175432694
-0.06631094177830221
-0.04754747253913316
-0.041186618414456325
-0.05044327725330875
0.05013515098361232
0.044493910062358685
0.0337547292385119
0.009391786470729632
0.009018795517442934
0.08264113119180552
0.012088015944175766
0.008602657170637648
0.02453384662467183
0.010721740430933659
-0.009076094710987833
-0.009576138350617765
-0.027115529377965582
0.0017284313624078562
0.00030062516705635367
0.0309845500109499
0.03143330253085265
0.007855612604051855
-0.0014999105200120188
-0.04160196707557633
-0.007125597579088174
-0.044826845134683574
0.013943576862192204
-0.051342243818397706
0.0021628953417573184
0.03863800600354021
0.03290219001722154
-0.012999228314252195
-0.026512914595188154
-0.0019505712227872347
0.007546300811330753
-0.007267166335795215
-0.05510057739476497
-0.14193802358472854
0.036924765989174144
-0.03955133911704918
-0.013362281329695249
-0.04481250413857328
-0.021118650895044524
0.02288199854699978
0.006489345390760168
0.03780434124933738
0.04739658121612404
0.013314426790614056
-0.02365818897607609
0.010825725914014962
0.019470194825610868
-0.0010974482064694865
0.03862971057982598
0.03533945621933598
0.06001455548577626
0.006595747624118814
0.009284620352545632
-0.008738051552749142
0.03517768297382699
0.028724996791634955
0.01019534582510233
-0.06332554042930828
-0.06271408946041879
-0.01912641884946548
