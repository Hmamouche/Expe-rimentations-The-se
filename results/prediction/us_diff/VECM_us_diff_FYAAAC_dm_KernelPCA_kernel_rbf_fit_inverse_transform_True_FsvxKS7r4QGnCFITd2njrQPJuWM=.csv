# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FYAAAC
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=2, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.033197590812278965
-0.0021054798109239707
0.028573436394002453
0.03346728296627577
0.028386518572863038
-0.010755693544765488
-0.01911628451941754
-0.03399533104627236
-0.041449194243973975
-0.054224533439619346
-0.06188012509870435
-0.05974883908140035
-0.05884922404275379
-0.04222393551716854
-0.0266766365726921
0.004769830034823364
0.04012258017836416
0.04904263770641267
0.01653079120102553
0.025068921214220427
-0.011028071750528704
0.011670601427793525
-0.009237787559453817
-0.02424203124126274
-0.028242081261805673
-0.02682146129291766
-0.008987072507817568
0.00586562445612593
0.001364023143411353
-0.0120468547101891
-0.02912236522638469
-0.002428085974209224
-0.002304136306062094
-0.02432912181605617
-0.016682361777180396
-0.011217233550944403
-0.015982365304867904
-0.005515088149021867
-0.014115444655734681
-0.016793067074902655
-0.027761951662885004
-0.025350513998895007
0.006959972566858717
0.040196787291515226
0.021814296394720784
0.061503962023467376
0.002787377057247001
-0.02268821566318724
-0.028292699588862807
-0.03997636220015748
-0.017088693766521435
0.008813250686436673
0.012194735443049411
0.00862130166735629
-0.0033126284694353523
0.013312559167805618
-0.0026580738932765694
-0.014809392696428423
-0.023023767418948203
-0.01611171187431307
-0.02562680959908459
-0.015564743864618072
0.0013585587536637115
0.017289868138236967
0.03362244181678431
0.02689389400843832
0.026393406509001825
0.01577756090992406
-0.007207444599231772
-0.021314525379869248
-0.02616135867140148
-0.00483178533465798
-0.014623601393143382
-0.017511541633134727
-0.0032165817812061202
-0.014359321150955852
-0.002748802607956162
-0.03618794499486971
-0.019491883743691686
-0.03468884544557369
0.004412403234710909
-0.00032217015963233436
0.0007230906952033814
0.011111112552082815
-0.01163955631088404
-0.0030149245172769955
-0.017131591450019173
-0.013476365526053944
-0.008640194075371178
0.002660455547956443
0.00822288979220341
0.022466950619919136
-0.009422971753710263
-0.005826602013742733
-0.004979808600111473
0.0026114634767745604
0.0044508012405399845
-0.0026608350932428356
-0.02449568009064092
0.003863615310710955
