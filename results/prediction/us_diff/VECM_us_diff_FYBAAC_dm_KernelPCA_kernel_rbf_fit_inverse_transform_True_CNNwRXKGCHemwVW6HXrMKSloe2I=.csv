# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FYBAAC
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=8, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.09308230816789084
0.07663270668586412
0.055046271128647684
0.023493957138766335
-0.036157348832830075
-0.07949059359947948
-0.05410124035533165
-0.07707804092240261
-0.0034641162770776886
-0.008517863905894549
-0.0459097819932367
-0.019161788363738535
-0.050153579555956576
-0.004082678907693917
-0.0584235877179921
0.029785488799565517
0.056672903806755257
0.03890355715909399
0.024046085063411514
0.0008820604524592112
0.0017405266293938114
0.011188081786659318
0.008651658969663812
-0.043247018607730336
-0.04263926106268961
-0.009108803492921647
0.0005205510984994666
0.01664968698641551
-0.02251713896445142
0.004958995726595834
-0.03196325530594503
-0.01606520748970941
-0.01884916916737332
0.009631138933480587
-0.01617906305029513
-0.00728009828432765
-0.04356307820530803
0.01495594539753545
-0.025567169097077576
-0.03252168712545329
-0.004754288746062053
-0.014414065071075906
0.0286332162447762
0.03730347029625368
0.03323625890406993
0.059680607005640096
-0.016068325490378083
-0.04138717228544848
-0.029472566216006905
-0.027661861472673677
0.006396197058574665
0.0284336808160175
0.01159603144935876
-0.005320605380905068
0.003991268711985959
-0.000581257361035788
0.01270072659670304
-0.0024139528352670643
-0.021928101086060272
-0.018350713361312318
-0.007816700938059305
-0.034481143753631406
0.0326092814317786
0.011617635536146791
0.013698826068449792
0.04589284314309916
0.011024316984399686
0.005192731367510029
-0.03386843190347269
0.014446309473762221
-0.03592191375670485
0.011405597188875759
-0.006635592077034333
-0.045490391401524964
0.025663619163555258
-0.007824233297928997
-0.03302040753323964
-0.0006502928417645316
-0.03833571161701452
-0.014956134950487915
0.016977158452050087
-0.005362619397175784
0.03217774162435174
0.004820565662686604
-0.05084247164319103
-0.002873583420825899
-0.028328480583082752
-0.016484860426800946
-0.0030826353058330813
0.017844051475529917
0.01271023833941896
0.016351795579627928
-0.005228288800540951
-0.004682884580926564
0.017386211430111244
-0.008361972597606689
0.03139131331685221
-0.02029965350866722
-0.027837304359902292
-0.01098528903890825
