# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; LHU5
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=6, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.01957946603214479
0.05761513242701567
-0.02881315911291857
0.010439514677987606
0.03245727118911494
-0.09230934430910864
0.0001551273742323063
-0.01043952827360291
0.031052245796916393
0.009134770846351227
-0.01741040310652897
-0.019211576100524735
0.00011565942546402713
0.040380575393137504
0.007644062047254191
-0.011721859730819262
-0.0351633573784236
-0.015388817554754382
-0.05004078270195924
-0.00021416771510124333
-0.01960340292127752
-0.045898744922224344
0.013260043826258935
0.012036371743229288
0.0023050897318095797
0.007474153862247033
0.005619923191309497
0.019939174315899113
0.007732020959569987
0.046788750186242616
0.03452814088731305
-0.041333343594295274
-0.03525041988915996
-0.017574678354144127
0.029646502741690603
0.03893816022822845
-0.01995501391001809
-0.02246093870841466
0.01986409612338802
-0.014528557053994905
0.005703462297922315
0.006842498923785217
-0.08532786171974756
-0.009559870292156313
-0.06953812612460902
-0.0489626135488155
-0.022767742903845187
-0.05371841755842609
0.019706004412404082
-0.0074215368828239505
0.0017195950040608463
-0.00668999393009064
-0.022734025613843217
-0.03138128298104847
-0.00012662338961768726
0.016157786008163026
-0.03649336908737484
-0.009634676730288526
0.0017111432575729092
0.01542668232433857
0.030129099895332148
-0.0014160923879489402
-0.019294848060128447
0.025332402247925115
-0.029653953297908677
0.017576656184859367
0.007396738981751607
0.004796235956470883
0.011400279119248057
-0.011830959940532374
0.02391072986098523
-0.02633600065261889
0.020814622828417312
0.02064100795799738
-0.0013340820185782417
0.060104917242824185
-0.016146930994642688
0.00997828107995554
0.03915462820510273
-0.02580006598774766
-0.042068621308127004
-0.02499654118353691
-0.04553446793655243
0.03979632421994776
-0.00393797828678966
0.01689569884902114
0.0008804217534213829
0.005993465218016542
-0.023704175090396195
0.02740343192077723
-0.04360907714476123
0.03830638213260448
-0.0010802645730990166
0.0003255237821396754
-0.03718606111073887
-0.011475263392626639
-0.008853712495290355
0.010407728896339796
0.02333693490201219
0.03347274902358022
