# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; PMDEL
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=4, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.05974722909481765
-0.0045328164384108305
-0.047969383702431506
-0.0304813280061694
-0.09660331514818675
-0.026599461789993697
0.03791350789733486
0.004044845189848781
-0.0014288567468283171
0.01922825130611424
0.05178640513407018
0.025499369145645864
-0.07009711323855748
-0.016298921893686355
-0.004580445434842708
0.04472743109592838
0.02436647615260102
0.007908918869401944
-0.002282431773864481
-0.014791706063716912
0.00951129483476864
-0.02172187299807439
-0.018111847307798683
-0.0633011867057445
-0.041182889584628755
-0.01285505490011202
0.03770383044023525
0.019095855622906565
-0.0422732772688241
0.009752647180560996
0.028278246874587038
0.06862931500980424
0.044098576409147744
0.007544451548734394
0.0017993302641971798
-0.024892342428451578
-0.0013273895502610746
-0.014187830535152133
0.012849664254954487
-0.032130411190659076
0.06052433111750034
0.020187501960079263
0.05962688401628847
-0.051288506600288816
0.011597847896811897
0.014384089967024831
-0.03673327743682943
-0.04023679528105696
-0.028304787178994907
-0.010521524956755403
0.0373478638539941
-0.007782291002471449
0.004589336232276102
0.001389930643501897
-0.026926607163899125
0.026672024934250263
0.0049938453217140185
-0.006808601926385381
-0.013913477102057963
-0.02230389813593827
-0.006103551762538828
0.0013516556997342534
0.05171283043242447
-0.04148964900289234
0.01520008613662453
0.0103881522924247
-0.00990421015646853
-0.005545694171037341
-0.04271629178479834
-0.0009167523106429088
-0.0035530187919381884
0.04106293210186614
0.019600091973645397
0.015327535503650519
0.033893432058486356
0.012976623730499043
-0.009681856551882181
-0.0034388102108616897
-0.034263470479573836
0.012193311897058159
0.04547520806852341
0.047791443894901205
0.04707457250582941
-0.026267223328614954
-0.055709338529076634
-0.0353383291138423
-0.03103360321165742
-0.0009735781806768208
0.016492948903465532
0.009411874430757097
-0.018913664884314094
-0.016013498908465637
-0.05284320174213945
-0.0021008941505083318
0.0006341774445947843
-0.001287343684413243
0.02167205217309889
-0.025839243200155414
0.017065587172720675
0.006722866181929906
