# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; LBLCPU
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=6, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.005547732211563344
0.007298573590135509
0.007274561019295486
0.0033712926778725243
0.00809504840599624
-0.0008977535148199541
-0.00040871918584807357
0.004108556071272624
0.0038387886544441756
0.005048230805028073
0.006211042108163642
0.0017709040344964198
0.002330441023721343
0.010134541241015371
0.007433814004614891
0.011126963927761363
0.011479861981815965
0.007926705918341319
0.0029937458589001124
0.0042327123888641245
0.006308310001437233
0.005497628363307043
0.006222012512967216
0.004692528128066392
0.0016910851120555208
0.001904547701832987
0.007083910700666147
0.010963978351752399
0.013174107847292402
0.012509704281207908
0.008206430899523205
0.006929641953374193
0.007638205065887494
0.0026847039362847477
0.005056943420254235
0.0046031834082420064
0.006701274580316484
-0.0006672652808472012
0.0025165735336937557
0.0067315297068613
0.002702553567618902
0.0005246604526232895
0.0045241833825822005
0.0063749179875459685
0.001560988667486183
0.0025270578912619055
0.0008997369297709659
0.0016098958472630731
0.0013228716553911697
0.002807772889660596
0.004393486232820078
0.0018886858321108557
0.003929404118392602
0.0017011573938705644
0.004132598707201951
0.0037141742786940754
0.0012026185152504395
0.004555658092075913
0.004141916651098332
0.011226011853879512
0.00855118513349277
0.006264660064311192
0.003949345312982659
0.01019715652396134
0.005202045838963677
0.004576374667830333
0.012723018057682507
0.013053005353311237
0.005536709291835501
0.024273499057428947
-0.0029516632648247396
0.01306809063527369
-2.6714365632481488e-05
0.008605342143989276
-0.006016934652252524
-0.0024665190468485068
0.001605992408307489
-0.001536529559155922
0.002559543211146079
-0.004035675100705095
0.0071445705452120875
-0.005545890690916583
0.007014085235791358
0.00041140965545485766
0.007124900336518391
0.0022267061548650532
0.006255193923448163
0.006737035643809659
0.006122749224768263
0.0016122682841352286
0.006170962552863947
0.01625624994425357
0.005331941099510223
0.00764115965621464
0.0059881365044902385
0.014266279678078668
0.017073655631359347
0.012732428639642624
-0.00382485787648603
-0.0027113706376633087
