# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FMFBA
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=2, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.0029706001391228034
0.002699222567249358
0.002578046105601576
0.002171808979585338
0.0020113045819782505
0.0021802992600355327
0.0022460203896611727
0.0025344001002134326
0.0032343191339853526
0.00312002072037857
0.0026715488382415323
0.0035109888069093957
0.0034080182960144613
0.003856927165418156
0.003844060450295755
0.0031938358201754977
0.002764451390985298
0.004106360777420552
0.002960364735080004
0.003235012148501089
0.0032558791737418583
0.003060767340794136
0.0023815890713271186
0.0018301261091694624
0.002281016404374091
0.0023295823656587357
0.0034755818458837066
0.0037563537103507474
0.005021374339158722
0.004970351839314184
0.006876624637213553
0.004362703251363829
0.0028385607419972223
0.005156681434301872
0.005185711496098294
0.004203153389217707
0.005721957173150406
0.007550082244157142
0.005732394059643775
0.0066486031273317505
0.007341626631874924
0.006698157745400676
0.007037996663210573
0.006614367774671028
0.006073438758161045
0.005172821503433555
0.004624275130434266
0.005214916046899245
0.002193492095196301
0.0019363405048492862
0.0009631390207693285
0.0017575334472451456
0.004163345750061657
0.0036993261411174894
0.0038665662084143614
0.0036395495227125193
0.004950075068481098
0.006203305451240567
0.00557785020806651
0.004470397668567635
0.0064341631267243295
0.008012385991441167
0.0074065722317543554
0.00895387855595408
0.009056679601221095
0.018824193328028472
0.018608847571209595
0.01113556570813477
0.017982607317088166
0.0023846935654600956
0.0040280171116737805
0.0040497619157818165
0.012802774646266184
0.007775248658812097
0.010788876314288499
0.00996622218346414
0.009223063364417375
0.007164659269271828
0.008029960817882553
0.008188029598026747
0.00702044975994301
0.007465919269187541
0.005405155910097903
0.006591290330733013
0.00810623451954762
0.006285665771729902
0.005785750359121162
0.005487583028220493
0.0061621167153628606
0.004932736010138078
0.007014999846689462
0.006153457369929133
0.0033110933808048323
0.004145961311226638
0.003931575629195255
0.0038963433801229557
0.003437569344472854
0.002277985820617898
0.0005609361160688036
0.0032975275770663736
