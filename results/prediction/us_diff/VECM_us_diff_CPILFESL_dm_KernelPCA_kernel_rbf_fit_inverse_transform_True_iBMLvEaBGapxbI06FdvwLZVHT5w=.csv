# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; CPILFESL
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=15, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.009268785420433068
0.00806920182872697
0.012207612282556353
0.008546714668674162
0.006026739420027318
0.006935091302660765
0.009001200690419858
0.00633773505718287
0.0025746810422752984
0.004092425632094783
0.005309313401272519
0.0029727655685308513
0.005041687344405646
0.008394666238129452
0.008853202971122893
0.006194698645151966
0.007774215258188244
0.0076517087412375055
0.007020157100190829
0.0076061698868077916
0.003837959464895329
0.01030885633101329
0.006813747167173901
0.006334898116757309
0.005132793479392278
0.008930585135522622
0.007955675327457673
0.008791311892654467
0.010142586762444132
0.008888677803501555
0.008842210863407822
0.005945648454673076
0.009495127507635065
0.007897121684445042
0.00726332647738075
0.006173534386581983
0.00475146685776973
0.009402424304907472
0.007464723026491772
0.006117579697307309
0.0063311948561021215
0.006066475418587499
0.006567632161957944
0.006774947450145316
0.005919344960109429
0.006613855468774889
0.006332259461909868
0.005801377628797059
0.0056183125023638435
0.004714770030830628
0.00624494984476734
0.008502994853605679
0.004453384772374487
0.007499824050764619
0.005658280453915106
0.004617391237905823
0.004076258557063557
0.00419373667553275
0.005038997027030526
0.004469820599952584
0.005057126567051981
0.005670739273736547
0.0068443795277756255
0.006228323904349487
0.005470120219552419
0.006217155987830597
0.003519356044152373
0.005926608916874083
0.006166421824902839
0.0058108918219380475
0.0053263434464772726
0.005940575396724332
0.0069868332504100005
0.0035392439982754416
0.008150442736153797
0.0046896898463992375
0.0062683229973361345
0.006214118024738263
0.0036843092968413178
0.0017830877057824135
0.002211266405341968
0.0037991149222902686
0.00541830587564044
0.00664180163260655
0.004440295456495817
0.005729202060186844
0.006261300011997048
0.005925579849783489
0.006305447249712782
0.006264832608757678
0.006544528136950601
0.008632697470624284
0.008817343895399374
0.00602679359508832
0.0049977598390251315
0.006141087730056119
0.0056525793307978245
0.006102910152757098
0.007684155475764109
0.007499235931702679
