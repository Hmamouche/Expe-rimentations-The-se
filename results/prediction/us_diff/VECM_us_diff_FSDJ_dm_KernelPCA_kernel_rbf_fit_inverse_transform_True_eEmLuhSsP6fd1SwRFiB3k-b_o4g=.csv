# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FSDJ
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=4, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.0010300336016364742
-0.0023783909860859353
-0.002498265042044038
-0.005729943070678615
0.0006227038537994248
0.0036589019907287174
0.003970261584951634
0.005935164508868994
0.003908325693323941
0.006568426015436
0.011042849770174782
0.009641968082245713
0.002170783094723868
0.008269519318871286
0.015919074961613114
0.0008406215257089197
0.009644987117451981
0.015355183815158172
-0.0010563499287729785
0.026752966635974193
-0.0017618956841128526
-0.019142003466509603
0.00398702402968387
0.008029819912356373
0.011772309173523269
0.01223143203958454
0.008117742280250815
0.008744012263652342
0.0026583056490953614
0.0012018264489660772
0.011436079407237888
0.0004056571020308556
-0.001703486560295726
0.002693246173852039
0.010762003798432535
0.005523781418984896
0.0014584484957886024
0.007473512719467572
0.006817028858157749
0.0059535004048100575
0.00809235731847088
0.00789714549556803
0.0016047675586385087
-0.007579062574439615
0.005382756466337117
0.002933059019413489
0.0012316850671267015
0.017720334941646307
0.021286579600535586
0.02147824480989219
0.026387934078694747
0.017536988694241914
0.014478403039938102
0.022098408137644476
0.024748280789951393
0.02776394909863945
0.043984532627817395
0.03345492004632627
0.040793044230410566
0.02340917284491948
0.047804316449720045
0.03483197526134238
-0.013163035299644625
0.027971436295494673
0.004109635749383538
0.047323374894447
0.04629639324541518
0.03148644412320191
0.011975319683563392
-0.004972024674510528
0.00681174161121481
-0.0018637259551529746
-0.02633760019354401
-0.01080904191597699
-0.006297427465125924
-0.013672153859844915
-0.034516347852812754
0.013558999167299543
-0.0474809867591789
-0.025098413980184753
-0.03809160964987463
0.022613838420255983
0.03045729276357385
0.017391916369837515
0.021266720657009484
0.020668818705731333
0.02302959187789757
-0.004249125275362707
0.010435497990917694
0.0024088456814710655
0.016802720731758484
0.005656396180827046
0.009754518715171833
0.03981877326309564
0.02821723582131948
0.03931748398796884
0.026304890123630013
0.03245611252853467
-0.017871570799981622
0.007703697155983218
