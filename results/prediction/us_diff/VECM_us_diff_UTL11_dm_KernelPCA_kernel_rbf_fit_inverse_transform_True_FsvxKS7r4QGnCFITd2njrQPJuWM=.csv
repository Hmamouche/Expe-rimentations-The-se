# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; UTL11
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=2, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.1371137066335177
0.07987170925426859
0.042307596590834155
0.02146814239177073
-0.040380334719548855
-0.017482320559435535
0.015175901940239071
0.002114010177796579
8.892621897397049e-05
0.006089387994560241
0.008261194629554528
0.001439048444607689
-0.018331195705324602
-0.02359214461045397
0.03719204716563601
0.050273102933247014
0.07493402669423123
0.03154176148677905
-0.01828139080938722
0.025496366110287723
0.017738237112431377
0.0592558868740068
-0.004670556379238945
-0.05131796066333258
-0.051264719948860824
-0.020643415911133584
0.020949868733792978
-0.0004052470260597482
-0.05234910632048054
-0.0990448276730031
-0.08472905211373843
0.03469262840891136
0.058675235960797094
-0.0221742104878427
0.0041980509177895545
0.005167610995735861
-0.019108547357547304
0.0049948742990671815
0.0025224229428159606
-0.01294322477658734
0.010674497824993322
0.049143355263155525
0.07715299635551343
0.08042057960587105
0.018761425099904017
0.04330696902092405
-0.03493883730619112
-0.07326373739792863
-0.015354494013172106
-0.016834505561516427
0.005797075513594402
0.05726821879385262
0.019682507669448554
0.023098999129807014
-0.00032058456049203175
0.01883029134731543
0.029789483536018402
0.009072701457212898
-0.010201521680021966
-0.02595688608839419
-0.03528251252108127
-0.02478350169259147
0.006557773081991216
0.01237293617293367
0.02609560803695849
0.024308972882981977
-0.010233352858532704
-0.03622917952686827
-0.061361238680318474
-0.06899901148809022
-0.07334932884529986
-0.033310496389836436
-0.04712525036266645
-0.08186082476400953
0.017173269408275845
-0.017104294553365824
-0.027980615927113435
-0.061227074715666484
-0.02951709183764603
0.008013556567195067
0.07621765258030462
0.09565190984808464
0.055008604635540706
0.01949292114868115
-0.011828848570964518
-0.015149145401594538
0.017608630306784716
-0.0034063954515438062
0.035029007627957916
0.043395812509442785
0.03138194199083753
-0.009336907078944027
-0.033731828927145
-0.029246693428986677
-0.0055523943929500766
0.024444450822670258
-0.011550181668862026
-0.03224863021634841
-0.06022544941135317
-0.03205208098839552
