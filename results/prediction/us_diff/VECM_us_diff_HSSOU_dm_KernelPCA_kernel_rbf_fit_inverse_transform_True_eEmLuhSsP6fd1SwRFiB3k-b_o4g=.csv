# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; HSSOU
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=4, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.06701265371550166
0.0069499442951764315
0.02949151106977467
-0.08700705789386531
-0.0033916660839099838
0.029589765946232433
-0.003996370613438924
0.009446627818190095
-0.03383027177120056
-0.0030395148190128164
0.05430307039757445
0.031198572845834445
-0.10523744209372077
-0.034075682464007204
-0.08494871381731763
-0.1374590098237146
0.033111334607203155
-0.021566281814575494
0.004561957439588883
-0.13577922320853117
0.04571643014896656
-0.023418763305464288
-0.017460414055386744
0.014997300986457664
0.02287953695833362
0.024317621979014152
-0.006851780534177244
-0.03656610339950227
-0.024525849751438733
-0.010823046013159462
0.04446265166210663
0.02936789283046759
0.002846586494407611
-0.03408948294627094
0.027021976113546953
-0.031095505641175514
0.003869524683302818
0.0005716272880186991
-0.015283380924725663
-0.008341761316430139
0.024082374624090207
0.0754956065038303
-0.00720436961315342
-0.021350784332723083
0.04521934225378115
-0.06833827705075766
-0.00012641960329618299
0.012906627893819283
0.03441520910014361
0.056275775374590664
0.045435556321671525
-0.009258760578979237
0.03534221748875639
-0.014772652747899696
-0.033500361692004176
-0.010754162276890827
0.01817010177172021
-0.009490726209924812
0.018868525721405745
-0.021301000724619663
0.060268791322053344
0.0760549314339444
0.0374018667668495
-0.001763280791381553
-0.028375937935746907
-0.0524539941403822
-0.014530625593740003
-0.0405463911232159
0.016824728189275003
-0.0014274784940034669
0.06081547631846116
0.05486553565068969
0.053700208077216226
0.04916332766288175
0.02839333200399534
0.0071655718133153065
-0.06850257732383938
0.0011443560336962463
-0.049065800504025966
0.03534532167550257
0.020882213149822262
0.038724152512915375
0.003342857539246156
-0.04231926209439725
0.03529005359909866
-0.012124208746459152
0.004093477689684007
0.046454029803214836
0.03875251601050628
-0.011298279595872436
0.07029690296866964
-0.0364953813406156
-0.043575970428765126
0.006052192761771297
-0.06616113851419538
-0.06953203328784366
-0.04667106605207101
-0.12075174583104453
-0.012122561585692037
-0.061849141527065755
