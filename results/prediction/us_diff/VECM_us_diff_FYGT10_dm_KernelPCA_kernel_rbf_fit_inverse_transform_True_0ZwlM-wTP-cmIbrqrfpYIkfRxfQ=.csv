# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FYGT10
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=13, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.05677919351810298
0.14734365037727448
0.06377009119621568
-0.10290509126787341
-0.006382904649193875
-0.01490406672127425
-0.019689196196307816
-0.04660233330603833
-0.012234900517741579
0.07321678760159686
-0.09092412875796635
-0.12330022228931574
-0.08102418495555208
0.001813905031740018
-0.019850270175473264
0.07604591587572049
0.057597652937701554
-0.019510209794193132
0.024540190395223614
-0.04152419436927279
-0.05059743195958159
0.07086796904024659
0.017778223797860253
-0.10047657010183228
-0.03150011115980424
0.014238175033099343
0.0021561931859789205
0.038554991353023246
-0.02747136598197314
0.03482026052225917
-0.07028870107791786
-0.1153362683641071
0.09515509960890492
0.02155167702213022
-0.02548282038350935
-0.00774520065089513
-0.06742665053291032
0.009432320230507905
-0.022602859827450576
-0.03523914000688613
0.003118706739855405
-0.02308708330373861
0.04848933774995229
0.04534600257921555
0.044364580866649975
0.09190627794977
-0.04703626433346806
-0.02302874959505606
-0.05234857672510802
-0.03889617925692168
-0.04342055848911679
0.03649925701514426
0.0599502554748342
0.003605574547118389
-0.005569000192689895
0.002944816763246751
0.011908987330034593
-0.05250605247540309
-0.020269088859354623
-0.03961697756869634
-0.03320756997478346
-0.056663078707187224
0.08942246039841638
-0.008129469671694187
0.03924471320028448
0.08220621876207014
-0.04462354978328218
-0.05981093643818189
-0.01988973588129553
-0.04134642886139751
-0.015147821676394217
0.01657177228392942
-0.03078043398259815
-0.0032279697709537373
0.03459262040994696
0.012406130262919167
-0.06512945539436035
-0.015585909175969206
-0.07875380812978211
0.036944432243138224
0.00888931570661686
0.025915511309271756
0.025219727114566298
-0.057542128400690604
-0.06112056557603904
0.010648639044921679
-0.044185642953268406
-0.005604671079273727
0.0014215396347189435
0.046401321400275024
0.021642053614192306
0.047031045937404384
-0.030907870308338074
-0.032439073345739165
0.0023354107521650555
-0.04576997133712778
0.032980253160477584
-0.07579610998740835
-0.03188997987153379
-0.027705010285058958
