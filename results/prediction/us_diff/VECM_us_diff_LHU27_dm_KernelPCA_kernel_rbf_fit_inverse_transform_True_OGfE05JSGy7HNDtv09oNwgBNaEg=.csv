# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; LHU27
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=7, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.021025651824345362
-0.16436397393793506
-0.13092337050862124
-0.03655218735970999
-0.030043981627530487
-0.02616904801432777
-0.013081645310771063
-0.0041313889106060885
0.003358016840783332
-0.01752631289956414
-0.02975049168548537
0.0015027135184329452
0.012580582146671966
-0.0036309636219436954
-0.022288012487688913
-0.023327706520936922
-0.05259797887562875
-0.029247657510558966
-0.0392740103226686
-0.005749237994134392
-0.045674821025142615
-0.03775287049300861
-0.03781018378395153
0.001983611552173363
0.010112493040343506
0.013745948722735496
0.007221166592551827
0.008840216840230116
0.05607193813139037
0.07995569289082602
0.06311817692265334
0.03914776649963541
-0.006130566799691933
0.018121803678224913
0.09923359929818268
0.0859701002820102
0.05134739642605106
0.013273565434368771
-0.04657510250361137
-0.017486837910074538
0.018617720264187617
0.011900191501618541
-0.04837414493185835
-0.056164839433234026
-0.04172075149525224
-0.020589660767219665
-0.07540540294883284
0.02387806865316957
-0.02318103932333422
-0.007098953814152722
0.010348969517778403
0.0402178232482414
-0.033433189037662736
-0.04430952895648828
-0.02858149957564944
-0.01683733882627892
-0.016403755265957193
-0.04328522379326344
-0.021407013900275385
-0.01675752570626379
0.013429116183306094
0.01447904461749274
-0.029484405831148662
-0.012607505903927637
-0.005040971800777226
-0.028795308012798265
-0.038063120143935196
-0.012587281429500618
0.02846451585183861
-0.0009128814401425283
0.04776975394479831
0.038313972385835504
0.05373671364790003
0.1002300772841617
0.04784577488499545
0.052011752604642025
0.005749244978527778
0.04702977296414683
0.035797344644798224
0.05971909190887639
0.021749366976163486
-0.015449157484229814
-0.023392768095595422
-0.019607147664850562
-0.04919551640297102
-0.015257975949018694
-0.012467251326650029
-0.034372590422126294
-0.03502839789037307
-0.010167157389633634
-0.04041694820725896
-0.012066982898369582
0.009838255181106221
-0.04314427016316984
-0.02000376130850918
0.007895287111808109
0.03989513821645788
0.03609165679610636
0.022933887035861175
0.04322248409021565
