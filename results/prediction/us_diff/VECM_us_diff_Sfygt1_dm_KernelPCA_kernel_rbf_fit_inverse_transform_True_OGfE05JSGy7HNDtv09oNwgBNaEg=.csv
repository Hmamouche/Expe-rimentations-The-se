# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; Sfygt1
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=7, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.18775044824156334
0.3219200305046477
0.08170892369670087
0.06789844362792623
0.013438092729809392
0.06011876212157982
-0.06617390718840027
-0.05633034207562229
-0.180431481021439
-0.11111667586328124
-0.1025663736266938
-0.09740009800816202
-0.08546087015464866
0.08119542284554945
-0.05612967864848611
0.09905106687712653
0.12485349076894137
0.1259507394999035
0.10539012418656057
-0.017045008120679713
-0.056960820574634974
0.12239160605129924
0.03410826636150727
-0.013981356517423542
-0.08801566398968488
-0.113616430758644
-0.08448967961262971
0.030534580823210186
-0.01982907517188181
-0.09791275299594294
0.019609846176989965
-0.03654595292535276
0.1096858084097885
0.16404490083075654
-0.10141590940496778
-0.0863905645552326
-0.06694126008885633
0.0794244782382314
0.03899130651953999
-0.10160488113764624
-0.004252534987909853
-0.01326295315889233
0.09523056642163048
0.1762001709616561
0.046773074424319244
0.06943759443461238
0.043928885648278605
-0.046844100764609396
-0.0330325683308096
-0.11287182857894976
-0.10378421023592635
0.034827979012439565
0.09559201901309082
0.007463723906975708
0.053919188731815226
0.024654815576054406
-0.049669792825407205
0.03523073305478275
-0.008089041618627148
-0.06487044433850653
-0.05362251593468314
-0.051759142854551896
-0.001035215815834406
0.0583720459421887
0.040157896456864156
0.07787059448364356
-0.02195028055028579
0.032380105753900726
-0.0364940377990655
-0.029297929021182514
-0.12251812607145171
-0.02326931943257604
0.05172644393158158
0.0016117520909894045
-0.024731307804049037
0.0629150971347194
-0.055636301204894
0.023268877491652185
-0.17332157959504382
0.06001235506367249
0.12856960302035186
0.04234614438247534
0.021259565258776033
0.022449304261279293
-0.02878908471554605
-0.06033138878740062
0.05977819394662337
0.01759360875401906
-0.013271684510597735
0.03292259689851677
0.03908199096234549
-0.03605037047393302
-0.006003432517322153
0.034783178298907584
-0.12305313091109998
0.04483931433571814
0.03869367016663254
-0.0883772796455442
-0.11070811415345636
0.010987463932522194
