# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FM2
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=4, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.008997195720160688
0.004019242152030818
0.007158132944765711
0.00401056544720626
0.005597144154372298
0.005931092481275121
0.007336201750387302
0.007588200020219477
0.005869678370325087
0.006489085760118351
0.006811656294336513
0.007939305274868748
0.006406800407013878
0.005922786193160507
0.004849974700674606
0.005053201372289299
0.005737030500953933
0.0035408332618731622
0.006640710078405103
0.005189735363614813
0.00401354742069443
0.004617789207094354
0.0034467051025447726
0.005032503072847199
0.006821198050949676
0.005952643787740512
0.003935220748172051
0.003415518231244566
0.005688031703939775
0.004032655063138645
0.007096743989278963
0.0048842949900648035
0.0026843905436595462
0.0026173304943199454
0.004606078275849337
0.0011611927961218747
0.0014898681879353297
0.0015121759640385393
-0.0002053459969192815
0.002043589104386343
0.0006190104686901764
0.0036374671807073407
0.0005258056725999181
0.0005942414462517047
0.0012044135656031796
-0.0006424574277620651
0.0020499604679231173
0.0044935300220841495
0.005248471009378038
0.003572336093388352
0.005030895437213422
0.0034607428065666487
0.006367793138244149
0.005371134432920254
0.005149543285111578
0.005941506509096018
0.007681114979350513
0.0065976585100897284
0.009499618612077425
0.008499994791672107
0.00954833661312412
0.012418508587528623
0.009676078549074322
0.009052528319700211
0.008591706969235908
0.008517456971760458
0.009683317746529384
0.010266416877643994
0.00804058843964545
0.009126855720375047
0.015406046677166014
0.013567186805979209
0.014192734928913044
0.01420632642887688
0.012231139051308699
0.008884929491855084
0.014993929998269137
0.013933071979712188
0.011976909377326546
0.01495273630482866
0.012471118441859086
0.004173003150365636
0.011149675171600062
0.008356156858541647
0.009036911791882051
0.013527095286800383
0.002743810398298994
0.01365688084204721
0.008422052402328517
0.00941111090487319
0.011332627422787825
0.005597519002790244
0.01229747869810769
0.012113741289196267
0.011645222873836816
0.012282577509388453
0.012544167979375275
0.013483780232900238
0.017639616413399967
0.011456739227971496
