# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FMFBA
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=12, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.002993035109488567
0.0020349765056846777
0.001926648330256491
0.00173399415570671
0.0021454087739714582
0.0029191111461679756
0.0021533944844407387
0.002913946730140609
0.003475314927973333
0.0033169689970705047
0.0023986457526753782
0.0030992761315940566
0.003603399381340462
0.0033162109008704845
0.003946208747119759
0.002566793072738275
0.002929795201336486
0.004162778043023463
0.0029542205202217533
0.00318431968174696
0.0025528736341846213
0.003461364882495716
0.0020873149655085414
0.002740669014273445
0.002696586089912972
0.00225530990156114
0.00348485920817072
0.0032345240159797044
0.005085770268644805
0.004949293353716473
0.0061348550405868965
0.003696154927011259
0.002735354509663929
0.00599967453964964
0.00545716314277925
0.004202024652737388
0.005718888132531512
0.006287369590127869
0.005711271392069619
0.006812502388781955
0.007336217370173449
0.006326466913806816
0.006879254362638413
0.006933826373256892
0.006535040618001123
0.004951508396552628
0.004867291660982714
0.005622609441483907
0.001648622759416314
0.002224492530080205
0.0014493076170556552
0.0023422877579511695
0.004159969199617351
0.003277940374634947
0.0035060219491670668
0.0031881616141225515
0.004521781317986504
0.0066305895945056766
0.004863871579205224
0.004738461425608068
0.006639312695090869
0.008361955629388894
0.007612151594258815
0.009842291946435561
0.00932131412178401
0.015189459041021702
0.017446060221406545
0.009687401315192561
0.022092553249574723
0.0049802782850887125
0.0031256116300565112
0.003659311866647407
0.012579164979652852
0.007977660120690006
0.012101709594207274
0.009340336326870414
0.011768853812837647
0.005329384195609364
0.00871418322879945
0.00860873403876116
0.005620776658833501
0.008536175599613944
0.00438074395703842
0.007725963674386638
0.005722900645763095
0.0037035144157190874
0.004955432809763768
0.005800035829888932
0.006569756381759211
0.006705220430761627
0.006758952681784239
0.005784985183133776
0.0034684379777195767
0.003388622892591408
0.002337165975131999
0.004665528549940963
0.004549324551093872
0.0030742684210169537
0.002077737798206311
0.0030413569372042984
