# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; LHEL
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=4, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.02033867680993457
0.05674077869693874
0.061596895108574655
0.03997229475726144
0.01271289438583358
0.060579819183055245
0.05621003075181271
0.024291687721921716
-0.01388083455565336
-0.017133343245874674
0.006784233258418537
0.012382071604572947
-0.013770103476244244
-0.004758661563204428
-0.0025612112255829758
0.015797601273334193
0.05163708992979507
0.02017895463675542
0.014000817590494204
-0.018223166302094338
-0.0012101480483223829
0.011665983071050578
-0.02419806891438631
-0.028415323120656436
-0.031388851747338964
0.008102757590744554
-0.030222973632027514
-0.07322884935661664
-0.08190865100615455
-0.13883287303790026
-0.0680773291740364
-0.003985733875764745
0.016791660275306486
0.0026606548776317556
0.02217518714045578
-0.0017583438872858098
-0.005936165088490755
0.025812398262458906
-0.00048721165881790256
-0.01401681025128536
0.021855757575191245
0.06056011194661336
0.09866471775198422
-0.006775447226643553
0.039163308064181765
0.016488112459645488
-0.013562983200351708
0.004805299359277988
0.01586027458991028
-0.0005079808092829639
-0.006329404086695948
-0.0238982238716042
-0.012624853358827263
0.015522926218480023
0.028557027795947876
0.013630247246041009
0.03744726296884755
0.006021529072671672
0.03007638114849527
-0.013849902457674404
-0.015729179606941972
-0.018238212318000952
0.022148831451085833
-0.03777679773175769
-0.012511635651671129
0.0036333227236877144
-0.00933742345998419
-0.028138643133658225
-0.032010754843683896
-0.033300489506693384
-0.06957375369818394
-0.08860679168737984
-0.06150321738484982
-0.10582098257571736
-0.03171831704700733
0.004931580938773177
-0.05451384933380286
-0.017471674219768002
-0.049457695988657895
-0.031138837305775786
0.031263462233973915
0.02562082222171748
0.014837028997923905
-0.02263907111735749
-0.018131785047141427
-0.012148048384178975
0.0028524548218790617
-0.010595363525176562
0.00040830278831613213
-0.013347508510047863
0.0040982858534707985
-0.05796278496338764
-0.04784485129419166
0.0025091671265193814
-0.01040038824989315
0.008707070536241156
-0.015572324795958475
-0.06315253868857874
-0.026911613981599213
-0.031772570942628225
