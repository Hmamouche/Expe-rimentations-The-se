# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; EXRUS
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=15, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.025440566449047283
0.10518151146982148
0.08345008111138624
0.025130049052673003
0.09818728236213105
0.06412803931440751
0.031135259747144137
-0.01886786151694686
0.023243202517742853
-0.06798520697044916
-0.1347163819517848
0.015372861100514473
-0.11512435477333167
-0.030357175641753366
-0.062038419569912504
-0.02858134288492994
-0.01916325991827576
-0.10920198292435357
-0.10046474609665432
0.03507161484124724
0.049355480551034184
-0.06061636112096564
-0.03002820187099425
0.03868989250690706
0.045187194370155746
0.012102429162803092
-0.05987258211833604
0.03835044225598464
-0.11261519313259227
-0.08877274918107561
-0.0507149533034519
0.0452659709670967
0.012489907919810765
-0.05807444839527218
0.033037225189614
-0.008482967621240664
-0.04082453733721371
0.05724445005021402
-0.012466101203331928
-0.02976934319560679
0.03124454329771808
-0.04107589730835382
0.039413663016140445
0.0064465937210310105
-0.03789952307342644
0.09586725652610471
0.02268381973830932
-0.06038190042298071
-0.0018825106628889855
-0.007908349369509272
0.0026879234821734835
-0.05736455805402408
0.006063769185725831
0.07045398867692351
0.016319293045883813
0.0001660371397571754
0.023751575744422045
0.047580291554877564
0.04029449020617613
-0.0013499267170668378
-0.018518904776430367
-0.04025179740228082
0.012303898368351144
0.005993875095144319
-0.02781104787151352
0.04696649087286406
-0.017233601138043203
0.007837691364332641
0.017223012093039673
0.03844658914203525
-0.015587019339345148
0.005149315824281766
-0.013504218771155598
0.07441969818672432
0.02227642247085354
-0.054468343850655465
-0.011706107043492127
-0.012177683041486823
-0.09701405950535773
0.027771722807327974
-0.035751044147432964
-0.09780838831246212
-0.05868995391661616
-0.00531152638592621
-0.028494673830183774
-0.023787250669871668
0.007890721490535647
-0.029766299008466727
-0.018533149848115906
0.027980251005276555
0.03689630610447327
0.011351765720626348
0.006603127341405855
0.08334153434510595
-0.05695652039168678
-0.06965552868010119
0.010620404028994652
0.023752083461998905
-0.021969084998257647
-0.03461195308353561
