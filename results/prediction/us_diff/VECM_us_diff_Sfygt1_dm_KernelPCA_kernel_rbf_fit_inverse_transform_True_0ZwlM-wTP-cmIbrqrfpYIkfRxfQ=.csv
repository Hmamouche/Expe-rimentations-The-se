# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; Sfygt1
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=13, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.1610859439102402
0.3630404670007391
0.2572482635038657
-0.22057771533977466
0.06683961139563518
0.1594687559466037
-0.15925328058562552
0.16465298824609914
-0.41968094425728053
-0.04555045860846235
0.1436843216755863
-0.11965152336592198
-0.2240541782060686
0.06880224516407624
0.06872569370007323
0.09099714226561664
0.1708343044072868
0.14994908102220922
-0.0064944137204922155
-0.03453273562149084
-0.12819542183483856
0.11586134299442719
-0.12603067517396768
0.019095431609962153
-0.21856928225906774
-0.08309020248059334
-0.1005923886207213
0.186018554081959
-0.020708226878581704
-0.09642629070254263
0.0032848195676496057
-0.11682736269033012
0.2095569861378116
0.056823931787429324
-0.08597048779037494
0.0038191080624696717
-0.1342635990024517
0.07427300053211143
0.2647994658708436
-0.14837102890105797
-0.0337040069781844
0.0040759633934313935
0.08042827806473125
0.08239507276476281
0.004076539806993209
0.20867070561259962
-0.037720927517479574
-0.08710037695948422
-0.12181937112848756
-0.1370938468314187
-0.08602972692322736
0.14748000568844719
0.08209423142102369
-0.009348960332083624
0.09434151773789104
-0.06669511332068956
-0.09313099037714143
-0.014346997878896598
-0.051576582326366346
-0.033338906532174764
-0.004836665472350163
-0.06554466824585252
0.07968192681289012
0.14333564361965864
0.005418759706767952
0.07149348020850584
-0.032584952051857505
-0.0352888453758616
-0.013451812003529521
-0.0749127103603723
-0.07551148968644665
0.04079115247642495
0.06373911224284513
0.08149111923112208
0.036151456878057375
0.0637089650324611
-0.2440267601512785
0.11962747373883938
-0.10877983899893381
0.06921651142287258
-0.023566160864678498
0.11485084889632026
0.14861739136155944
-0.0969496610062437
-0.09330249171379643
-0.02390480756803126
-0.01765473458301051
0.0547817265976137
0.06166116936878055
0.022483190505619602
-0.057762655470588634
-0.024357165670588564
-0.07997745123037353
-0.12514441684777397
-0.06371078771161132
0.05124853811272938
-0.009503926418642769
-0.07512282616086158
-0.056681218152953954
0.019263466211162948
