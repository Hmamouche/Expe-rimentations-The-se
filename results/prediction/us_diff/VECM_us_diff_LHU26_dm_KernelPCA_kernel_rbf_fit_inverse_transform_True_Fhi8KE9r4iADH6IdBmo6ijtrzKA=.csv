# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; LHU26
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=12, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.0016780768682704839
-0.14963257875931907
-0.005287375834676112
0.06356202290730448
-0.03865308961646743
-0.015570806345909516
-0.053646071177534754
0.0007440623249030144
-0.023472919633750807
-0.04269200718893711
-0.0030810980976182394
0.055828894898846806
0.009869803456591712
-0.036012890137833264
0.009363465947967206
-0.02602359988543266
-0.06276441766172564
-0.000421167887951282
-0.0054514515030539695
-0.07738382614594996
0.020503538899036267
-0.0831889115660536
0.05820699317017139
-0.01616980495874983
0.048006039826396354
0.019595908524412002
-0.03205529965659707
0.04275817860745877
0.00592152940384047
0.10554735041859825
0.11589326095173108
-0.010602620485397793
-0.09651971427303226
0.09153895779626174
0.07110677782240024
0.01516400572206537
0.01119015829768871
-0.012689590357995172
-0.05379499291465036
0.0036226591327681752
-0.008703185905713062
0.007192084771829793
-0.0432785546515618
-0.05253178104056915
-0.04263501267442337
-0.05755202901793091
0.02265148200268599
0.002664221165979634
0.04244532531190178
0.07675188616162067
-0.008955712327670583
0.006028970475056336
-0.03946192569464435
-0.07886730488986624
-0.05023413137265345
0.023832154814423245
-0.05739035186344217
-0.021397455439152103
0.02569147264562863
-0.024629626395154584
0.017831744847011718
-0.03408620640047452
0.01082333684101615
-0.05326860184943875
-0.002282419506360628
-0.032811437156646195
-0.026499403957000463
-0.016465202584275723
0.03308815161926849
-0.021279495182684488
0.07350317683918758
0.08850461840726616
0.12226242697567304
0.1243637803060774
0.004665643017703527
-0.039588487773176126
-0.030101124819729562
0.05845813873390018
0.06605035760692472
-6.407721265988715e-06
0.03304305355462725
-0.06667353257066533
-0.013733306584113529
-0.04094237628325474
-0.025702296403433265
0.0044505462591202145
-0.0052554792873088976
-0.024083633800777937
-0.02286076493733484
-0.024609434322908748
-0.006823750300138518
-0.026345720294142838
0.06896335227849111
-0.07431363994352493
0.014407008456283702
-0.006113931526139156
0.03637908930678788
0.0678444093434944
-0.011443141100413995
0.07506816466386132
