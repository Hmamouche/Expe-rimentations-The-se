# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; PSCCOMR
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=10, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.05026593537958397
0.1180555627803497
0.008397466193988318
-0.047309206073691254
-0.03111599158991285
0.018381298926742437
-0.03212552188164058
-0.02147407741421898
-0.01074304754759701
-0.005883139430251712
-0.0023996492638796146
-0.06276342940176366
-0.02342430035097736
-0.0034344071481852874
-0.03735821341357274
0.062221951060718636
0.030852423205354057
-0.052051120785864304
0.058710249441065704
0.0007020190375847182
-0.02275308510772015
-0.006548559895801883
-0.03471636958407408
0.0027639133119431412
-0.025930376735169448
0.032893119737216675
0.02518506175835676
-0.030985515878263478
0.036550815009702475
0.011173962962783052
-0.07184231554457503
-0.08661270349065989
-0.028266852871989073
0.042306087687271635
0.0162127952772169
0.031008707338591755
-0.059979333479132674
-0.04628029521557034
0.012344426317358864
-0.024476344606144767
0.02260336141715214
0.049733725909372035
0.02359143109677162
-0.031656853596871114
0.026643112478115717
0.038028372501946066
-0.018312262042035857
-0.02591506903353577
0.013997683625369354
-0.007799657650985219
-0.030552415796757965
0.04602392128755725
0.014852708509251234
0.01740124007673253
0.029096129293563318
-0.036239417355487136
-0.00321906204907767
0.0032858929995439063
-0.04340503142108094
-0.01978407817731541
-0.03827657380141276
-0.021237546007478847
0.030416071383442093
-0.0325102988350465
-0.004150119574792055
-0.005434921536075569
-0.00654980851626942
-0.05965238999703907
-0.029976369661312317
0.0052534482121712025
0.006907491742352182
0.013967712751002
-0.03425866860200256
-0.03202084011897511
0.04666393158390133
-0.0012220026397503497
-0.02160343380548086
0.032651039370276635
0.041869693041370254
0.0033071250513009047
-0.014768597115994592
0.06595797181701614
0.04948940949590866
0.022207214766421686
-0.01849660978992636
-0.023975980144803122
-0.025999001912755025
-0.017010469030966435
0.022203260954917314
0.035866704452551496
0.005324813723896128
-0.008361482357789832
0.008545672681294681
-0.0008018921232701946
0.0346468653406644
0.043297027580150485
0.03411134984936015
0.012361751736934266
0.014073691069322974
0.005779426257793692
