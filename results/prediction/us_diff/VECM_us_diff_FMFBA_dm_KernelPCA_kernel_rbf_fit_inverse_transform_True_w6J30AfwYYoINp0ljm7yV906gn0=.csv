# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FMFBA
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=14, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.0031580395832091043
0.0021332980518603568
0.0019093393481622607
0.001597787438777823
0.002371827879806478
0.003278580874316906
0.002119627841019406
0.002506951940379368
0.0036794290440807214
0.003475518351083668
0.0025117503091924313
0.0028327034575164702
0.0036843422476067187
0.003350778129013178
0.0038164332807181242
0.0022232192455894466
0.0027747955274735436
0.004170880916687114
0.002786796305641743
0.0035933479962207467
0.002904648369809559
0.003262619348851325
0.002007475829966323
0.002824653021535898
0.0026581712759411956
0.0020645477476379324
0.003675626195457765
0.0033269054285525804
0.004784872095047824
0.00497566581505596
0.005836578382580775
0.0038921905595755034
0.0025725820912685378
0.006381118765630103
0.005827896228386604
0.004278966445678561
0.005472501298036543
0.006061445582814092
0.005936678439633107
0.007015925206846874
0.007080793253408275
0.006093234665775943
0.007026505765610967
0.007139969620014699
0.006656294344542254
0.005037397222645367
0.004792545493653164
0.0051939938562534736
0.001695400005428808
0.0024236944103348717
0.0020003440218868468
0.002429521392005064
0.004586181200796121
0.003096412639247722
0.0035913289087110612
0.003240991921535084
0.004824246737445284
0.006298539637423734
0.005178554194958777
0.00441755935648684
0.00706615903629201
0.008760478699199927
0.007510294434874405
0.00996866738667281
0.009284212002522893
0.01586738168751156
0.01898317217637009
0.00974570983882123
0.02165262018362779
0.0031333906521791013
0.003256385745484168
0.0035187971885669933
0.013275036171795932
0.007619691052395885
0.009948887957431254
0.008726815852734491
0.010122079452633357
0.004788094370972263
0.009182277574154084
0.00789531876354715
0.006534633391025023
0.010651193786235446
0.006765947373704317
0.007900599772616434
0.0058849470397527295
0.0039322187663792885
0.00456506322977883
0.006307708244188983
0.007329654604436106
0.008199993213826256
0.0057161278993913545
0.006006497195911948
0.003722553720909629
0.0021570659252260235
0.0010826601024850752
0.006643190644269328
0.005976368077915033
0.002038081533683964
0.0032576173706539736
0.00263668190613136
