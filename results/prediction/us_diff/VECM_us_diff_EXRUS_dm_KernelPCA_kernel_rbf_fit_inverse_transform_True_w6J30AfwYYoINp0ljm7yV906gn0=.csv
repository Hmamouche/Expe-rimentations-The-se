# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; EXRUS
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=14, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.0066389055349553375
0.07463354682674542
0.08337530326440967
0.03468443926021995
0.09253713119577302
0.04816835210148098
0.014164752786252323
-0.02396012161780451
0.03073423434610518
-0.05313186394956057
-0.15651877450780843
-0.013818866117725603
-0.09955964251058894
0.0023927611224723866
-0.07443889458104608
-0.03276464625370183
0.00791716554045321
-0.05812587292891546
-0.0765833452649223
0.00590851642775532
0.02296869564305838
-0.05562966411977608
-0.01593920428169709
0.021885337677070554
0.039442140369898335
0.010701149492908226
-0.060904000197195295
0.05670991198612226
-0.11733038787235552
-0.06718900851273492
-0.0431014761111998
0.004383747111197782
-0.006081194967115863
-0.05043498281419227
0.04647429652877175
0.00122297709747074
-0.047132796177276254
0.07379705194069756
-0.02702829482783506
-0.03667482417343876
0.03943379947547876
-0.06763439144229855
0.03175215692205732
0.011987931965413225
-0.032011797949310264
0.07338145870679108
0.006901658034841267
-0.059436917566265374
-0.007976225064428208
-0.015106031421934694
0.020696096423267647
-0.05233830062757462
-0.00015672596245340856
0.07465073872301267
0.005624850663682037
0.003852609844941273
0.034870763556035037
0.0566604222310306
0.034447858138766566
-0.002549547802731124
-0.01189440717455473
-0.04037161049570932
0.021495510548304446
-0.016758055409969327
-0.035190031382804784
0.021923137698077175
0.009884566205247473
0.022547463695867513
0.014720095836880963
0.03642913180777892
-0.02108454840375698
0.015350293883196482
-0.008741039300739117
0.03863130076815922
0.01985632401318714
-0.038638897071607795
-0.0076247429633582095
-0.02048630569303684
-0.11007480744285483
0.01192808577644709
-0.031195877325436782
-0.09903053181719557
-0.04083402273834712
-0.013001208230118912
-0.02891715961504009
-0.014861226968901804
0.001603380621536421
-0.021615216159356063
-0.01718733384291373
0.03114630901171122
0.023726848726054205
0.014241046862706122
0.0036904841888237266
0.07545881441596786
-0.053160350861620555
-0.06888241795671146
0.01938576512069118
0.008603445508050355
-0.015335338004772853
-0.03732961840177671
