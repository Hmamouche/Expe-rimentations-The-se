# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; GDP272A
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=2, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.005213830965522415
0.005573738700923208
0.008701197293970144
0.007431517619947409
0.005880563877079298
0.004424981378595025
0.006885686958438882
0.004136605776313066
0.003433845128342887
0.004558446676244025
0.0020094276647213347
0.003593338412935355
0.0046916005897043485
0.004339254219057769
0.004979332881638055
0.004443313977505605
0.005647098512334412
0.005552962922527706
0.005507090062202107
0.007050938196299143
0.007946155711110124
0.0068396002827327725
0.008129429738967724
0.007479062925080594
0.005496385506648089
0.005906674408967598
0.0077810522015080786
0.008004463956930396
0.007424216064721906
0.007632774040304772
0.008055461827143968
0.005561404363862162
0.0064436157448587925
0.005781885318940512
0.004205140722999316
0.004956774790123854
0.003419195728416577
0.004859810680540586
0.00508992622948788
0.004349534573380107
0.004678634503553656
0.005039530604969761
0.004875812579743205
0.0046873604931018786
0.0057882866028855975
0.005153365295153965
0.0047646366050339995
0.004249822708297396
0.0035872839617642777
0.00440960114720248
0.004015656288772175
0.004297162562854568
0.004347131824751137
0.004605206349889805
0.003978799589565617
0.004237969803846029
0.0031405469925919463
0.0037038169386754112
0.0019493247153736112
0.0018669156494260492
0.0030815540882833826
0.0022723389112555767
0.00312162635020236
0.004267204818266581
0.0033446251886480385
0.004697193888646844
0.006798724663411746
0.005073109670573386
0.0055395211656872265
0.0053771614871651
0.005015057980128429
0.007115471392964212
0.005144136382076437
0.005888556233918726
0.004816049092781263
0.0023113366557668933
0.003922710362332741
0.004981568422181705
0.006812947328001304
0.0038663141416807757
0.005982321245209749
0.006421363417353297
0.006774218861097223
0.009887382135535026
0.007316619954117098
0.00935039366589512
0.009885748276787221
0.005749278010798617
0.010188985589765952
0.010178128192209613
0.008591266752381446
0.010272572317650379
0.008347503136441102
0.006878131630090103
0.00902014944648759
0.00739994613849395
0.0058458622958070085
0.008717878427153177
0.005969099797700533
0.0046137392021976574
