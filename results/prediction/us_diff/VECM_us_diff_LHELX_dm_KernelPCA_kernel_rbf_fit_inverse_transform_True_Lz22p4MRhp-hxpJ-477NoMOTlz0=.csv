# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; LHELX
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=11, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.14531823980682035
0.02693287398515428
-0.013029094288315026
-0.0037626179448482927
-0.006479994383136414
0.052707474558236364
0.015551720274444764
0.016789647376772826
0.0029137913637446236
0.030444468820056324
0.03489535307115234
0.003493259683863436
-0.03329803850669306
-0.01008986578667332
0.034959344086762455
0.03693230061723569
0.047425152235693165
0.0034838628755040147
0.020607847566786547
0.016481249128354282
0.04610217795994046
0.07758063706715751
-0.04250998601112744
-0.0322209196119503
-0.012016103646338205
-0.009641443076160555
-0.013763441421701883
-0.06314271364771083
-0.07356774727451677
-0.07567570015296743
-0.07998506302477586
0.012609807075522214
0.008859102399637782
0.016942155053825795
0.03222243223457029
-0.028242332587108693
-0.033325823629425856
0.009771524450454202
0.012638632658746448
-0.024327605620546117
0.060021201215351275
0.023078365363077127
0.037929116426526244
-0.012389355176047452
0.054629813757790746
0.03362146518502622
-0.002960319914860841
-0.0006274706720941327
0.021732822849540617
0.0031105958423900872
-0.025548830796333313
-0.0007733369677118233
0.012579531809380786
-0.018607637805146862
0.05975189904175479
0.00783947318399245
0.027071156624629184
0.012017651373498302
0.031052646809830057
0.0017760666007400998
-0.01812122246930064
-0.011446983455170272
0.0187526382462125
-0.0259894905659677
0.013216065565204481
0.02636761544721728
0.0061230433358263805
-0.0035356134641084565
-0.032874648071433636
-0.010174382908538905
-0.05955126353982526
-0.056954332678256975
-0.09126631937085307
-0.06288290348248637
0.0311407759487752
-0.02585630298213399
-0.03660139048265858
0.006696533050598475
-0.05144751690950759
-0.01586362832789949
0.02433090582812821
0.04201522726547422
-0.010754637228734255
-0.045826162663106115
0.023420593858944204
-0.004221722199764482
0.00023730950757469473
-0.0038360283554790212
0.009520024565580421
-0.004133444751623893
0.021364545621391024
-0.024354532409472798
-0.02156823235346233
0.010193463263282096
0.0019592614683094523
0.012349792012938367
-0.01312707291456652
-0.06130954288205011
-0.01115989235132473
-0.014042942352460138
