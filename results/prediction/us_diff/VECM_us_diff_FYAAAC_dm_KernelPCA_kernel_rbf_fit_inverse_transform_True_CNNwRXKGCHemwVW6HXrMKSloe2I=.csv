# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FYAAAC
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=8, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.059966398710988016
0.0805097702850604
0.08074213209475221
-0.038713590126281804
-0.02737527499275093
-0.0482950948591796
-0.039693245090717505
-0.03984045278219108
-0.022549934361988738
-0.03052211918245476
-0.05184557080622299
-0.0031644862792149456
-0.06769906749273188
0.0032397768522124323
-0.05469708275752873
-0.01922609478119425
0.0607062004204343
0.039980356922769425
0.06630777316898953
-0.025069070591282588
-0.00042691372684629074
0.02231539070427247
-0.01016533287463341
-0.04305343836789022
-0.03875253354413623
-0.004267143824810133
-0.0019417955254939431
0.015769620290480915
-0.021448997545693665
0.0008691924230888968
-0.026282726805462342
-0.042563893739009435
0.018209831424714232
0.007638441381233284
-0.04925286508915731
-0.006322810074069648
-0.03334160159554575
0.009311062146066907
0.034943932115568725
-0.04147740631048226
0.011694440167013433
-0.018874842211518358
0.01698579265685263
0.023015928230161537
0.03091572900127859
0.09926748565104637
0.013437348435168129
-0.02150615299751661
-0.0262264362201551
-0.050924102300461654
-0.042105594618724616
0.01289840353082359
0.015269736785181226
-0.022114932095083112
0.02304949772468244
0.013239931660553862
0.01041366723707924
-0.011728433239675397
-0.03392736016013912
-0.03073088797897891
-0.022285206909942498
-0.06496986728928293
0.02850506060143071
0.005985023857510596
0.027793643772058365
0.0579266160735347
0.05066190645400698
0.002698643865005859
-0.020333846105235878
0.004905944845889203
-0.036236743871862064
0.0017013493708869266
-0.030344947076451505
-0.06837251505629988
0.01583600518903826
0.006071137987291917
-0.03972865951881234
0.010985128757879024
-0.019321472955561653
-0.03137907628918186
-0.00744520746897049
0.00495178152260429
0.03381971089991603
-0.006607862456193987
-0.02631095751855428
-0.007249948114858747
-0.02892818549882919
-0.004437605037048658
-0.024650931437996817
0.021838132352971756
0.015906967509813925
0.004915178798044163
-0.006037863569292475
-0.0026253454987122237
-0.0073700037037628865
-0.0004433146545191058
0.020199785343529897
-0.04323814305886651
-0.01001367197830225
0.0017572706442280234
