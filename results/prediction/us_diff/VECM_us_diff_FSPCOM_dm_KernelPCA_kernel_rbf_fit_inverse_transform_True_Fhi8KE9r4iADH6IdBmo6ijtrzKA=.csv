# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FSPCOM
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=12, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.004610140072319656
-0.005457574392449304
-0.0013586530852496184
-0.009305184953407592
0.0054328166095188105
0.0009433691833937905
0.0031706839009929934
-0.003167458557577214
0.00836122205441975
0.010726463445429052
0.01792271817452242
0.012111810815981663
0.0017100792018006606
0.009097252950812507
0.01397344006243748
-0.0017020587616269718
0.00031587027878793154
0.008467112689726077
-0.004900968242916711
0.055091608804235845
0.007892738270601546
-0.03291410930821202
-0.0006798022385087289
0.0015903413745785404
0.009126974729971978
0.01721285173704838
0.005316928717051284
0.006865825726047724
8.965488149306397e-05
0.0019975039665823635
0.01159990373453872
-0.0068446413844912325
0.000626329927748592
0.011261193131693534
0.02049003617302571
0.0008622638908590232
0.007831506207563338
0.013039710831028763
-0.000670035809815438
0.006618458433001211
0.01433722682240478
0.0037021105951883567
-0.0023329265980792345
-0.007370767028855712
0.012073338483712257
0.0007343889076706503
-0.004458604534926129
0.012274608378602874
0.034479987110804015
0.022272809384176757
0.02540213768059618
0.008113570159605475
0.013339412661276381
0.021177833823566597
0.03043255231979558
0.01934479327250591
0.04447535843529903
0.042446366441574
0.055244583644640875
0.03351916581849645
0.05447064376691868
0.03556940982055652
0.0015175898058665763
0.02845638699256023
0.0378325392162707
0.050878927784388864
0.047251365404959694
0.03896465187100154
0.023991134185172736
-0.004610299209308579
0.0034977089656071138
-0.03166091123759434
-0.0554409534817789
-0.025442408416641256
-0.04096803925907905
-0.02596539445098731
-0.054989804647395515
-0.01609846604322428
-0.07721409436856713
0.001635238304737512
-0.016860552691177453
0.021421839535212504
0.03868648304408477
0.010024250453094238
0.003666297205340969
0.03167948990299142
0.044417845469198275
0.01609660435665983
0.00486794990758033
0.007842627802128693
0.03873434815109351
-0.007178293489424258
0.0134750498750696
0.054946757619777895
0.006252940772189358
0.04821472380978827
0.01374339714346598
0.0376298640070433
-0.05763669495959674
-0.01686201672525257
