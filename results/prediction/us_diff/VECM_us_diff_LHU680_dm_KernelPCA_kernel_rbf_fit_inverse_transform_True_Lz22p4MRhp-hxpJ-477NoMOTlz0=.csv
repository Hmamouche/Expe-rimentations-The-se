# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; LHU680
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=11, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.040574754332325276
-0.11308615610046904
-0.09039540107607344
-0.037996734512553296
-0.05853158940025742
-0.0005800060355523745
-0.033997881180241954
-0.002346034560483653
0.05123215922527978
0.020752184569728528
-0.024723066685234878
-0.01345491401574601
0.02115075808720565
-0.0513320996866073
0.009273064145756087
-0.002532365656098965
-0.049137498276933526
-0.0633795548556745
-0.007540137434782367
-0.03169868719205036
-0.00873781789395375
-0.018500252494985357
-0.024953402689310712
-0.04834984842124027
0.013793875803098427
-0.008280004616653368
-0.0015194131271522582
0.022178439322207755
0.06467485237250467
0.031174872802633355
0.07866963390154579
0.06365489673006659
0.02336826104319579
0.011096566524845794
0.06801030469211274
0.035240981975131216
0.03520208950359527
0.04604643996300203
-0.03775614889401744
-0.005962965430964794
-0.024158260010105632
0.019861461961271677
0.014972363666200347
1.6858521573225385e-05
0.013199502231615728
-0.019300981812939585
-0.09839742175719537
0.028287008020415005
-0.04278798569174272
0.022144278984420188
0.007877554159691203
0.03922480333188724
-0.03163546445038849
-0.017339750831765473
-0.03091991026895852
-0.048281506286379436
0.0005864119772692575
-0.04406055809479681
-0.003404797478361296
-0.032206778438849386
0.017947599464578218
0.0015836641897025473
-0.024490413254869742
-0.005116624338321752
-0.06912484142344048
-0.013000032155772086
-0.04814713087070299
-0.04512137077081097
0.020420646081680448
-0.02755439312742371
0.08999385157591278
0.05438759320546515
0.116772755522672
0.0941093562618754
0.036010079410846885
0.032456476122528076
-0.042572512879956295
0.05006705021830534
0.009422945996061992
0.07978407149069625
0.07772857009173047
0.012782396885718306
0.03328900562592881
-0.015551499289468345
-0.05584150437936057
0.008146886162150977
0.002866718813127108
-0.029002649853214095
-0.031216028997615972
-0.023733823382678706
-0.039357436656144884
-0.020449499835289278
-0.0019390505465244163
-0.034395750179411703
-0.014856473056167174
0.021484486549337933
0.04700713667038957
0.03364072140116073
0.01243684023362219
0.029020117462859624
