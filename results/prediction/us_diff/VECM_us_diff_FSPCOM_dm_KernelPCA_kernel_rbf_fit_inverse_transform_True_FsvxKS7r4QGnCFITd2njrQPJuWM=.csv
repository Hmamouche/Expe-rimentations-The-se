# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FSPCOM
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=2, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.005285038931247856
-0.0033261543121405474
-0.0033740983287322444
-0.00462613382156817
0.00045525531426126946
0.003972525563856006
0.00413493742912857
0.005936941249894469
0.003602309025491981
0.0058581289935408145
0.011171479981046204
0.010471846365827112
0.0013793072261759072
0.006090710292008644
0.01906338071686708
0.003987194433236558
0.007432572452265005
0.012286391830367593
-0.00970487233103291
0.03360777227480288
-0.0070516769093865945
-0.023058276708914197
0.004457283215043211
0.008922806803064938
0.013157651433296768
0.011476979152654759
0.006456082325229944
0.01193712134395334
0.0061649182140875915
-0.003370205862733627
0.010077985286534542
0.007942919385547517
-3.102397051629865e-05
0.006369316131903532
0.012020428365230254
0.004538807653235575
0.004683185208727142
0.007183071076169797
0.006930037694648587
0.0064634970184435005
0.005787610486909643
0.006555587216435783
-0.0011421557957587256
-0.006656789187567539
-0.0008436603922253071
-0.002091837113891885
0.00034932416974445485
0.016482641076061066
0.020078433828944295
0.021925148097217004
0.025616493672743556
0.01969158094583845
0.010641111477811518
0.02254291884837442
0.02739399170499909
0.024683720186274436
0.048846004087722994
0.039321793684787845
0.04411847443154443
0.041397591371121485
0.05246445954845125
0.03151509003258679
0.019325267083071054
0.03517882693093489
0.048689646627723175
0.04670834327951199
0.038627552489633586
0.023703318528328993
0.02703434966707256
-0.003272419626232103
0.0004041257290038876
-0.03734644653019675
-0.05416881821497596
-0.026890419485092864
-0.03344939558203837
-0.028916585143154743
-0.07204165441245348
-0.011820910816870278
-0.07010569595515084
0.006122990984823428
-0.021073684896713553
0.03183461113826623
0.03957718128507488
0.01647052907934882
0.015260298214378371
0.027296026040930392
0.01988438526152627
0.010566634763374034
0.019554873282504003
0.0048575551240496815
0.03224050777320165
0.0005778718409398531
0.019196804294427373
0.03908265143673108
0.02525112355215236
0.046458941701096775
0.008846810949911089
0.029705118307439056
-0.04565413332135904
0.001058219905990654
