# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; Sfygt10
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=2, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.04442233602214385
-0.11407173670856702
-0.01585175597300604
0.10849980437540374
0.09675377135830318
0.10972200035318054
-0.058376015965922644
0.07925351829487534
-0.04971708575283512
0.0037627838969096702
-0.08634586688760768
-0.09506525889112902
0.007213235192006319
-0.019900572519719245
-0.15404992018461833
0.061758169035996374
-0.05431131828884003
0.13139663297518372
-0.0041308004610551355
0.09651855508644967
-0.017349004752876374
-0.13189227548208815
-0.059623609832723634
-0.02127192498630564
-0.0706738072676788
-0.09485463365757095
-0.07142098118354562
0.020584222909732875
0.14347372034084568
0.16567155895775326
0.144140822147971
-0.057186484872703426
-0.028914369946898857
0.08956627927522522
0.04116747644526408
0.1281167766718022
0.059808681690256715
0.024949309291982046
0.06890875734877766
0.042626422201605084
-0.08658141709559189
-0.07324437684497556
-0.14047183896901624
-0.05490226191137022
-0.0015129571734416036
-0.05363059522262781
0.05006117134408034
-0.010450761101869557
-0.10265062897185658
-0.07335950706230421
-0.08164131094019095
-0.061859409986343816
0.026285350503086558
-0.054238542246517744
0.06379776057849407
0.013747724313447736
-0.06953192933423928
-0.044321223233521376
-0.03698271538865346
0.004393200358907246
-0.03951047029693235
-0.033319969902865086
-0.049602316902449886
0.047262070549886556
-0.023577878487312732
0.00907376963289954
0.01957363203225339
0.007932226598077997
0.0019593127164370763
-0.013008303594940797
0.037766585740375086
0.04235138788134735
0.007331230216571602
0.18474870050288353
0.0377940243721895
0.21534403586491654
-0.008324602389521797
0.15670088608082972
0.008499986457844687
-0.02328393195812148
-0.10138019034367246
-0.06051501487089728
-0.013566221023256385
0.04092760429631642
-0.010654944078265317
-0.024945997838595774
-0.10692358824442443
-0.01912198256387955
-0.14203125128847188
-0.09276402739603624
-0.10174938376371934
-0.0009151339401092841
-0.0037820790093774995
-0.030496303497669422
-0.0502844796141848
0.00983172644163618
0.02830063176538907
0.09551724797830301
0.18575139892465364
0.11184782666063756
