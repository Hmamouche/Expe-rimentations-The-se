# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; PMNV
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=7, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.21832107246359517
0.026918290377439466
-0.08693322743769046
-0.2608724324993133
-0.22896142920233703
0.11496420280764708
-0.09000426039313057
0.08993976295533483
0.1145940872604537
-0.0021256626916109495
0.03165526502182786
0.06206552192048283
-0.01133710870160158
-0.01915666252502863
0.03654640747200391
0.002684812904836211
0.09696854486268056
-0.008934993487413195
-0.11509700206332582
0.08569952061205126
-0.1465638700338858
0.10766217014240612
-0.12497904014435779
-0.042885774363377226
-0.030021869653784183
-0.022429763593154448
-0.01562966341619347
0.11688618424179328
0.0341711337760326
-0.15040232529875022
-0.011897214041680343
-0.07280682993180976
0.03441219519590588
0.16550404560920917
0.10994567163494914
-0.051021159011850455
0.015183956472903867
-0.08728605661346225
0.11902785490666264
-0.08727362113331794
0.0686546244376149
0.03452107329568288
0.04615398430597716
0.06727996752494553
0.07007713104080122
-0.01057703983068932
-0.013000239360148609
-0.03030219741505328
-0.05434932196502107
-0.07521351850889763
-0.06513227671208253
-0.006535257004783111
0.006984878273579506
0.04510188826432343
0.04681013728899608
0.020138332296532897
0.029264132782548295
-0.008486438994029826
-0.03534832890206166
-0.06622227671150781
-0.07641618378681864
-0.0751323031604968
0.07758088148550091
0.019087212927333983
0.06661645057541875
0.13096397740560065
-0.058715119650072416
-0.025257265020306133
-0.07080361903961992
-0.06063510749118245
-0.03337189478292383
-0.04367411940430356
0.01332448937620373
-0.09530001480103632
0.13743992876581979
0.01512488026907425
-0.0285352747610771
0.09368065547572482
-0.12368227117673837
0.040055172779266415
0.06692037418587887
0.10963522090178676
0.1401615031947989
0.004999163531128656
-0.012300584749790434
-0.059474327395193365
-0.09201649216287835
-0.04152729781831374
0.025572677262073646
0.0443419424306093
0.1002657846821716
-0.018477335600384854
-0.05933475465952894
-0.07356544239000279
-0.04209681071821452
0.01776117635842902
0.04372235711281554
-0.008157929602641785
-0.03679508352913147
0.02267609269296056
