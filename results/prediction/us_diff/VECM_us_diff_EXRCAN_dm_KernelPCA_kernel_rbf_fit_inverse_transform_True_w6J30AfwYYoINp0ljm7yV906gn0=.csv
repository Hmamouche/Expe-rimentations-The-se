# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; EXRCAN
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=14, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.04688848679290037
0.0764660226996902
0.021713876024162422
0.0046120093222786936
0.03761555738901102
0.037762399639706255
-0.011521619848359626
0.04103600293457145
0.06527506526419699
0.0030677093680858396
0.023449040243790756
-0.02718087945000836
-0.0699509458166459
0.018717959266577478
-0.014042149236928446
0.001561654325982929
-0.03662278875467648
0.008496319741014549
-0.05448367046273627
-0.06739555490334949
-0.07243868280839526
-0.005670421234787102
-0.03205502742539887
-0.020057621751003188
-0.03704092526890136
0.007481460617316025
-0.008331812044370631
0.001456265643540791
-0.023111637167964067
-0.03249801052765787
-0.04356479308148116
0.006833195008637498
0.003697349051420523
0.004401729674182571
0.022684793151713085
-0.004705562891354491
0.0212769950025534
0.05478294403128243
0.025674606202155822
0.0679630753186864
0.06670999331262799
0.008740653137327102
0.03225531682475777
0.053478216080924235
0.013773538420935184
0.01570298761766431
0.023269935485090612
-0.028334298313201597
0.044739873498016856
-0.0694854312054139
0.0021816499763505325
0.009108169736523532
0.02228871905879454
-0.02098721726010318
-0.030128074731725327
0.010666154025828533
0.010798142431016955
0.048111360982307526
0.004036715247227901
0.024555055676706485
0.08366082738411043
0.051778162555749886
0.056484538622677154
-0.026194540206582323
-0.017618345405022447
-0.02415157944154259
-0.018661113885609863
0.008519834201778579
-0.036762251459502
0.054058471987945236
0.006510141841169242
0.03045977373121089
0.03493083954387831
0.02371374416792972
0.04067615184449533
0.0008431551766499602
-0.006672578401678115
-0.0454692304097552
-0.026371682915819157
-0.025562520241793577
-0.012497064351237045
-0.13112919082071708
-0.01952429292622607
-0.06619599424957465
-0.037845671949862277
-0.030468382222396468
-0.04448377743757746
-0.05201316727774527
-0.024282750483945077
-0.029178175722823347
-0.04276730581120655
-0.05057678056501806
0.0010838414249254111
-0.016268071713282733
0.036743315037209186
-0.02841334942868367
-0.017483655761278288
-0.10073075392125477
-0.028531047482654535
-0.06833856879691942
