# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; LHUR
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=3, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.0016210303487407401
-0.054676706021273086
-0.07118401584358608
-0.0163370778730191
0.008840223532866762
-0.013159068839936075
-0.05541305538415089
0.005414777104743737
-0.0020461388178099607
-0.032455245109281064
-0.02238116513409567
-0.006036056460084741
0.005495188003006963
0.029464138182132428
-0.016391040018488415
-0.0314284673812788
-0.06556555190843269
-0.008833461700210073
-0.04048806093205615
-0.020334359930727897
-0.01769949760690026
-0.034382522865223746
-0.014400456721008467
0.019128930297181337
0.014946812018480343
0.015961576341925136
-0.01005033788077006
0.007538787650265518
0.050875068025687975
0.07563516020543272
0.06200525795970159
0.00611779733883643
-0.024840896264971515
0.01887804122600336
0.023170440092569466
0.04967317213239361
0.020900769632067315
-0.009708287992965244
-0.0033320408017548115
0.015601812898783502
-0.03731247038338857
-0.04000751286662032
-0.036200535139479656
-0.049818033442637744
-0.021842127029461665
-0.04732015122785688
-0.01696726434668015
0.01763531162263656
-0.00927712531671912
-0.005547278789448522
0.002219569461630218
-0.003484684572632123
-0.031696172958004046
-0.014188651253446715
-0.01919416370422739
-0.02468979362480949
-0.034678722425352734
-0.025913744734540296
-0.01419563629889806
-0.005351876518489258
0.02145206875032422
-0.006341728158765001
-0.020880700861201626
-0.0034165446613270553
-0.014181838253877081
-0.028311243289720334
-0.014204050235884697
-0.005244605025311728
0.019022248330380312
0.009152418658457492
0.0454169322235452
0.027486890115321256
0.04573434619198639
0.08266085215773833
0.011110915320162264
0.025619322619646744
-0.006018115296932353
0.04191663045703309
0.031758820931247246
0.0320725989443964
-0.02293494257024469
-0.05825493188631002
-0.024159064350801954
-0.0032874698896958373
-0.009280798735604421
0.004371338314212068
-0.022423219173154138
-0.004807718500946512
-0.024242622177299
-0.008503211718797477
-0.03947353553508206
0.004369098166846351
0.011584615467769278
-0.0014206117972870832
-0.00691090047805785
-0.00906446142500626
0.017579348154004984
0.021787977933969666
0.03716859092818563
0.05189611704837082
