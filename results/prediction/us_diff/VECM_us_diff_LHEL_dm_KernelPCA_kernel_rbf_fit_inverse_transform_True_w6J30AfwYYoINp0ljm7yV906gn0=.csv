# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; LHEL
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=14, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.03390145226236688
0.07600236805792893
0.04010078413199472
-0.09999935862426426
0.02332602444860469
0.0343139157861527
0.027418448089280477
0.02268347682775296
0.07139478492582385
0.029045460666093346
0.06549482497969458
0.007228988258622587
-0.02090395632283388
0.013415662925746849
-0.005084823473772518
0.02920877726270322
0.0033620136598363406
-0.00010633236735007809
-0.0030485501804709825
-0.049687769609865165
0.030031527523559927
0.05240497661229261
-0.07232616364694838
-0.05018744702476381
-0.006322241025939267
-0.007084850119860818
-0.027854987866445028
-0.07185351092939285
-0.06645022172533295
-0.0941513990958398
-0.030712748802120153
0.0007879008548795224
-0.05873463803265171
-0.015716553841694746
0.06743079282430019
0.012122791485340276
-0.04202387643845058
0.026664153343010106
-0.010323550137160794
-0.005747536810298014
0.060116577024423995
0.030408222578558165
0.08056885700868104
-0.021145629456698865
0.038483993451282464
-0.005716005380630231
0.013814999186058417
0.02961403283505211
0.03686066437267439
0.01690900435772404
-0.04163085234425297
-0.05247167506104062
0.009500612525015774
-0.01775465973762392
0.06742260325059407
0.019643003039847052
0.025235329534231403
-0.014526555434858946
0.03520577411702024
0.010916956298208364
-0.04406940687442332
-0.03892693192036559
0.029236014284822413
-0.0557603018729307
0.008855280005005682
0.02388302031799301
-0.028656891759099298
-0.03423761091293258
-0.0325334005195596
-0.029112124626234984
-0.0667076451349789
-0.04709935756017171
-0.11913580896149813
-0.07033686296537667
-0.025624096375532257
0.01804338814988713
-0.029420011618589592
-0.03976831652162735
-0.051510984807073855
-0.012208313221770229
0.03614885271262588
0.05994172012358105
-0.04042014843602515
-0.05106801152019909
-0.016547921041258216
-0.011732166676199581
-0.003855300798136181
0.01700630244459308
-0.008908990306329002
-0.012839845272193801
0.00800043251298138
-0.05503775626046805
-0.029625881087901325
0.01924102714241984
-0.016833383595541714
-0.011234958836989568
-0.0010655176509639114
-0.08016611581698854
-0.04497357534420146
-0.03927450056024452
