# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; LHU5
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=5, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.056558208284861564
0.027451521684673075
-0.012242516002931986
0.02940487483273095
0.013991771593028772
-0.0702778203230978
-0.010145639779638262
-0.011465261398254874
0.01317567032950287
0.0009170935784195578
-0.023495255674892482
-0.007637235164786394
-0.0007776616722828906
0.029411813831312687
0.007313224117292402
-0.013138455088706315
-0.03065247188521124
-0.01429648977060271
-0.04927001404076517
0.005765881545678784
-0.0063869288492937396
-0.042589666540605114
0.014447885213932895
0.005984340142220714
0.0049301850472011095
0.013666750842269985
-0.00590922129878057
0.009982817104561193
-0.0006729927360834944
0.026674510104962426
0.0394187165243657
0.004240865069997475
-0.03140455812325831
-0.020137856882169826
0.005265171032357888
0.04206417916620601
-0.012893234562708755
-0.012117958041089538
0.02567910108423697
-0.020325102936415162
-0.0005788055743124294
-0.0028000587636068577
-0.0752500498533134
-0.012085534544967006
-0.06582549723799264
-0.051879656238892694
-0.013478080026199261
-0.04959755332323062
0.01850049993838729
-0.004007250824723361
-0.004336410840286487
-0.010165246756131886
-0.021665657580903566
-0.02383676072738301
-0.005878433946787291
0.01694999766174107
-0.03786367538147322
-0.010944247369742088
0.004267522532740134
0.011822400730755087
0.030299276524741275
-0.006436634501970975
-0.014330059422098697
0.019492761969680885
-0.03343060488759882
0.017383298810574863
0.0035557113024060043
0.009683388915930631
0.013738495391386281
-0.012599711390969808
0.017149072602033813
-0.025054097713374426
0.021837250189596823
0.02290615646050061
0.006862969292754623
0.04861783030282921
-0.019747506824561847
0.019123209917392164
0.03514106576653365
-0.019977875468809338
-0.03650252204465956
-0.034646397713455956
-0.040084977626969366
0.030684145990894328
-0.0008730506468829224
0.018462822629766028
-0.0008120460525898069
0.013072015107703738
-0.030625595943726717
0.020729483237176567
-0.04170249237277347
0.04218719567324319
-0.005400392592096936
0.015375709100081313
-0.03329054864797261
-0.029192305167724038
-0.009125547119665038
0.011017125006458989
0.02548859957669726
0.02731510011164145
