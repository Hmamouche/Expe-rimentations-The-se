# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FSDJ
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=5, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.0007679965080053665
-0.0036112737526361217
-0.0035123970829394257
-0.006504066291931065
0.001080416332657231
0.0038256074764391806
0.004430078359756142
0.006739579803709962
0.00498954267613333
0.006338278868021558
0.010911071230414827
0.009520625485772203
0.003276091527337418
0.008534855424304037
0.01555082870136117
0.0009222378505864884
0.008966450753793698
0.015082193884438223
0.0011402534099984067
0.02664592534819648
-0.0019790431641708416
-0.02037075077991389
0.003543836789843516
0.006612055985981489
0.011918004736719437
0.011377860504239496
0.007265186650391775
0.00906445879506208
0.0023968807978638506
0.0018898755487366306
0.013075948766447076
-0.002756471788157457
-0.001170318216853236
0.0039963826982790255
0.0111691115345127
0.0046027881425708935
0.002528641922757606
0.008039501858120423
0.003955087714012137
0.009811839644122218
0.007241347452339177
0.007636827030583009
0.0011990325639713024
-0.008039836347174477
0.00668478185022644
0.0029781807677169442
0.0020369018074349234
0.0192916440833709
0.021464755716434428
0.020611175980360955
0.027248940838153636
0.017204732032456593
0.013228775229270628
0.02298248937862019
0.02555309936153631
0.02744549752253382
0.04458440669177983
0.0337794190394701
0.03992694315046219
0.02348489314440086
0.04800795557223664
0.03542048408984225
-0.012631191739459141
0.02494583359713957
0.008063756659735783
0.045388569634691206
0.04707072255631881
0.02972072593048749
0.014289053513239933
-0.005245042634588259
0.006291773955984688
0.0010211651934669197
-0.028358721978324147
-0.010750085976041625
-0.00727709522626005
-0.013786067782912135
-0.03246888933821624
0.012880180021524908
-0.04786998836445294
-0.024668416849236173
-0.04799610004446031
0.02549703345852977
0.02810985920012628
0.017308175100354727
0.021944336576635262
0.024534683151522973
0.022047345931822712
-0.004079032259629554
0.01077022081201886
0.001502139408867024
0.01682879780301471
0.004841328323052337
0.012526851592923579
0.034316926083474225
0.03166468385965942
0.03885326877377333
0.023357761880998154
0.030939126184081095
-0.01617095673103918
0.0034677455796792217
