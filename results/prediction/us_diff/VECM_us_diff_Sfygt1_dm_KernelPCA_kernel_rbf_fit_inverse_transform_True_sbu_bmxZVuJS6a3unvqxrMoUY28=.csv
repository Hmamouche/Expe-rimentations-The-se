# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; Sfygt1
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=1, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.19742868834650545
0.011280332905238832
-0.046163717739441376
0.1037189183417161
-0.08800277584469415
0.012225497921218327
0.07896989302896655
-0.03701693418772001
0.029095279434898355
0.022428806560330423
0.01594086376802962
0.010268103842733953
0.004219500422285591
-0.007560381560793277
-0.016375581031163124
0.03058746900959899
-0.06439282445348836
0.10116058836604622
0.039720002922883135
-0.0021118927504315266
0.01985737377725057
-0.016979248816576
0.041382627990065174
-0.005057086205700428
-0.029730132719819627
-0.04300394991343773
-0.04549614446009112
-0.03314278094120294
-0.013898785157071963
0.02217258697585832
-0.010192886844562717
-0.05712588745238964
-0.0464341546945991
0.023797299808172193
0.036062761411317965
-0.010654888897149727
0.002271578978585655
0.03001916303180953
-0.010623154296687008
-0.0008447775808016564
0.013565670834100634
-0.009673595190043344
0.010399338460366381
0.021547190453205512
0.00870181592223662
0.0681806449070627
0.0365301426401806
0.00677810056504002
0.033018223198940565
-0.05287436285376064
-0.0379930854133618
0.019100307500816528
-0.03627776419227678
-0.006214852519615307
0.02587646821819444
-0.01927698422025343
-0.03045752346380665
0.040717125965350175
0.022060456343872964
0.0015942341281418825
-0.0018104150485147054
0.005786162921389005
0.01528368660042884
-0.023046108709352503
-0.009626992123275021
-0.0012816827662501607
-0.0009404319898243764
-0.011050360491105
0.006243929087896956
0.03348115888320195
0.005779871868083815
0.01315979772992015
-0.018337152895816774
0.01523170586025651
-0.03269002448846495
-0.0541709947213494
0.005646084122352935
0.03639191938545805
-0.017830612344258784
-0.007191111563872211
0.030840754528077382
-0.0333729796767285
-0.00993402003933734
0.019285847480720365
-0.055087749087127304
-0.02000138017483515
0.01992021544519302
-0.024955360330777915
0.0014299930174460984
0.010206078780495864
-0.003978264149361319
0.03222574081922751
0.0012277705555658433
0.003370400316203297
0.025043396242962708
-0.003985952992988834
0.006727051941816268
0.004880623581707367
-0.003630024038436345
0.00479914544493882
