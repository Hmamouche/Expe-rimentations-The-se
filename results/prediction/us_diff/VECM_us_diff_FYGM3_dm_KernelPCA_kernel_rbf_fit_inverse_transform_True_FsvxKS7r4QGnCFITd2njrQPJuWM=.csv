# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FYGM3
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=2, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.01929544104465261
-0.047442008094804554
0.059689345918475856
-0.013898342602532572
-0.004527094359540636
-0.05367598892938446
0.03593877732953267
-0.04046051557942394
-0.00650436248516232
-0.07521509339551058
-0.040070934437680866
-0.03192771169967548
-0.026862513717692688
-0.030021699694494912
-0.004942209757527686
-0.03215635458761582
0.02616254774427732
-0.016847157386962176
0.01098716761691413
0.016967584592180902
-0.0087450573401065
0.030807025219513142
0.005786691707242537
0.006117919513558615
0.025441278407241585
0.016599370094033002
0.02528994761982387
-0.013013849150298405
-0.0457471635276941
-0.059064761171586565
-0.06317312535428153
0.008294432966908353
-0.004731544449393032
-0.04540351315347969
-0.016020411675887043
-0.04735984167246668
-0.04634291665941315
-0.0212118188521059
-0.04664626634170044
-0.01611469543239593
-0.008958833163724105
0.007898942650664122
0.03017451523951781
0.04787189536972308
0.0036799023483282197
0.06529548500896569
-0.0029217990077662504
0.009483388459910553
0.0190139809917107
-0.00013217766695233432
0.009664457280897328
0.02464997789010516
-0.012188651776062494
0.02550647373103973
-0.007224949878589302
0.012535948626240374
0.020267634840343093
3.9856929342159224e-05
-0.0060244028169920754
-0.016278757241019855
-0.026442496987926136
-0.016925948613702453
0.02183214944012769
-0.012656509457569503
0.022527689325177885
0.0036697189902249946
0.016390036097970538
-0.000919827570340015
-0.0022022456981236685
-0.020254217137592
-0.03162480540124696
-0.011222038392686984
-0.03944018027559185
-0.09242298845754256
0.00173345505232924
-0.08117689377115236
-0.002358931132620451
-0.07781777142235866
-0.022764841002082037
-0.016136768441548103
0.019230127628852754
0.032270157541130724
0.0257522108388007
0.015717497619246776
-0.010711513408855431
-0.010926809762697925
0.015476573584300133
0.007332384734237067
0.052310537666712474
0.04024808677235865
0.052850988935452994
0.020022236506160578
0.015722192396255125
0.0036800979303399668
0.018819193560702236
0.00927982876684547
-0.012109599462968211
-0.02825201901583784
-0.06259408184119532
-0.04014406471610419
