# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; GDP276_8
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=8, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.006817373250356904
0.0034816083724696504
0.0029924930654226132
0.005624826538926101
0.005392747616657678
0.003031190684680826
0.009279547298665591
0.005495672841533478
0.00045203410793059103
0.009390190481871792
0.009158828109000991
0.006747724278681719
0.013319669059737506
0.00931794493902462
0.0028099661175035625
0.002051742402069972
0.005377154944069591
0.0015738365820028
0.009050797822367167
0.007423351307121151
0.0074070265889422
0.006886939439372847
0.007422679308291494
0.0033318895536566546
0.0022966681679287646
0.00333375205590579
0.004530993998095322
0.01283812954770739
0.0056135535142998435
0.007660708778536744
0.009242665875813972
0.0017211493943011412
0.010611857516081173
0.004955741316640133
0.015128022013878212
0.007633426144047698
0.007788960821069034
0.005902445840093076
0.0031998251440317825
0.005482968749775363
0.00426548985846853
0.002718640270959199
0.0021362320162734284
0.00464685881479811
0.006894828312750773
0.0069569172942605855
0.0022062682311463216
0.006940424337520292
0.008398641447407108
0.005885077494066601
0.004827255906756346
0.007574075728936273
0.005084390399371897
0.005696916351007131
0.008872626070610217
0.0062357788474665336
0.006078973952499215
0.006904785343905257
0.0035833135324944082
0.0025985032704364443
0.0031978814548563783
0.005624117570133833
0.0025814849552315963
0.006517877987715896
0.003357532003478326
0.005861262086583807
0.0076589848350410855
0.0038646506678447637
0.001097525818326425
0.003171148133463378
0.005847393175650091
0.001800688584991006
0.0021438091158994814
0.012411283624839748
0.006868187818981963
0.010513493927357405
0.005460071917674236
0.007578165989685904
0.00584597700848167
0.003238892754881253
0.009744047381517577
0.008857886953070992
0.009228547392662226
0.008457768780044174
0.006478549320674239
0.008393073895059102
0.007762308274584968
0.009988462577090513
0.008467777931814493
0.010249860059345178
0.006716701324395403
0.01248381095063674
0.0073675777921964536
0.00713657194840839
0.0079690082004727
0.013260146783602506
0.009472416472621589
0.010677286497422481
0.008827930870564538
0.012436567563666468
