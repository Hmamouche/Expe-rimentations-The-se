# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; HSMW
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=15, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.3313265438242808
-0.15012885459944691
-0.477348455490822
-0.012505729294101792
-0.250588113559712
0.33006160097957776
0.007262536539838202
-0.1165862640305148
-0.06320676178979842
0.05268717602572552
0.055790150724740514
-0.0015920927653252648
-0.06052719549654505
0.21693944544732824
-0.3242106714258019
0.17577873760303203
0.3012562275345449
-0.15844524363918983
0.04082369757717343
-0.15584458746560548
0.10983243944088261
0.01669108370725251
-0.021790978996987852
0.048580501722476625
-0.17004310385120674
0.0509099215379573
0.2158926423989369
-0.11180489828769843
-0.0847290326861539
0.03903824544171097
-0.23426098942439294
0.04988206885684323
-0.030842063644970212
0.083011213479608
0.05866417010708709
0.016754245415702085
-0.006666210268099798
0.039249072576737674
0.06882413425324169
-0.07515088041529092
0.019603972276080767
-0.08885419149680546
0.1262433975737753
-0.11568372155784393
0.16425722946296956
-0.14435391546580406
0.16528962212838727
-0.10181024580909376
-0.00020961314137962067
0.062415649869996453
0.06911246160613725
-0.127931436272113
-0.05766531953618974
-0.058525894301366704
0.03845953844384868
-0.025762689283589726
0.15382179218070055
-0.015427628998870404
-0.016104269640796684
-0.0009299683778150256
0.021192910291101826
-0.005618040527254668
0.056334154799213276
-0.11857286220329769
-0.1218897395256372
-0.014922493161487988
0.09803777174382351
-0.06513657433954476
-0.04709558729114255
0.07624584536986867
0.0006709800971354922
-0.049839266702682954
-0.17164148339854945
-0.044433568107947435
0.11021501085819121
0.0991275584663646
-0.17057028585334746
0.026449532593593436
-0.05711006011405619
-0.01636707682766521
0.060575805479702316
0.04500610074837906
0.14943098800410332
-0.02254872730806482
-0.005904610932991904
-0.09398910952705289
0.004639719682293202
0.06364714681441305
-0.09133645302000071
0.019897496998857976
-0.046380012237676446
-0.09228144881221294
-0.053893859763051694
-0.0030325915886115565
-0.04103184838714763
-0.05719694122245696
-0.01036566173660844
-0.09490557629476665
0.14845368787474503
-0.08379172055678721
