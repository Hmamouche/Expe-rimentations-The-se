# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; LHUR
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=2, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.08548773342045471
-0.07981305685132066
-0.04721611464905786
-0.029343532232183243
0.016141741747508737
-0.009573198844983718
-0.02894021607455659
0.00018254976264029874
-0.017842057209395044
-0.025628177690325958
-0.013889870957274397
-0.0021014686146391534
-0.012001829151410662
0.007136075617071656
-0.02469935270100812
-0.04595860224370903
-0.06002712618048257
-0.020361207915557625
-0.00709013210499808
-0.02177798849834669
-0.001037974477244818
-0.03559434415408686
-0.01376142071231291
0.019339647364939067
0.013933554349467744
0.015354418591930385
-0.022779369783503138
-0.01450705504617357
0.041737958103695155
0.07337480058909006
0.07200939082232254
0.009401346874653216
-0.014764783972998904
0.026976634341506853
0.02501125713425723
0.04102900333158979
0.027459858976271803
-0.014560866714791107
-6.180512919467564e-05
0.010086533926596801
-0.03529351337166919
-0.032223823471106285
-0.040638260888275116
-0.0626583793378186
-0.02232921203644947
-0.04385782117832133
-0.0009923581088024827
0.03531612918846077
0.0012936477608661463
-0.009255246560911277
-0.014400733253654223
-0.026175297591464284
-0.03147072915886436
-0.012370545911830379
-0.00879984932507757
-0.023202241297461658
-0.026815743937206676
-0.01641925433095873
-0.003728791216058928
-0.017734140875953545
0.020282130146923188
-0.00770884477097851
-0.025189309404553682
-0.02167014519239538
-0.01893851186225859
-0.027044814460186726
-0.004942788562900799
0.0002790384828239632
0.026803120771648616
0.01429785018942686
0.04264243885737926
0.011802934536358825
0.03738481677823221
0.07178179019394033
0.004918393739039537
0.023337864288828388
0.009029851148077358
0.052241412002279226
0.029125354703086827
0.031668537850822145
-0.029201306859501383
-0.05208696811788005
-0.03108667036763102
-0.009577883822052893
-0.004418110405251105
0.009862140721684261
-0.014586773003777784
-0.005586983056499797
-0.027741754043509675
-0.01609098956868546
-0.03118360706855649
-0.012754093940790776
0.00990846249259349
-0.0005387153866022801
3.975136744627673e-05
-0.01104814711141353
0.012438042365039065
0.01361738144656106
0.04115029778385408
0.0467233752516715
