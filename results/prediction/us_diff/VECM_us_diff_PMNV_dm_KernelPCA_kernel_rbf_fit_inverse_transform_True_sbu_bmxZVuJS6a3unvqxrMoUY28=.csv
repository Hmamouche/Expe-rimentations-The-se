# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; PMNV
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=1, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.17385491454808433
0.09743103680235664
0.04468374552306968
0.04060508076965628
-0.09898447261621471
-0.0050734842465704165
-0.055228667207032185
0.0007646405461346957
0.030379138458448703
0.017586316495804975
0.010360281301506057
-0.021749273406806154
-0.023512275997811843
0.0004151690370940656
0.04841507284003082
0.018811325510781175
0.09386858492009613
-0.007787791180141942
-0.02005393470826665
0.0460144836741563
-0.05860278300969328
0.04186811114989992
-0.04017266752484565
-0.054140613234769146
-0.059233756785909385
-0.03269856178470364
0.0070142308971831895
0.0035119299606544106
-0.028494493376646224
-0.10155213282224466
-0.15094077070671072
-0.018412365020199155
0.028093899429287597
-0.0029819850332256734
0.08839766327705931
-0.008678194619454752
-0.03332485898529461
0.012697883942193332
0.028864555576779187
-0.02218939828269391
0.0331059774195719
0.04240162912749764
0.07034572879468796
0.127800991240994
0.08753846868354306
0.10736912813569215
-0.0402768138871618
-0.06885481755106063
-0.0532851374588228
-0.09823954610387678
-0.033002497772160445
0.05794807821975563
0.03058201067647182
0.08335882654526519
0.06435063758022545
0.038505123340056936
0.03162427118484907
0.012036985685170856
-0.0294641944414361
-0.025521626768654316
-0.04712365632773567
-0.05236698344040993
0.03705287439840485
0.01718145744877878
0.05575104761918622
0.06296736192331695
0.007534903866201327
-0.03670762946569127
-0.07070188955138738
-0.06593634132966113
-0.12570032548174787
-0.05834987866647466
-0.038412066116577555
-0.09138970724132732
0.1157936560099089
-0.005393501208345655
-0.022859291299774553
-0.031988044427200525
-0.056324582790272734
-0.021843947930717707
0.04785395657387375
0.101032462814805
0.07007423192245982
0.060103457814876464
-0.013037037014976775
-0.01877263772623366
-0.03009473936762895
-0.05805587482452761
0.052992483030964536
0.03962656289438937
0.06051775955529993
-0.01610226641078477
-0.0403596649422079
-0.07702963946409296
-0.03271680780115087
0.01043947342263533
-0.04282779985260953
-0.02868234032572543
-0.1154745529410354
-0.10216259207641448
