# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; HSBR
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=12, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.06250650051497628
0.011688961168374978
-0.012947306997611305
-0.04712927380481763
0.030440592579507562
0.05275672982711363
0.026265775712734026
0.06382865985169008
0.005228820670512224
0.0006193038135543418
0.11013341491411706
0.09826545133852341
-0.036143166614773334
-0.06579962429517221
-0.12127115209641147
-0.06629957750342265
0.018292849261854593
-0.026260654439121997
-0.02321912919463183
-0.07747792155834832
0.053855124041558455
0.025084915863516064
0.008461729361696382
-0.006952079393615772
0.039700991995617274
0.03581777487250332
-0.0015939004263347024
-0.0419993311309522
-0.14007041516166918
-0.07782858123793882
0.00023311284768319854
0.08190059172390993
0.006306874119813656
-0.007631732381049966
-0.005296515430523958
0.020946761545077863
0.015426446888151216
-0.020238363917055524
-0.05799128654526528
-0.11680408110579182
-0.04045237024890328
0.059742574578211055
0.06756132925530332
-0.10841406126775839
0.04361750155088747
-0.07624639833093096
-0.00468923583631823
0.03398177969793826
0.05014162987696143
0.06051187338544807
0.032361159735141296
-0.017611424502327753
0.03162276881368357
0.007998070807932375
0.03388132489055338
-0.04036337258600903
0.03178351852905935
-0.004135983875492535
0.036782206650529684
-0.01769566595093193
0.0614775623881539
0.028220235923608287
0.03181312112234177
-0.012004383104099228
0.016829419375648854
-0.05213675607254216
-0.031161684502056218
0.03821439311166244
-0.02744858194648027
0.042878419449654176
0.007038029802468357
0.039771490950541895
0.0023398744014433088
0.059101372271262095
0.023944502573534674
0.00876514292103021
-0.008507533003526989
0.004249397844546116
-0.08212486465553073
0.02789462078397431
0.06658193123291328
0.0659897508218432
0.04834779185552872
-0.02991822273929437
0.015449873800493043
-0.010619381452009403
-0.0008926546301106252
0.05170915942102442
-0.01665352428455845
0.02790820212959896
-0.007767089405571557
-0.01862354884478733
-0.0471871407288602
-0.047873887861868
-0.07193528141927405
-0.038112080185093565
-0.03638989862102997
-0.15651461928914298
0.041080124656844824
-0.16488257998050287
