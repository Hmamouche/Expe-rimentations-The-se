# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FSDJ
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=12, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.0017395030077862518
-0.002880127774511124
0.0005308642525660922
-0.008583913523762872
0.005219153983186967
0.0006729156707603722
0.002357787628733671
-0.0031890098829446415
0.007966291087129512
0.008466758120985551
0.013174529154607557
0.011396709382982043
0.0010651496770166732
0.011256271342390515
0.01333556343918708
-0.002214368726518513
0.003916881496065107
0.007077194413082414
0.001293034939296795
0.04999269414119074
0.007837087219078314
-0.02814702804416116
0.0003217930736212516
0.0007201381161289994
0.008311213613399274
0.015426131585896986
0.006569046010009743
0.003780834290030474
0.002089311137587538
0.00289919176309684
0.012513637138200915
-0.006666198386006775
0.0031303704176017036
0.007642355854578156
0.016841150437338772
0.0026677845805108077
0.0025272314563784615
0.00954991434335965
0.0010228616634918048
0.005573272105007542
0.011957079187906814
0.002369014639941547
0.003665888635033419
-0.0062545412572700115
0.015638182692843356
0.005412793381919131
-0.006641881937279508
0.010370022670375633
0.03289036729265881
0.021754182938007845
0.029135156300887692
0.010666727793262838
0.013902537531140666
0.021365239816839805
0.027726082453655432
0.019609257764781247
0.04377659447749178
0.03902698574833535
0.04860931450535204
0.0234843502338907
0.05284212410924307
0.03241344642373672
-0.015397766252673258
0.017022939029034293
-0.0044815976233389485
0.04764154668030883
0.0402135910320532
0.04528127534056977
0.013301438056621203
-0.0063706656695355045
0.009729402805331015
0.0023274440534246394
-0.01417370919392794
-0.011795894473339216
0.0017858845139246985
-0.02198695997897461
-0.03448579169291806
0.011100812698614529
-0.06450588665872334
-0.01828758674339519
-0.03714261343482022
0.0017976462467440314
0.02140801361569501
0.012615305103532136
0.016849820104060026
0.027612109258725013
0.04472554223855223
-0.002293361642657704
-0.0010048998016980778
0.006395262248519242
0.022399938814597134
0.0009592292983419803
0.019063061635913675
0.041976690966586044
0.012372736869948777
0.03733081057520759
0.03597251495607204
0.02730940963629654
-0.009920563536846031
-0.0013032349072327216
