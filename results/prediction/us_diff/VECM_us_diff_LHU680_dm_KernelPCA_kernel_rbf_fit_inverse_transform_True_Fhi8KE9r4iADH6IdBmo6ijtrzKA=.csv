# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; LHU680
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=12, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.027784018055498422
-0.10402446991819474
-0.12438839415849326
-0.027481906410926182
-0.031191748417479074
-0.012129776109795588
-0.05096927871991559
0.006797087345673143
0.08415817528553181
-0.021086795117244084
-0.04662754697876001
-0.01537006865704331
0.043666326400970484
-0.06104890563670278
0.00693788561177516
-0.00568130560372623
-0.0650837968871307
-0.06734377763007661
0.009548240456974508
-0.03908088443171059
-0.0067181460822698805
-0.018276364568884644
-0.0034725425193821227
-0.06433207871353745
0.018662331268121665
-0.0028390140810711966
-0.008210536660811712
0.006540855329282939
0.06143713481833578
0.041816571709873296
0.07091810173874641
0.08394762056701481
0.002336986002463383
0.010508435060274799
0.07172683418015649
0.03596815974066551
0.039005307966728825
0.056389597894857285
-0.0354311544991813
-0.0009448613703282698
-0.02445594513100766
0.019621260554806855
0.01761254536551325
-0.004826218036958525
0.021233407757544134
-0.020524522773705405
-0.09901445755404031
0.034939919649133025
-0.04375226263786725
0.024781559796119016
0.01056125892433153
0.03807494346872431
-0.031623860972555755
-0.019081468397966893
-0.033017240587279756
-0.04618477377534721
-0.0027754617988932956
-0.039977724308944335
-0.006495967457654706
-0.03697136652905758
0.02951826538571064
-0.0066847786771221055
-0.022712583863391665
-0.008773078360221596
-0.07019464139306324
-0.012544139102802877
-0.047890004618732225
-0.03942340676908196
0.013835463649711138
-0.015891572993605052
0.08311227071683974
0.059702371049467025
0.11208741143391233
0.09350828239253264
0.024867785269102095
0.0390444392833744
-0.045960510844502395
0.053371509627522566
0.009502523930037855
0.08121070673181839
0.062305259879279366
0.02291568392417713
0.03881200815492846
-0.023054649293559784
-0.05089082464337741
0.008921202586552861
0.0007176076736870492
-0.026847347122096937
-0.025306697284144794
-0.01587743125234694
-0.046603284165405656
-0.021410788256900175
-0.009440535262875565
-0.03321736961010698
-0.01904945804253532
0.016775542275990717
0.05091493399740011
0.021866277349393802
0.03389862918776798
0.01199736063085674
