# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; LHU14
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=4, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.024419505187521603
0.0034331132544838314
-0.02894340911730948
-0.018161857836491165
0.06620258634031272
-0.028710297676066265
-0.06347526304244717
-0.008806413781151837
0.020705639761084617
-0.029856756804215234
-0.023216733279404014
0.005057205425393731
0.02752200743374601
0.04148621448200088
-0.01892640330739921
-0.00660811316062157
-0.063970710128192
-0.024372215147436246
-0.024690283680296354
-0.05550836811946386
-0.01276442621134012
-0.029145869082688153
-0.004593192378233482
0.019354294928527818
0.022602873984924724
-0.00573871420943187
0.010370957179244012
0.0335896934403848
0.04073503850575645
0.067983448872553
0.06516209646710087
0.01583749611536492
-0.04142264601129421
0.020912826281824587
0.018131651446541854
0.028065239211734087
0.03960742311979666
-0.01976071814791497
-0.01026914417066501
0.028258952432282602
-0.0316579681367946
-0.05446791551064628
-0.023550643834331742
-0.02713515520564599
-0.024025351201726303
-0.02951157552760412
0.017554874031507677
-0.0037175109543296572
-0.010737967033710608
0.004270263059834571
0.008503681558125413
-0.006082741118522075
-3.2333323740706826e-05
-0.03783003160396666
-0.020386279837736665
-0.01646613256260066
-0.04037158796734467
-0.011613472567410868
-0.023926526507174906
0.0038239984708869842
0.01866040473806662
-0.017574488165735463
-0.021750415156025625
-0.007522736521086106
-0.02240823601593975
-0.05059495233558453
0.014259115409065987
0.006251168617369664
0.011338411342260546
0.015906181181147293
0.048272337147705895
0.0203729159863205
0.036279786074580815
0.07714791264282739
-0.0238901418500206
0.039816041643191434
0.04927997303204264
0.02795584713668322
0.043766225214685234
0.036059263611648566
-0.05356904107741381
-0.05400971585377379
-0.027594940872889103
0.014052429646506034
-0.0005912756834301602
-0.0076961708099241785
-0.007063256373081703
-0.00031987118048206614
-0.009538678774687444
-0.014990446913032293
-0.03131888090519563
0.004323752478930153
0.0238196177041905
-0.008108901014868827
-0.024958202562406044
-0.012844841451429065
0.02617802515372103
0.02857699836563313
0.05154447635782196
0.049760796393346934
