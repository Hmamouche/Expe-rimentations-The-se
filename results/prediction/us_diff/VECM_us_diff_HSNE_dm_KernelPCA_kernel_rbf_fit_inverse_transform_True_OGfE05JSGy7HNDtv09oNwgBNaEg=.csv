# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; HSNE
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=7, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.18494856360876172
0.04096222553682356
-0.2012207649748944
-0.07641150659807111
0.08542244906875085
-0.035030765106875444
0.12826062519617717
-0.010964878952492069
0.06530248901842033
-0.03483076383486915
0.11221598158516409
0.10742805750056009
0.027370077811044804
0.0381623882307093
0.0314862058569719
0.024078735471924934
-0.01726358212371673
-0.03805987669276358
-0.06485277881610763
-0.022938144646133682
0.10713781999719135
0.0326001008285998
-0.09620027993033775
-0.0551423454006075
-0.022020082597719294
-0.052341288520857954
-0.07792148161901588
-0.04498434969432521
-0.11697848213152744
-0.1252630035579104
0.13325053541775794
0.05599180160541668
0.21951440015832147
-0.13591974492783165
-0.10327395344033423
-0.00545534759999855
-0.01663330460127199
0.07890849574253381
0.06635700195534697
-0.016499684919737585
0.01752369345312938
0.016917881706410454
-0.060757103372165285
-0.014652525451451948
-0.04944361223696768
-0.04488589934939598
0.010067643541337094
-0.0009993014434731239
0.07573858229701223
-0.0035599022362490726
-0.06436267107280257
0.034254490937313045
0.0665327863356072
-0.0813741867855308
0.023852104207006695
0.029060638782486343
-0.01933142001189368
-0.029107411944207774
-0.03091339224947668
0.03550345071082082
0.032971343069825014
0.030853415205849343
-0.05498588497492378
-0.005471020003960589
0.04617704597794828
0.022961277860452118
-0.0689862704987801
0.011083948413000014
-0.004601984360092827
-0.01946727977198027
0.04727821726868797
0.0323855616923424
0.0029366348593051067
0.018806366453020543
0.04959476898035287
0.050370078220130884
-0.05118748419864933
-0.002737802261896821
-0.06208200019814679
0.0199272818837325
0.04228463603311787
0.0756082502898846
-0.06528229999330541
-0.07194507103362711
-0.010956124272876129
-0.006492893817102302
0.08037165828714485
0.039285883830993205
0.0137630293879301
0.002453404164380576
-0.021976970529097875
-0.07513152219186202
0.02556631264843253
0.00963681475969503
-0.04528656304043113
0.031728691984091575
-0.029418176746344622
-0.10248338228923543
-0.041943098171032346
-0.05487889598150027
