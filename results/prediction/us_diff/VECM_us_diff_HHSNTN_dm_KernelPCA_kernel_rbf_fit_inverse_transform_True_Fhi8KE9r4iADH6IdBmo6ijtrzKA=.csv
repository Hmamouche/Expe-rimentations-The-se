# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; HHSNTN
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=12, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.19073400563441423
-0.35635912833844985
0.04465053564384569
-0.11396835464055266
-0.061995451898183875
0.09746540757469296
-0.06430272150513269
-0.026055202838617668
0.2768614600452757
0.058209723045924956
-0.052415046689757744
-0.08962099033929746
0.005015680321361095
0.031540421770959615
0.0048189821304156465
-0.05623392351320827
-0.11692675656041443
-0.03989973285931754
-0.035985044686669444
0.11979022869476902
0.026152134708401352
0.012145607717690379
0.007906675328699618
0.019045651828649597
-0.013871597657704028
0.034493630421512815
0.05621781625473687
-0.043035940029506656
-0.08111964054699439
-0.009893327634960977
-0.11855827573952828
-0.20683617698231524
0.020141449967461297
0.07199731328192466
0.13011234170041455
-0.1124681900552143
0.04617085136593022
0.10932963819331137
-0.08241600098394183
-0.010925717141462989
0.04712102385192001
-0.06452966464106481
0.03304867035327809
-0.029736387514208096
0.07060269721333437
0.06142552962449367
-0.02905898584254081
0.02027018547295216
0.1219351270203147
0.010422739498470411
-0.07248647909295476
-0.09014155953197489
0.07577262500116036
0.05936793405351069
0.04220424249240569
-0.023659861384367636
0.052404952013428166
0.05651880962175625
0.0725078299586251
-0.034312823743904604
0.003859532444279258
0.06428917407095014
-0.000953630625718252
-0.15283806540138575
0.036748910940009094
0.13691646658430523
0.004664270220454486
0.011508018205892395
-0.037253518389268124
0.0250382958943451
0.0009359263886924976
-0.004823688069232862
-0.10660883406429972
-0.01048875338134282
-0.03379325283484508
0.06423101762366241
0.09915983054008477
0.06478950186571539
-0.2323517989358395
-0.08199835619920624
0.01055183443857629
-0.06078933445419078
-0.0023752651655876353
0.03841965676289568
-0.029923162565028492
-0.015749984047866823
-0.028681577783183934
-0.09636813793920115
-0.020698494055862063
-0.043226133038038246
-0.014064310721597311
-0.0036305046005371336
0.021471146189805942
0.021112419619926125
-0.057809319160974454
0.09492651182638114
0.08394292356115014
-0.014555781854885673
-0.004760518587290954
-0.07075796642530285
