# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FYGT10
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=8, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.09520035156853945
0.15030760460696826
0.08712122502969069
-0.04150835187456284
-0.02587455485527739
-0.06157100397101721
-0.09103815814954921
-0.06492986220679657
-0.00813193623313023
-0.03686312814919464
-0.05729844872365471
-0.025524266270567898
-0.07017287863361414
0.003361188003689279
-0.040285776219043806
-0.007699304559671679
0.0819246804187354
0.030511495659746465
0.04166118171969708
-0.06370522563901235
0.0015018787572928863
0.08425957834722955
-0.008258220867048152
-0.03481478854192128
-0.044767223391214014
-0.007079816960012358
0.008805369305681775
0.0021242310813905305
-0.023581729198398904
-0.006094305658266666
-0.03924630083282077
-0.06100890324082197
0.05088867306650709
0.0356258792352389
-0.01596856570124011
-0.01603252841101087
-0.06906581730749635
0.010198779841761357
-0.015338710906230492
-0.04640344893673682
0.00773787937635914
-0.014832670257699788
0.028203258684338012
0.04464464006564084
0.041227066262695365
0.07823087646575863
-0.041516052917809965
-0.037015398817182285
-0.02630677693734768
-0.03946697758487995
0.003880553826614682
0.050937174228700324
0.046430084763212066
-0.006049355749552336
0.003787565039125691
-0.0043801150667131875
0.019257907124291228
-0.005847010869594624
-0.024735885478857727
-0.02146422385291911
-0.010737255212182445
-0.05201307865183692
0.049366349115248925
-0.019864167208184298
0.0122659412762021
0.053564524324904064
-0.00358730519562395
-0.04696144241528337
-0.03149372403549283
-0.005596182762560166
-0.03865668442553963
0.012746676119585021
-0.030908222485377564
-0.06932998884902909
0.024001182999733264
0.00010468831434130334
-0.03452937910705583
-0.005066988036719951
-0.05154259150670797
-0.004589636730714507
0.046329395693072144
0.037486014408633285
0.04447385262094908
-0.02033295955850393
-0.05054091828516777
-0.004830848504971504
-0.020682006365222903
-0.0047860503863935595
0.01653244276753104
0.030098401302510666
0.04663785836621754
0.003221702381638235
0.0032032092746687967
0.0056426445009379435
-0.00615317674982138
-0.00037141243711504013
0.022973545761558727
-0.06854956477771369
-0.06923315127689621
-0.029832856112566572
