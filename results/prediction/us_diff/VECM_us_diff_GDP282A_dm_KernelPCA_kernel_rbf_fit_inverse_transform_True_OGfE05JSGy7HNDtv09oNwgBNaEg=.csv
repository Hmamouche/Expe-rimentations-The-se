# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; GDP282A
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=7, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.0022197218429911925
0.0034373517280663676
0.00398736491392182
0.003477031022542973
0.005887461253739892
0.004368334776444904
0.0024053008446400714
0.003469293877123786
0.003151266459062947
0.002776965427203771
0.004964715233030263
0.0033426853715421106
0.0061483886455108595
0.00794456681853359
0.005558901801642726
0.005142755655217446
0.005490973508010755
0.005583607638612268
0.005098356565142202
0.005226615767058832
0.004456443029408736
0.0055370833874914425
0.0050009023034434255
0.005195254844239769
0.004377272701558883
0.003329521241134919
0.004888080326148192
0.00432343075673902
0.0034075104484400263
0.003030362740087059
0.0004089369564256079
0.002336622828526388
0.0022131167350783
0.0011897327831471643
0.0008032086141044008
0.0035351530703294307
0.002677448130875965
0.002309456197689231
0.005708322294080783
0.006249718942236952
0.006332620595347333
0.0067187113519498985
0.00583345273527958
0.00519352959629849
0.006328142899478568
0.0068126111908554205
0.009264065727967235
0.007054932870742984
0.0034547493786216497
0.004400312847153103
0.0037328826323546763
0.0028757620324936822
0.00541168867596592
0.004948260499952781
0.0034895350510577043
0.004646605568412808
0.005774948547955568
0.005139577322866975
0.004166610254576392
0.004572879005359277
0.005916343782723325
0.005769880133161672
0.006775397192710169
0.007507033209554526
0.00816845067415264
0.0067781134175048125
0.010231180569634069
0.009640287721536649
0.006883517608902923
0.007289303713096479
0.008259870394766614
0.01103102836266112
0.011088083086291103
0.007742047772865383
0.005771453281808605
0.006292546894518881
0.0038907899346889317
0.008258373797862345
0.013683545918149086
0.006579305133987648
0.003310155302146966
0.013179236903569072
0.015606449794798596
0.012818946137929953
0.01752623328126222
0.017964346964525396
0.0161874784458895
0.018611261257491594
0.022465414968265197
0.016818883025591463
0.017987132103534195
0.01588360349977439
0.013865392341401864
0.015730255998108532
0.008149057179712343
0.00255865892459737
0.006498095234761264
0.0008890430443151242
-0.0038852440066130334
-0.007338476156097217
