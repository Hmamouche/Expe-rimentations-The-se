# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FYGT10
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=1, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.025143907542239177
0.013783381051732577
-0.001841692208971412
0.06372699457366239
-0.02434608262413095
-0.007883594592507391
0.016503521066079815
-0.0633278193493307
-0.02728686548182442
-0.023059187619010484
-0.06855216176305882
-0.062286209463051516
-0.0680978483463353
-0.06018974851282708
-0.02407488026652228
0.04515633489305412
0.03630175599167242
0.03236814883411634
0.0037803745227346216
0.014638885112492186
-0.009823994249726412
-0.002256902005344584
0.02496468692333261
-0.01761912205927483
-0.029396356308820423
-0.015729806002773485
0.0025620067579149226
0.010157649689216824
0.019599118013996983
0.00128323390631167
-0.015235742636640038
0.006860048066523756
-0.011832296955749121
-0.017730452698698395
-0.0013353684927543278
-0.033422118853851525
-0.0374818485493657
0.005317804753047344
-0.03787211434716238
-0.02790336459819711
-0.0136123523873297
-0.027306493023530765
0.0084857994307894
0.04082275047750272
0.019161013438376458
0.05382609409157277
-0.0010025772674262749
-0.032815097388976734
-0.018714816988263803
-0.046225036443691164
0.003333274366460518
0.03466302372185379
0.001922823281989286
0.008345748920589148
0.004481628054713753
-0.013139915158240074
-0.0254372757804785
-0.010269283783116003
-0.025727210582272828
-0.01762708018657925
-0.022720488181300957
-0.021333665130024458
0.012944225799288568
0.009560543048732007
0.02089382082403053
0.02322922339620426
0.017334795919433824
-0.010181382499458809
-0.011433190533589831
-0.017262882339686907
-0.022292844497234218
0.018505848523227966
-0.010863928448302067
0.0096451022969367
0.02838844167127587
-0.01955765447956621
-0.009577579698986169
-0.01912578315006397
-0.03590810025103265
-0.02183174906808625
0.03208528843764388
-0.003725007365575176
-0.012241936900570193
0.02593966501537793
-0.03895972545326396
-0.008139742983029032
0.007850231275091425
-0.01352094135459675
0.011066718238711342
0.005074692592092521
0.007727077834818398
0.022847349974077988
-0.00324725888460222
-0.003295783856701392
0.010816155208985562
0.0020431410980524976
0.0005800211165715072
-0.002241116089486344
-0.030774814577670263
0.00515219708646445
