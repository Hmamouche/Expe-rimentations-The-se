# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; CES140
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=3, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.0075019362702235855
-0.0005027307879292412
0.0007314045183007707
0.0031362621188151234
0.010503032534140321
0.005896518391607843
0.0009909196081662281
0.006011372158387663
0.010155614215046292
0.005451542922695671
0.004165015377658103
0.001531499099787835
0.0060527231212941165
0.009098044648709605
0.0051522988446209145
0.004811519818501262
0.0037604856032142963
0.007358684614087981
0.007043965759707933
0.00799776130521356
0.004464963712613002
0.008196264954596064
0.00693157433705726
0.008222108819877451
0.005466549075806086
0.005262827565765989
0.008196763079043247
0.0145246177140493
0.008224787848576824
0.004477953325772337
0.0012660819879186565
0.004741113940518499
0.000667703705077668
0.0015785137934228555
0.005274929019303891
0.0036198993865148212
0.0064849351864953945
0.0020294772878692007
0.0038191292287538987
0.0025643871143271064
0.004359397557482655
0.002883045378953455
0.005099016337629483
0.006826403625769213
0.007023768043265465
0.0033519294786010165
0.004200258165304727
0.003318595439285597
0.0016172153168217437
0.0023481907702480823
0.0021437873469829557
0.0018394321566485307
0.0003000769825385937
0.000971656574058374
0.0018524762970646962
0.0029163280207176734
0.002728351715998574
0.0035134666984793727
0.003432213869190682
0.004938563536647787
0.005851683772150027
0.005011864811109582
0.005191334432186174
0.007429128476045169
0.007765036265225121
0.00737683891240219
0.0089170133137055
0.015064400679080015
0.005669977502263944
0.0049479739680310586
0.001695327804677571
0.007894264623620155
0.0059120730238047875
0.0056643043711187575
0.009523964152474713
0.00712498623325439
0.003685251894715803
0.004040580166857396
0.004392140141059636
0.002053901489322229
0.0004872531521994111
-0.0005033215091974169
4.444284631009742e-05
0.0016012969531332489
0.0005849207586113448
0.0015410206361940546
0.0031374090005325873
0.002565077725239804
0.004029139424438843
0.001465985282357639
0.0028496626479100826
0.001109337008824411
0.0055627909282163724
0.0021680505542865346
0.002629538444039104
0.006098159005494077
0.0024197851618513078
0.005550986264622034
0.004978938114430541
0.005005738787837888
