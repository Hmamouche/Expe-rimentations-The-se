# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; sFYBAAC
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=11, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.06929070839350854
-0.099737517952237
-0.13074562619108776
0.17736008304608664
0.019448973367062826
-0.019968391989581794
0.0019855166359530885
0.0019362087693474087
-0.007397207301510389
-0.03062385461502634
0.015231821061347284
0.019460424947869794
0.05716342297073718
0.08236371689221499
0.024143387772854265
-0.020826253024837274
0.015714170746860802
0.07161035502402473
-0.0986716616025207
0.019499580103265395
-0.015326869353289724
-0.15822491959535784
-0.021268404661498236
0.08978651501607893
-0.08216793320240875
0.004349543231584218
0.005492647087041752
0.006116884418123268
0.09637736624464213
-0.019772766074826306
0.10013246257122724
0.02749177175153904
-0.11838365641017244
-0.10820732950909627
-0.009157596752047597
0.007673166511534352
0.059893233535666675
-0.013612909801564028
0.01721334255489542
0.03666438994377114
-0.039327393468467337
-0.05345986075197225
0.001834687133373314
0.0383478680411591
-0.062326485307791224
-0.02029750643786568
0.03574623150987437
-0.08942723197385517
0.03832901382972785
0.0045790421888978194
0.03613737467662256
0.03277324323038225
-0.05301129042284544
-0.019037000426084884
-0.05059416401948634
-0.007033073535973603
-0.020888682789468517
0.024488880417514146
-0.002565235500198434
0.04272640788486984
0.03999076786286703
0.08882095038479726
-0.036020742802053286
0.048041543075030974
0.004616679817006721
-0.028921124606315497
-0.029248684226674815
0.0883607808856704
0.021286380398919273
-0.014772224871314087
0.11581829712530531
0.01044053570672573
0.0958999761075897
0.016135452431630712
-0.09077808097997012
-0.016265616955131
0.03871734572163525
-0.010095728694784298
0.10591344193694995
-0.12713810271147913
0.061717298965469236
-0.10511830084976312
-0.06608427037068752
0.03676864085685867
-0.017586426029728795
0.0040634472933668425
-0.006340514122596633
-0.02038289729728582
-0.019481211814552233
-0.02466605437817038
-0.02933061444746787
0.007129915960381097
0.008117558988823404
-0.009895955197231614
-0.021118397172868852
-0.017725172629406195
0.00021730218170285844
0.09969248374494806
0.10191316999005695
0.054705988151688874
