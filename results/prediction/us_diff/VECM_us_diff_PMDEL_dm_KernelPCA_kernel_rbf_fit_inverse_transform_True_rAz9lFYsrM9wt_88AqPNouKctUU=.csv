# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; PMDEL
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=5, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.032610196449432347
0.0562975853853369
0.000978905184507474
-0.005137881502753669
-0.10375528965161246
-0.04343755742044281
0.019031055364470376
-0.02155480185274883
-0.02807204875695307
0.024411927861641286
0.05479186745838677
0.027673213056140282
-0.09809625562908211
-0.014206683287751192
-0.01117160873627615
0.07065755049209893
0.04313306774380846
-0.004675963385954699
0.00630877410362753
-0.025177327584384956
0.02988966253796327
-0.003609773328990342
-0.007599456191935543
-0.06960752269809896
-0.05396217132712022
-0.01893889888742708
0.032049204034150125
0.024705593326280414
-0.06703823912028897
-0.024325450131175017
0.04497803331402862
0.09737467713287706
0.04813913042827174
0.005253235794644904
-0.007461664742016271
-0.04414290426494861
-0.012108813188590972
0.017644972201523865
0.005994016138470589
-0.03978335984118643
0.07024287642629679
0.021086898783599987
0.0777464389451589
-0.044514229527119954
0.015234845256523024
0.012220912978878139
-0.04550659064494587
-0.05078765550142929
-0.023738864270905095
-0.012613973308958285
0.0438231405353439
-0.0007787439061863089
0.008976297868962495
-0.00381299061566741
-0.030641403362244333
0.027474447800588685
8.777767887840036e-05
-0.006811848167881218
-0.008665738367303989
-0.019443440911834482
-0.006180149312640906
-0.013023279879895273
0.05886373098192653
-0.051272475952372186
0.009969844601072194
0.017067933214674667
-0.009081559173662357
-0.0007791186623852791
-0.043634595126951496
0.00011672322581633525
-0.013466805659534353
0.048222731388068124
0.025455742831951712
0.018973270005007304
0.04102662639906367
-0.0010526168010445563
-0.02913842930943522
-0.0008761456603760889
-0.04551519430105027
0.03462183233198825
0.06356999328476376
0.04400009351637993
0.03298247413072263
-0.027631545090868397
-0.054058634506786434
-0.03127265970552213
-0.022601714924334153
0.0035183102069332
0.016677300556965494
0.0017758120276362387
-0.016085867909392128
-0.02892046366515228
-0.05999597220509324
0.009765515965368472
0.004109013141526147
-0.015212979775435533
0.017012682737683613
-0.05270661290854739
-0.007986425398550195
0.0029299536416359812
