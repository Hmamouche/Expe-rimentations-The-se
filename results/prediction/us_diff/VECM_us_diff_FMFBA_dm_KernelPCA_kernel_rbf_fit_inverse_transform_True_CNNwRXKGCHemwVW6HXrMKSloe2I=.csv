# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FMFBA
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=8, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.0030480923047922106
0.0019091346360434916
0.002674110048329154
0.0018261081981268046
0.0022338963441636
0.0026866032758768024
0.002178712971409732
0.002701790047005484
0.0033323900407260703
0.003041456662397723
0.0027296249174772044
0.0032294394310509607
0.0035025376963323702
0.0035761985418166892
0.0037491445187029836
0.0027733254421497644
0.002803494457133894
0.0041154780171542395
0.0032415237561282334
0.003400115805285249
0.0030315359026473064
0.0032684404866048233
0.0022772136967560744
0.002338608200593875
0.002467335196352995
0.0022804760124623727
0.003194954181859032
0.0036398835332346886
0.005316378856617173
0.004953898744731241
0.006237540790460445
0.003985328293989195
0.00271854636031032
0.005863359544663481
0.005489303711852138
0.00420500968063147
0.005743094527876798
0.006363920800607865
0.005920399965231145
0.006756205529299271
0.0072101342315002035
0.006489190027733364
0.0069647981853724885
0.006825462820187974
0.006409000270938774
0.005294508047108006
0.0046582242280380735
0.0054920562835433315
0.001787375273771844
0.0019981822434544344
0.0014018144254926393
0.0021159130256998672
0.004440282664809681
0.003507123527170763
0.003933618163565436
0.0031745027817910287
0.0045236391814128755
0.006136941556658527
0.005312246620755734
0.004403336126184051
0.006727847370289155
0.008160142473787253
0.007363768987456171
0.009860391725985997
0.009368415712057135
0.015147770233349574
0.01771166219826104
0.01079647813380493
0.020317227014945396
0.0018387998422874639
0.0037041345432741665
0.0031873007516192843
0.01210807046540028
0.008333078254640497
0.011678927011250385
0.010607290081228954
0.011847019694786378
0.0059976628209077905
0.0071153166666580985
0.008128254196484441
0.0052452834166688625
0.008244314795398864
0.005283236723226149
0.007898551206875395
0.006998709475996584
0.004136465498365941
0.0044152481667795
0.00562328523835589
0.006745191208762832
0.005945469436148921
0.00666465335813905
0.006250759711448253
0.004764556347826575
0.003169805879170503
0.0034459225334900158
0.004756051105250674
0.004327507330657317
0.002789941761409372
0.0015597824646193055
0.004215514631709293
