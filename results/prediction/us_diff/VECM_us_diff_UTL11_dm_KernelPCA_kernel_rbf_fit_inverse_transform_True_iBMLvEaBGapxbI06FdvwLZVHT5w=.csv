# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; UTL11
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=15, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.18832904366643982
0.15147009513235615
0.17331051275368453
-0.328602127496567
-0.038972504813951384
0.022890131140714837
0.07096203729754832
-0.013390400842301114
0.07327762627520407
0.005426679325363566
0.1789919552726952
-0.07543207680427295
-0.11656816268175316
-0.1401726632282776
0.07524639015685869
0.0678117679244497
0.0750015613809589
0.0120065738817467
0.03280830123273047
0.03684038486283989
-0.013242567626164279
0.0959004923277575
-0.0009634030399198459
-0.01775695658333911
-0.05381722919955114
-0.040847941874857456
0.04481726284457721
0.01531671665074982
-0.10438418374172176
-0.06253934621703577
0.008986107690108394
0.028797100270857613
0.01976326719848442
-0.028724495131342103
0.01882469462710262
0.08110228756060145
-0.04301743403312257
0.0686000654084516
-0.0701503797533542
-0.05187753442864083
0.10604621998785306
0.03827405350229322
0.08345058875681013
-0.010824607561685632
0.0001367309488832097
0.02044185483713257
-0.008438709882971308
-0.014381224089117208
-0.03440196921273811
-0.02455407440054911
-0.04251855299758132
0.08155231764768499
-0.0027078413652075403
0.0060453742503654775
0.0377897029840918
-0.006880387106990486
0.024747758609701805
0.016347718175688173
0.02161497055345197
-0.06562424373198916
-0.05282470921548955
-0.06582811942420007
0.05495115316025494
-0.030559154588929445
0.004590409879194495
-0.014247610412134612
-0.07933539249637556
0.025649677514283296
-0.07522976151758706
-0.04991253691761398
-0.08409054249583729
-0.011803241856305515
-0.05960863697097271
-0.11671321289605924
-0.010681741943137117
0.07803736430466199
-0.03968064624507185
-0.061263680685268176
-0.04777289366503785
0.08789458578921003
0.0423907439170642
0.05039609902392446
0.05091286066225228
0.012790020566350212
0.005509866906819342
-0.0003550082795939144
-0.016391925266689488
0.0064733349231911475
0.015544011557385497
0.04664622680430962
0.029740160280184606
-0.025784910171368764
-0.012543109127754774
0.02445268957714701
0.023680049829547273
-0.02806628056071008
0.032633839494868117
-0.08043757726338985
-0.0729273741906504
0.014429396704068388
