# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; LHU14
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=6, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.01248693393235576
0.014611730419428655
-0.051793297857331694
-0.014415326234088964
0.058396460817229996
-0.04821294491596274
-0.046686374341600334
-0.0203825284782827
0.043465583184346394
-0.01692290900125223
-0.014440875765570904
-0.020208709430191964
0.04729506649462973
0.04184472832927732
-0.01972244832527887
0.011398207852279744
-0.07805302724230367
-0.012124146612162306
-0.02151322049833203
-0.07428991070788524
-0.0028599631561986912
-0.051164394436644
0.0037565205494016684
0.008012794973099951
0.00779708475581305
0.009161849081886839
0.020547060600010946
0.019136550264792992
0.07031843995983106
0.08762775151833448
0.004616282818535585
0.0355069892885983
-0.02909128883202654
-0.0036923543472669497
0.03903645758263333
0.01946691661509909
0.048088710899019206
-0.04022935074468825
-0.010406183076601745
0.0419266723715629
-0.028509289128651343
-0.05343115833101971
-0.03561620342743628
-0.01875318244469104
-0.016080788561963332
-0.03311632261285875
0.004181684273724909
0.003327650814890726
-0.02033317162914004
-0.0006667161930567315
0.01543569719766642
-0.003310457688854394
-0.007386545716159619
-0.03357922264042407
-0.009827380868450275
-0.02742152190156245
-0.02758709489006224
-0.009474339567675814
-0.028061344026722328
0.0015441974732090767
0.025621098442092375
-0.014528145447410549
-0.03360584043069595
0.012449310614081043
-0.018733247467958538
-0.06342586153774042
0.022739340683070007
-0.0028342033627991176
0.009466648367749606
0.01696393555775389
0.057203452696966056
0.00542940622106245
0.03881460384780712
0.07118054913404048
-0.03257617900209896
0.05607504556904889
0.040517139832151695
0.0208347906953844
0.046808421754177564
0.012110964896936253
-0.04512121680803006
-0.05775698949936272
-0.026876940052961526
0.029710965697507467
-0.014947842205124545
0.0004132117621818875
-0.015899510514261906
-0.009931256183890812
0.0052364694818149305
-0.0182084632437811
-0.04290776622399703
0.01566784058239455
0.019083178586903025
-0.030472873474356933
-0.0073026291421368975
-0.009913945669211473
0.019703043378824024
0.03524101613976928
0.05444777994191166
0.05794869655349305
