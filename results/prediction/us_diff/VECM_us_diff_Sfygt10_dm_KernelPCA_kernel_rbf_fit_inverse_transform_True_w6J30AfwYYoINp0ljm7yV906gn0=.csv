# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; Sfygt10
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=14, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.038821549296129354
0.09495059885364909
0.17180979508447344
-0.08235172765133093
0.09996047617015133
0.23892995319086524
0.206250643104567
0.19437559424953374
-0.345325727435338
-0.20303050061077857
0.016530757054876603
-0.0586957019854528
0.017689555303010557
0.05439795406212164
-0.07732682748087535
-0.13262913662007825
0.10572797310781601
0.055441169093521835
0.024080171641010735
0.034890267571372494
-0.05567558661453596
-0.0875362561114627
0.08138727171764262
-0.1444310148496234
-0.03793085699711639
-0.08096386007526975
-0.11984292911300364
0.06037302802742668
0.08790061885660216
-0.07972569144322424
0.09547448683801744
0.2279898883477962
0.18515010159548684
0.04838633611713136
-0.16212064546363586
0.045563697443089524
-0.01250329467769657
0.24780015396866414
0.22510359344138078
0.03950421036451694
-0.12599711556726756
-0.0672602575632919
0.05950048686785381
0.01256484116473752
-0.11345010760442889
-0.010817523191147593
-0.028285986612655377
-0.1272166129113399
-0.1781059962956499
-0.06727228777175706
0.01327835123612775
0.15163207359229436
0.03630339018832471
0.005132966494398421
0.020074786921034427
0.03346543606687585
-0.07966306851254577
-0.1927034336839044
0.004329757189632664
-0.04416126895067702
0.06203455045455532
-0.03235773580650878
0.015232049664776533
0.14267648112192877
0.0004454481144803811
0.11991902819591423
-0.05540702380924753
-0.04273824907577074
0.08343204479720097
-0.149511468518092
-0.012249558152980214
0.0015280697530896913
0.13881372540776105
0.32440827651610227
0.03074828149357564
0.13202312438948433
-0.2852355818788629
0.23765828711607093
-0.0323002894866025
0.011263906135170768
-0.12632861913958301
-0.035723520830086744
0.15392809919802616
-0.013453840196134818
-0.07028795856086417
-0.042759888253368704
-0.08833949255872789
0.0800140323077316
-0.07162444707503149
0.03157081869777019
-0.08065917433171321
-0.036003764169614524
-0.17183475930921685
-0.13595557643441017
-0.07876925360862169
-0.06165298636038709
0.006193983651592412
-0.00936177712455636
0.16233108530415685
-0.010244532275776975
