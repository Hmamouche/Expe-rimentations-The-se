# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; HSSOU
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=11, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.06746567475381998
0.014476082205606672
-0.012104689155445061
-0.22164724765467958
-0.10770557211550295
0.059346367070951055
-0.05414640044891195
0.054470798028118536
-0.007801958409153653
0.01118372441479347
-0.002909576738591553
0.04687680235099939
-0.1359681744058785
0.006407156515092613
-0.08014667709794528
-0.1296893062286248
0.07592859397632061
-0.0005819829055401923
0.06006826473380749
-0.09395096367688623
-0.06406533493384294
0.005848347734475144
-0.05623334439323068
-0.017774689896183027
-0.02995835762888682
0.06814611289504545
-0.0007585122586232264
-0.05658496729205878
0.07667336129700646
-0.059137147789749996
-0.05315879485232551
0.08788444080877034
0.01284461715058332
-0.01884265273523196
0.020845582182191173
0.05496120011969404
0.0058145590604438725
-0.09857640643487162
0.0800418734744902
-0.1256768042010591
0.056757214921935965
0.09164685832209868
0.00991980743494603
-0.047911540193154634
0.08690150281227035
-0.12215211064693712
-0.040611027521194824
-0.02874496805186589
0.021887249498950927
0.030589792208664403
0.0411068497949346
0.03882436127128513
0.09063734048894605
-0.060355741868007434
0.06512544637948572
-0.05995563765918616
0.047508661960741425
-0.005124618454030842
-0.013613157374305046
-0.002553131525721156
0.06232036657521971
0.050393085925236926
0.051918278892507226
-0.013826247722998238
0.0009240935570969909
-0.0387475448862897
-0.0465766507265803
0.004205596246974535
-0.02571112654293698
0.055373452951565566
0.004185978775192653
0.006214175936865678
0.04405106295978931
0.004306948578470096
0.11957045418425666
0.08219244673262163
-0.06475000635739837
-0.0199274869322855
-0.0032848708709999758
-0.06622041311877869
0.09295556284424586
0.007532664965768071
-0.010385662199839503
-0.002882535809829806
0.03948470688237311
-0.02150235725567824
-0.009184882597665877
-0.01259583247562426
0.06368034329848773
0.0031828357020801806
0.03439868879955132
-0.0044258167675777435
-0.0406496106737631
-0.0770521306594932
-0.012787872403245897
-0.023426813209891095
-0.12102137567554386
-0.1520598581721332
-0.047574134563228815
-0.08482488392310454
