# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; PMNV
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=6, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.2326202524142899
-0.00805600430001549
-0.03231341722282952
-0.20416922986394187
-0.17606177122563918
0.15007712477770663
-0.08549881183644853
0.08101152854048677
0.07329628326744361
-0.034884417931666814
0.041134272324544835
0.04060153461401307
-0.0019574667502380606
-0.051131186489696466
0.05838001256935452
-0.022213340518251322
0.0705418275807429
0.016845344697432874
-0.10426565352736708
0.10938386071793879
-0.15344413237573928
0.0842085860130197
-0.10363156604799426
-0.034046831357569224
0.018162656539209805
-0.05563834663872229
-0.029491900018405053
0.09850739162674733
-0.022247638859317125
-0.10303373608342573
0.02334964723785319
-0.09738173476163849
0.07975401285598771
0.11470064221990162
0.13432188420157548
-0.07383913961627812
0.010531157037870303
-0.06437901247025903
0.0832467423858694
-0.06516941534419915
0.024147953925331308
0.05824473644601652
0.025314131058649006
0.08570700494325413
0.002300056281026759
0.037492393714163214
-0.03477532173503381
-0.028919385671558683
-0.030952015148691912
-0.06661628937454506
-0.06696980073368482
0.009775830352669301
0.02972217911029343
0.04611733789236029
0.025398143271948826
0.009941446754129321
0.01011802434691323
-0.011423044513239847
-0.021019430811746086
-0.08398337071529892
-0.07531858745994716
-0.053315438531627075
0.08210750165570319
0.005388744879575885
0.05790267718302281
0.10878204392785912
-0.048122255636298225
-0.015862473264650664
-0.05711322710985155
-0.06477659414094213
-0.05698434283581652
-0.024792524548783383
0.012844335747791018
-0.07170212643169321
0.11901489673029896
-0.022152642474364832
-0.007410057263329214
0.06715720594509894
-0.11896862487142265
0.05872555631226553
0.05833160200673538
0.11671532502567573
0.12227854730751081
-0.01880620133886801
0.011073823084622375
-0.057701436313377916
-0.06286156633397533
-0.023133959560081074
0.025533790944398525
0.06676633636524093
0.12157203125980759
-0.02778523456589279
-0.03549028125353369
-0.06054082921215053
-0.09006078719733959
0.038991879121288625
0.017802918355839418
0.014424169969609137
-0.037304398014417546
0.024437269509531064
