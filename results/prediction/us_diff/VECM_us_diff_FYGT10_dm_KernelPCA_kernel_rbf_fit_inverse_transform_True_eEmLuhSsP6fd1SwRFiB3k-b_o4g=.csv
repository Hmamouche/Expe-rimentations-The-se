# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FYGT10
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=4, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.036516068750820425
0.0028251429892534596
0.07318014078857825
0.0709157207698215
0.008315996361190525
-0.05871692563586551
0.027955685927128424
-0.08829095120087374
-0.06885769910422601
-0.06525024909256785
-0.06352588891287989
-0.041997615334164504
-0.08549013468481201
-0.06354693421647875
-0.03523804229217877
0.04013232199474766
0.07145793374417686
0.049741564624214446
0.061958050071351266
-0.01855634736865385
0.03216654601481638
0.012791574721971925
0.020432615281746665
-0.03147265471750828
-0.04559999343289034
-0.02367114354639972
-0.015108466555645992
-0.005638036293296132
-0.013361311158970959
-0.02859762814444701
-0.010534728443006137
-0.0013287483497804488
0.035193196453741506
-0.017985904012860505
-0.004939000599783687
-0.025317561429930434
-0.036572557005968294
-0.002955172541855023
-0.0408513939406421
-0.04161033270634805
-0.030871080296940585
-0.0033638552083385016
0.03109759550552797
0.04840761529675022
0.04381939473960883
0.05834452205769701
0.007106021496488092
-0.020308064436428068
-0.022097433499510325
-0.04618337454849308
-0.020613303218049453
-0.003187828495073261
0.01543987728106434
0.026049804991936962
0.010320554574851135
0.014754084703424484
0.02286644781640771
-0.010612332168741773
-0.007584784488304285
-0.036624295877966194
-0.033109924395448465
-0.05287311132718499
0.020404816373779486
-0.014042220467697961
0.03258666782566042
0.04172789239905864
0.04381958831980145
-0.012335701430062644
-0.004165697900532046
-0.043894958366396904
-0.04403779018694452
-0.004497048918358306
-0.0335600596064517
-0.02806538138395625
0.017831742932024497
-0.020391968477922823
-0.029090361044436946
-0.032012365697765686
-0.05873840158544796
-0.009152696041663005
0.03608642203864153
0.04003334353750255
0.012298614180111187
0.013536181690717979
-0.028778313774692185
0.01551584608205134
-0.012744091936158795
-0.022907493330855667
0.004881588544770916
-0.005885219733878981
0.036404416822940176
0.009037090969925691
0.009764670197849538
-0.015426815227318109
0.03198183723585149
0.007423140948884522
0.003894102955842865
-0.039501067052919614
-0.05484204894653912
-0.025621213759377006
