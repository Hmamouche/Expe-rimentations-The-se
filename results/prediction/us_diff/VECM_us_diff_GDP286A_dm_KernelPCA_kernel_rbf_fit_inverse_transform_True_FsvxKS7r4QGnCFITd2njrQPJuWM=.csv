# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; GDP286A
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=2, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.004820190880664604
0.0029725197973153317
0.009740216369142874
0.009790411807046649
0.007476710768324523
0.00712952699680712
0.004855082117186645
0.003987358489467619
0.0028641846276186433
0.0034956045095344375
0.0014704046612795583
0.0026103263742482895
0.0030653951482901265
0.0016870204216135381
0.0032495958524283147
0.004228764356826884
0.00493128411143889
0.004177478323870408
0.003542633092304742
0.003899149767416843
0.0033926574039995707
0.004007485048900114
0.005127251728630877
0.00384789635976211
0.004206143994519116
0.004942139498515004
0.004977388462530331
0.005148662994552473
0.0055938642280168715
0.007618779770301537
0.005445101141453008
0.005586192982923477
0.006421400911666747
0.003318347977340879
0.003349360001687128
0.004435595276471618
0.0039025068366296185
0.003466825204832988
0.0034770979793328763
0.003138923622715873
0.003545882471659152
0.005178173067232874
0.005131793982234034
0.005796743774858967
0.005272516955094713
0.005243484520261485
0.004988351136075374
0.003906518715904438
0.004556749272823754
0.005027525546914692
0.004877555230945938
0.0035033053976588563
0.0058470721187112405
0.00445396811763615
0.0012581509494337048
0.0048249486457181585
0.004193546728070892
0.004090505062742298
0.001444797890686684
0.0028873603605206496
0.0036320386360684746
0.002060208710717825
0.003437944402290132
0.00478649485780765
0.005869166934445176
0.007495508776256888
0.009153682744868273
0.00730229365588613
0.008562582587735937
0.007913824310045245
0.00405672365093376
0.007014003410492516
0.004558678210349703
0.0037626726719006817
0.004415381429670367
0.0043601443277879095
0.005347630622232418
0.00833654758378422
0.00816870162779245
0.007450985282115704
0.010678451162614762
0.011676272517514656
0.0046946057320779114
0.009661068644614098
0.00944027358889806
0.014487640290292793
0.01221735070924397
0.01166650557866935
0.014647764004418102
0.015153007085842524
0.010932071687707226
0.015349075318110248
0.010554103807950566
0.009261368145873862
0.010706312111148713
0.010060136088955712
0.010085502385315272
0.01555816150525399
0.012800108700432206
0.012225963713945074
