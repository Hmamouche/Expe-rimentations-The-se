# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; EXRCAN
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=1, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.006711278329496069
-0.006251639019197911
0.018502068882301926
0.04682271610915238
0.025083145361096557
0.024214945217535585
0.040365267868692864
0.023490032763961374
0.014198617828466265
0.01837607762453904
0.022280961287136844
-0.0002972306033461637
0.013525434393363286
-0.009990867109006808
-0.0259114476117664
-0.01090527534161567
-0.03997424417090697
-0.0005210951270547559
-0.054320815381327395
-0.029658522117740477
-0.042111199587032015
-0.028019414075713796
-0.03129252669548979
-0.015564717904501592
-0.018393860741982317
-0.010965818556459072
-0.0004268419432297624
-0.02514030601664628
-0.007482205209057794
-0.007302421578307216
-0.010434923092886142
0.0036495856720278547
-0.021397106605661062
-0.01679697258721743
0.04054431958099181
-0.00047390041647067725
0.033156989238963167
0.050966505717496995
0.011781540358676443
0.05652819708373432
0.013562235578408166
0.0404886012965277
0.035061469812375214
0.04343713626194049
0.008549438230682927
0.040732907033827176
0.011272821454814952
-0.008888225419837154
0.026497095153493442
-0.0475179267309864
0.015353001088782313
-0.01149954271997685
0.00597993922340642
-0.014717232329644973
0.011254669975523112
0.0052263053468027554
0.012395826785089001
0.030067118523341096
0.01089415589200746
0.03723932217642271
0.05584780280238593
0.03539690639057485
0.036932025755978544
-0.018976189402908402
0.01297529427624267
-0.025222762710582717
-0.0031078690012124105
-0.0010963446328375644
-0.00812314655753796
0.049246487216042
-0.013015453737158966
0.042731112202038034
-0.008034614236273229
0.03331577403450696
0.017795448575899753
-0.006295905805327837
0.01596156838448382
-0.01644706139480359
-0.016128295212151027
-0.0785429910062933
-0.04984846088211351
-0.1275750226073789
-0.028603321927528853
-0.028101847356559528
-0.02776045559008803
-0.045859118708879376
-0.026642477083773253
-0.04272601247770061
-0.03929454687472558
-0.025007529805255276
-0.03548796690758439
-0.03968762585633807
-0.022244899925263878
-0.019135460240126422
0.016077061792140857
-0.05182425710404062
-0.004453455806931652
-0.08431971554517696
-0.015223432987197558
-0.06341943386722369
