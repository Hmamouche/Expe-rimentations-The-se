# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FYGT5
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=5, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.08256195440604683
-0.01487575842025974
0.11919225565822436
0.048658970506075816
0.008812014496108574
-0.04135311907393702
0.016621073452419824
-0.06351348597789185
-0.0631857315707432
-0.07625678703599133
-0.05183624068341672
-0.010805967775096135
-0.07773908024269768
-0.03277531791793624
-0.0676915496008076
0.015525371024350577
0.0736492356998247
0.030028172150290328
0.03961346761974493
-0.018340692011516113
0.030771749405842506
0.041722899286055486
0.01587096541457405
-0.016671462304772233
-0.019170270439833506
-0.02159416936069497
0.0006743123335162145
0.0016377488322821612
-0.039222979843127476
-0.03277970395864353
-0.006628265310309354
-0.028795920603452246
-0.015327173688285548
-0.005444491177850525
0.04043325110294964
-0.03216024267358163
-0.060431419242739245
-0.01936340135041526
-0.04592333497872285
-0.029245827957774556
-0.004236658976665296
0.014007048912208418
0.01960797483654645
0.05238049025162823
0.048543885872436576
0.0821416872229599
-0.016898010385374293
-0.01853693544933986
-0.008017868758236719
-0.035213137236252426
-0.018464349362491004
0.014859681629654523
0.009928435637170897
0.006277033104417658
-0.002187785783644699
0.038556449541178525
0.03383901524471049
-0.014538063727615312
0.0007256793944552583
-0.0322255211362894
-0.02594916803063151
-0.04721875209386539
0.03860288013897635
-0.026315617445181322
0.019271797882341533
0.052758507168767566
0.030656986448818677
-0.012616092437686816
-0.003960685185614942
-0.041636885005530644
-0.03393051144718645
-0.017129622926213398
-0.02800949831710995
-0.07226811153668142
0.021901843166619812
-0.018736659419887045
-0.05653865486739809
-0.013856740929937585
-0.07855560594932146
-0.01875169597165385
0.03570336781372355
0.025083560355955825
0.04784764564418505
0.003860121170860771
-0.029651069655660774
0.027403609092406845
-0.01163603315295118
-0.009616156664090552
0.015908031949200697
0.0068849689325455425
0.04604137420628567
0.002984187910424251
-0.005035286980912944
-0.009209391278257838
0.0030823527618844253
0.028982494069899528
0.020862679918956215
-0.050192510441982
-0.07383601211413547
0.012925327374267444
