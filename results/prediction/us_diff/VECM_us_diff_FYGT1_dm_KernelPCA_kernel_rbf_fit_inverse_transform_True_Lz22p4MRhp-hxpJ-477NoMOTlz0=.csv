# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FYGT1
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=11, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.13124691476008984
-0.04714373871060096
0.1864658376036969
-0.04161413260232963
-0.03381203984857281
-0.09635103268382797
-0.0015570263632506548
-0.039506815252316425
-0.027612674488095908
0.0024172409367724072
-0.06318656290616818
-0.05820532347062928
-0.017746609107522928
-0.012425511065113831
-0.05573813840851013
0.09631315302261925
-0.02384107254597314
-0.08585154984702215
0.13712229838363582
-0.008096225407594804
0.03361742726868276
0.1303700313406958
0.03399797498693986
-0.04325139356038143
-0.05823345637003054
0.05445530357970124
-0.008151882970347694
0.01172157061270247
-0.0711895139771841
0.0483844573800132
-0.14003633375958363
-0.058985597668976694
0.04997601591018655
0.0196132068223145
0.006934086004586823
-0.01495793783581045
-0.13796741032133125
-0.02482290929972967
0.04093631369746406
-0.08204435593783982
0.09242578510561883
0.013306038192140782
0.03156565954525929
0.0002836714930251928
0.006481195223340768
0.10874690368641493
-0.025869403465161443
0.041761662888622056
0.02813346270074238
-0.031306731280665474
-0.038223491205037624
0.00039553295882576583
0.007053857837307844
-0.0005777215381198662
0.02623004725397737
0.006312311162187913
0.015669152296062576
0.013968504938720342
-0.03918714641431568
-0.023527593247779352
-0.030822293061746404
-0.06811305734464183
0.03749469271876019
-0.03278093951870931
0.01960943369912944
0.06661482278636435
0.02619083494768525
-0.015013531606333709
-0.025308451866828934
0.01594042556821093
-0.04536416204335285
0.005494180021236761
-0.0750851479352217
-0.10096219566422786
0.0933639507791553
-0.004879626968553884
-0.06388804085959865
-0.03394291180201772
-0.060488950048519695
0.0643014570986945
0.013686215108733543
0.04464342484916657
-0.005478185996936749
-0.01845605784215342
0.003311663378612502
0.018833529885125593
0.008686463650746198
-0.009202319460359095
0.03659411822410507
0.04026403309389108
0.0203361438804089
0.03711102900670371
0.027895184650113685
-0.02451650600320046
0.03272377183222515
0.03164475029982411
-0.00692211236348678
-0.07191769824244516
-0.09632361841601367
-0.0030323169580736588
