# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FYBAAC
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=3, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.09628467204995503
-0.015672761874658214
0.031411805747645065
0.07219117195088724
0.025778999476493453
-0.06630291503390622
0.014083507566477974
-0.06097328875527712
-0.03475855132291804
-0.046065389382046726
-0.07121555549975879
-0.04783379510943738
-0.03630332096347916
-0.038268521863541474
-0.035282093553642396
0.03490731447264903
0.03270443927244006
0.04973909756507327
0.008622752078942832
0.009581410652898258
-0.00501965963216207
-0.017414934024934818
0.002322478078050986
-0.03705050168302003
-0.034088872790336
-0.020456632179604715
-0.0010492165788411976
0.009824050265330364
0.0040919421436105
0.0010752938185316636
-0.027967601463602262
-0.010607352971433552
-0.008051227161640036
-0.02596035518911027
-0.022247720967171744
-0.01116651098925631
-0.023076912848583975
-0.00504282515823053
-0.02646962680637048
-0.01913887035428185
-0.038184081292483886
-0.01728286130780076
0.017423773333652703
0.0635090784539566
0.019198951919221732
0.058114959896788165
-0.004257292022337901
-0.03848387305458867
-0.024764405723666325
-0.035804543090812706
-0.008081906416768996
0.01638124360954279
0.015367623029775668
0.005086753968506505
0.004142040629052362
0.007406535065892181
-0.004503161238660219
-0.012475242077911982
-0.01866813209714925
-0.013694713759596714
-0.016636115771295312
-0.0076822385536358204
0.014298124746390983
0.018517923298814316
0.03134026640829956
0.021659056693990663
0.02314477970144983
0.016242325976444576
-0.017183248568693466
-0.01567603512503628
-0.035257671280977544
0.0025299393605903208
-0.01343321913469199
-0.007563219351094175
0.011594995423761167
-0.016071132083716348
0.006429186748304458
-0.02585998371795238
-0.026182537063102924
-0.03421987635599032
-0.002251386665542257
-0.009940298065033007
-0.015286752140544542
0.014933099490412242
-0.025941112764845786
-0.0037639045368602405
-0.0196644296279462
-0.010773488692320513
-0.005149879047993174
0.015355830724649966
0.003922410572586061
0.02784615752202431
0.002151952053343245
-0.02291921082240385
0.010944117069413966
-0.0027905909692008454
0.004218559528983355
0.008944598780475516
-0.00254544049110023
0.00919470088622516
