# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; PMNV
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=5, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.24678050767603998
0.04822962726049581
-0.138199399144377
-0.12479893024995724
-0.1888408428606302
0.0644623725162653
0.0014822912237695135
0.0635386375499208
0.06610222520086899
-0.05592465803595966
0.04904891084424305
-0.007335429171014464
0.067037367668159
-0.06585816912432933
0.04556071754650986
0.005415526983953818
0.04316026933119317
0.029870972170446583
-0.0820640969393322
0.08131672679229118
-0.1282613051414479
0.0310803360571629
-0.10336452289909315
-0.02195083356295062
-0.006288128132334481
-0.02645388584707971
-0.008561686343218983
0.029088067934995156
0.04837893238493924
-0.09444172608743642
-0.07127664360810607
-0.010886919995804219
0.12962226804709326
0.035622675910964303
0.15932886946339503
-0.08798595804259514
-0.006506401427700793
-0.040759199002260646
0.02833932543467125
-0.06416449278965337
-0.015582941035374494
0.05401055323664663
-0.010992174060350676
0.13231406074123708
-0.04953937385056566
-0.005333584441423369
-0.10283387699976919
-0.03723762467195688
-0.07572032658084574
-0.08429353589549135
0.03657028245975263
0.07389184555990569
0.04757583915599538
0.08810006114200399
0.033279310717073136
-0.06403668697609802
0.03550882110739031
-0.013544939721732298
-0.029927928777529797
-0.0353792375399207
-0.06614799876303612
0.01008790576034372
0.0885512237906462
0.06822357347620125
0.04934718893905837
0.048969782506630796
-0.05628182721397507
-0.0652311738419965
-0.027662879575824147
-0.05388101561017536
-0.06316032515747338
-0.026189085585793817
0.019944250739900526
-0.024674179950776094
0.12093352256443898
0.021427246152483637
0.02427705540212361
0.04368106186512691
-0.10832113766711059
0.00021342892479425668
0.036965448933414705
0.11106018031104911
0.06429811076356812
0.042209503314903106
-0.055966606135652144
-0.059402173136940815
-0.06339032075257267
-0.04422662095904554
0.09470436401044699
0.021852674051046933
0.08039377294943634
-0.04347796061087056
-0.03800401323989018
-0.06030216529058993
0.0027894240422013904
0.030736947429484608
-0.022107628448909008
0.07986336234057645
-0.06287545435121383
0.00610935556090219
