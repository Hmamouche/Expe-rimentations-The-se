# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; Sfygt1
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=5, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.030687297841672012
0.16488985138771695
0.04130743280445009
0.13448802238864635
-0.08572158259300827
-0.0006316159456572143
-0.011064969903581724
-0.06448469938105397
-0.19037276373681394
-0.06104709825850086
-0.08554880402007568
-0.08071025349981907
-0.08345109599645059
0.026688703143742974
0.0026131143251449038
0.14158882828966446
0.12722804540741123
0.09352805987605281
0.08856751067740529
-0.018133428660712752
0.005377518968839784
0.07185655965472255
0.017869290851719975
-0.05444497649260711
-0.10318204965829438
-0.08984032588492064
-0.06957080035600574
0.019887888156309798
0.030233358738433774
-0.09240636531344153
-0.019486696346123086
0.044223467797308805
0.0045249282190646944
0.08458912114314748
-0.03580808035313754
-0.07818996638518241
-0.0350672695262811
0.027917291080711996
0.019815647769485474
-0.08601953214638425
0.010162522549709847
-0.0016987448594706425
0.11427868600777592
0.15396474508183317
0.04695359225460065
0.06531798866746844
0.03474114314449954
-0.025724415961066463
-0.05133038736100665
-0.12723895240071853
-0.09453512232532377
0.02203234676356582
0.06826631324054351
0.03624034634091611
0.07455236435185626
0.015150190927429814
-0.04120059725019812
0.021960377097578705
-0.010495604549745298
-0.05714377475570488
-0.04377210016370056
-0.08713393843912713
-0.008246345019806294
0.08676818098724225
0.03905026442508611
0.08009238652185283
-0.0007786302820529251
0.01715809019962654
-0.0446796545760763
-0.033475478888339995
-0.13087317196362155
-0.027743078285111432
0.041910097699242095
-0.009158633942053261
0.01526294496043494
0.08905308850923847
-0.09922073060752575
0.020650827336467514
-0.15889151770275023
0.06226313399018042
0.1339583463380191
0.013155360790890278
0.016354697328882964
0.031295938639437824
-0.048608980265312664
-0.019982159465322075
0.04314690321090443
0.0008468372090562042
-0.015357730198335674
-0.005288828251670773
0.039822686832253575
-0.024034405432604405
-0.020070441927645982
0.03471453880547472
-0.07267936500986946
-0.0016778401121210346
0.012470540681332114
-0.06586128475114922
-0.11219541424532346
0.0035372343157236766
