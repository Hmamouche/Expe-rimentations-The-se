# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FYGT1
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=4, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.12369690631928934
-0.07116551243950908
0.0713398747346577
0.04217584130209161
-0.030834170182890218
-0.060358107424389275
0.08243770600605647
-0.0726647689250903
-0.0660848509025393
-0.06759968121472121
-0.032838749684916825
-0.012659159438586289
-0.058670688934108935
-0.05549842665172207
-0.027999125370705126
0.00013937490214011548
0.01922195734432624
0.0018979248469528447
0.08002398554311849
-0.005415338024162908
0.029423398817027446
0.0090272127843198
0.024990545079487187
0.001816087823145407
-0.007837849498954323
0.012496846395442185
-0.010287439402009678
-0.026547014966404194
-0.06614788874847621
-0.037933840628521905
-0.029705477784740378
-0.00927610723474047
0.010292388355202678
-0.01977618771971242
0.0002216515966016741
-0.05299407126388326
-0.05223411063753175
-0.01090122178060969
-0.03881235863682321
-0.03137249029720822
0.006890373061849102
0.010451996478495169
0.03486827077644464
0.027636988424849032
0.022632948582970134
0.07593312227654465
0.015112116590545694
0.016415826867571877
0.014244301829357247
-0.024417671855606924
-0.013278084053850047
-0.005657541255327121
-0.013219825181120965
0.042668378896596575
0.007893365860435785
0.023231901640304756
0.02438284667553874
0.01000962555955249
0.006135094528337419
-0.03326100250067721
-0.0361250453607737
-0.035527605714332916
0.037181533154439096
-0.03959954549630232
0.013305511032219228
0.02819731132614691
0.03820858031820942
-0.008296691178872677
-7.499821272181559e-06
-0.024810143681190402
-0.044777484764904216
-0.02023464890803511
-0.058924892205295176
-0.0906786364312116
0.0170913910622291
-0.056444896588212265
-0.005885889385438159
-0.041414387584188576
-0.050045495957908526
-0.01110433697004776
0.015950354659821663
0.04151000197961121
0.028167435368212645
0.018894699537915515
-0.02114726858046575
0.011351190948055858
0.01196345194940944
-0.008119606673633178
0.04167307398644583
0.021828851994173788
0.0506325424112679
0.012934609231029187
0.013498652949405339
-0.0034215185521336827
0.04155613443907355
0.008411774302286387
-0.005426223311889044
-0.03526771094077115
-0.06590943560952756
-0.025378694966867867
