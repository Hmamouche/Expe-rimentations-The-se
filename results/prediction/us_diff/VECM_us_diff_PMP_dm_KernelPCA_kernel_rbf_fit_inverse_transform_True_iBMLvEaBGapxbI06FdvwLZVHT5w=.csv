# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; PMP
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=15, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.39816956947359
0.2287297557927566
0.07618267454710884
-0.38019429790797743
-0.09077699685797636
-0.26336258882369434
0.0010889668911785721
0.3634342616634292
-0.04603579714275813
0.3172369228964034
0.3000406939666701
0.010370633973617378
0.009698193345612588
-0.07334426901306502
-0.012998734331803513
0.056829657776990436
-0.08674595254759082
-0.174167510490227
0.04502713429104663
0.01998769291574641
0.025505089684238168
0.07074036265828239
0.023113947826996398
-0.15419439648010544
0.03732041829830053
-0.0704507341893549
0.09712859019105544
0.018797252228180888
-0.10466746142282349
0.14616294582263484
0.15950387966573876
0.11677898696024423
-0.023420949387482667
0.04875054860217182
0.0920536643942039
0.09559476987274808
-0.12249865837330631
0.10472910198355093
-0.14479228078322604
-0.1388059267528405
0.043681311437150094
0.02309774785874497
0.06621167466982548
-0.080229875998657
0.01796760018657719
-0.13011984005618557
-0.09405144359785719
0.10434262729811522
0.07114235560117821
0.0574712756963355
-0.115100171813275
0.0482113177975662
-0.002820285833539476
-0.01050308495433698
0.07769532279249719
0.03705911995336435
0.032069231423965575
-0.059842084829265244
0.04491841882534182
-0.054293430049254465
0.008076291784138169
-0.18936858009576973
0.16938679045299337
-0.0888893592052249
0.154958691024653
0.07812010151691363
-0.05594633566367528
-0.03911947840558436
-0.007445218574374957
-0.05471260332584045
0.026985455733403885
0.18613976406660562
0.04499441326437294
0.13672821456716372
0.014010077197798648
0.12319065889441978
-0.17624725245239098
-0.08120183115737595
-0.08091442216983842
0.17467858201714082
0.10555236638401298
-0.07956330922818841
-0.023288785079206197
-0.09200672299328652
-0.07224352126101319
0.03784174011365074
-0.08100077596110647
0.03862548904692694
-0.06765184388037701
-0.08433559952236741
0.07721331019721275
-0.036489357574802386
-0.06165172654537804
0.042359665456268444
-0.007415992938132734
-0.06031544696818528
0.14671725662622606
-0.11176414557523127
-0.13831938999459642
0.08546559889569268
