# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; EXRUS
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=2, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.002032534475024529
-0.0012050985742182406
0.01280949694692727
0.039138944311568266
0.05622107210630566
0.048005028090469665
0.071359278603476
0.018727563005989344
0.004707559236836311
-0.0811357368426775
-0.09655122495089766
-0.0698122170665273
-0.05871219060670129
-0.013505792516506908
-0.06154202574042528
-0.08060616520772375
0.004241773916091714
-0.042673452044181145
-0.04370482992116939
-0.010475087918814439
0.027941962638272186
-0.03625022015555852
-0.0018530987463822712
0.03645603144920817
0.009425613335396977
-0.009128075834328528
-0.008436779055042686
-0.011666383874615553
-0.04872190652458308
-0.05283361388703806
-0.01787433309789831
0.03316546880966583
-0.014563640370437924
-0.02298435046419414
0.037336015524576785
-0.021459651544992124
-0.0363417710777531
0.05314221614884147
0.006674173903004156
-0.00476241157033903
0.03718897029494939
0.0015402518460504687
0.0004868877157926197
0.006457126818381141
-0.012548160362103475
-0.00040447428377998057
-0.02584367452710297
-0.03369955929058782
0.013128253787445834
-0.030275981556620726
0.025851966418779203
0.01677167467772951
-0.01577885575055771
0.024466515034650214
0.03720473486108981
0.018654656815705148
0.032617286212267775
0.026333708668227047
0.03353076992357482
0.017329115045990537
0.02840696347905642
-0.025584530030940123
0.005126105705341688
-0.0041352703833551245
-0.010404175012814104
-0.0043166618617583325
0.018514980199785078
0.01730249459522478
0.0317753395245047
0.04760225156378539
0.0017080139176679013
0.036249067950710236
-0.009590144376339912
0.014807271019036165
0.04685444447028459
-0.04385875559460357
-0.022495562221007816
-0.009185695243488858
-0.062246727614212724
-0.04777129770994896
-0.02921012136023867
-0.07679408242883354
-0.033440812233301136
-0.001550771264019377
-0.02496249316316875
-0.013792761760505878
-0.0063383703983388474
-0.017332470127113767
0.009493979121989654
0.0076568264320557656
0.004946396649268174
-0.01173610795616406
0.0007897290306969435
-0.011243153225486708
-0.00841879285445217
-0.022398637477812414
-0.033856486479462
-0.030980432245265515
-0.02475016442882136
-0.034703178865759084
