# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; PMCP
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=1, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.02028611331558524
0.0037147297594784527
-0.06072715768807904
-0.015383426760040701
-0.06931280784321842
-0.018883797103693693
0.014154112059533729
-0.0022702779920308926
0.03005250523230651
0.043519174201473956
-0.006652257571662767
-0.010891299762686404
-0.00041334665812543304
-0.010346871214514991
0.04452445985122652
0.013255401947152162
0.0270886801856733
0.008804837289793834
-0.03718302522286129
0.04552228406741808
-0.014787730174157705
-0.006144256088888276
-0.0068263449907904426
-0.08207715914843675
-0.05909952068730927
-0.02259194141609719
0.020442291159622865
0.0029989328264679593
0.009521116250926588
-0.009565600218597425
-0.050010932430665164
0.06694727024095694
0.012664862054087005
-0.015059554627245637
0.05840492066251198
0.036844526883340396
-0.0721690426674992
0.01508846929052694
0.022959079043596117
-0.05777550053937992
0.03843287156794414
0.042242071137234785
0.001102174971022071
0.026808147882803474
-0.0016718703923224527
-0.006246700198477836
-0.0655807249146409
-0.09526402309440712
-0.0014408420661023852
-0.01776653092487318
0.020765884983055864
0.15288114661458552
0.024502593437446096
0.024612165293837106
0.017538463035220164
-0.018171783234393557
0.016407875185184273
-0.017262964997558788
-0.02277352878079679
-0.016182332059216643
-0.019341198176348048
0.005708660229231515
0.059004566178175705
0.035307580712200345
0.020108911803773638
0.03304148976442922
-0.03692560019319178
-0.09951979660353835
-0.05026057091202338
-0.05262437605303871
-0.043073685898277744
0.021959336618889054
0.011763945581061178
-0.045131713480358694
0.14792586910855787
0.05856602588209744
-0.05255966176547343
-0.019543380812144497
-0.03283071594277977
-0.057868651006818275
0.06422146961384512
0.07080412270224543
0.027319844355823738
-0.008806636732449257
-0.04763371139043017
-0.05523251819363789
-0.027138697068405847
-0.02435126485085446
0.07140494860029192
0.0516866356993396
-0.019819429431712242
0.023646462823480093
-0.06819840210804187
-0.02862638617054646
0.02901029510643656
-0.01567273836732709
-0.005406194669182393
0.019862842126643812
-0.08784125125748671
-0.01742367355881827
