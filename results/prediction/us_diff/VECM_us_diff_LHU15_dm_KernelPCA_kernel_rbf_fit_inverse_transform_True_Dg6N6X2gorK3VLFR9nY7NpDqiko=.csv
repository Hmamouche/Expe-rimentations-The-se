# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; LHU15
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=3, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.055519997341837865
-0.07786103427002519
-0.08056236531081168
-0.028190766980536255
-0.026311397809117152
-0.020577017415351166
-0.022820626214233114
-0.005083003416661812
-0.0008839121512133549
-0.021946166873776173
-0.02034394516210662
-0.0018422126006684067
0.01926230538467041
-0.006034505626321401
-0.010891761821394006
-0.008814160068642957
-0.06003558085982487
-0.022853792734318458
-0.013621945975190035
-0.028841355986530412
-0.010033585145977365
-0.0348701066809729
-0.015656375969656404
0.006567054778668528
0.031729808501310394
0.014565350736907566
-0.009250271719441296
0.017839996639070425
0.05219157717226034
0.05950900163959562
0.06605417214693005
0.05296880212968622
-0.007381986486747262
0.058381225010820194
0.05594986712650286
0.060968188514543255
0.028397190237373367
0.012579844277980916
-0.04251005038871502
0.01383302933357157
-0.004786598454638039
-0.02576125225507938
-0.02084431859445684
-0.07461953374571863
-0.01652410081261023
-0.04515263781365463
-0.03701787699188732
0.017801027724173545
-0.022871860540726322
0.0302947236744379
-0.01585300674472403
0.0148979524837242
-0.036152531738234954
-0.02623936459158513
-0.016733888517613496
-0.0058754989420601415
-0.03458573391567546
-0.041281872308252185
-0.018129178145852295
-0.026156522088758138
0.024426738683233945
-0.010784812549659639
-0.0060243800738397826
-0.0281885842464131
-0.006473311955293527
-0.02928125256380351
-0.029919783672408
0.005925426478419092
0.016794653789068158
0.01435837373912543
0.05233022511928248
0.03526225352775016
0.07071222995129728
0.10030584293124686
0.05980686840083965
0.026285003574455915
0.011108507949566507
0.055212216994018876
0.022286825627212247
0.056977871220664694
0.004683458694560662
-0.03620390576760956
-0.04823114943029991
-0.025457280709985698
-0.013835691408417337
-0.011362861823408711
-0.012452739567718335
-0.028754142609336366
-0.040345527415484916
-0.011045290826913329
-0.025340722411371138
-0.021228011198222922
0.008384247367371077
-0.007041572517375556
0.009999392907326283
-0.001493236180092338
0.04493100573810942
0.020908638072418988
0.03652312462982572
0.07243007189254089
