# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; GDP276A
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=3, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.007009849922173484
0.005309913489155943
0.005802534503590216
0.007437986114254804
0.007829062360455769
0.005058755783991191
0.006948522244330697
0.00436347844930958
0.005426887056182508
0.005314670713276183
0.005362622951152151
0.006650158999713623
0.006430074450583973
0.006308442574319959
0.004109809332288139
0.004053859559994083
0.005132188767136288
0.005070474807669822
0.007971622799465469
0.007562085344352555
0.008556427727726329
0.007280811729060955
0.00716313028591007
0.007011133357198067
0.0064968685249646415
0.00649360556740604
0.0070142058209746805
0.00866628276857995
0.008540445528643649
0.007835742546627118
0.007912084300169814
0.006805883656531059
0.006766457872748827
0.006079245974695837
0.007632015394066722
0.00793363551700916
0.006531265775447398
0.006072459951405058
0.005602666806644035
0.006313994309068178
0.005715315926127833
0.005451649254606212
0.005111203908688289
0.004573391860033561
0.006080404369281483
0.005733536364246819
0.005704190876391713
0.005263671664471748
0.00572057034822072
0.005940296762210457
0.004920260967808
0.005789379013777639
0.006034082820538753
0.00579430992937578
0.0059444817031475035
0.005616004298760368
0.004803080820606416
0.004489227118483266
0.004529167922576738
0.004146027006563303
0.004177083477724628
0.00457079205439501
0.004550102199092649
0.004856429476063874
0.004524858264899263
0.005122937555440819
0.006948790861064669
0.005669863183751716
0.004934590753949414
0.004842553640169741
0.007850660235653948
0.007601553303351609
0.006491353397071332
0.0076170822385457785
0.006633419625858405
0.006263509577113339
0.005959829437252317
0.006059749037204875
0.007751846258230931
0.007318966722588862
0.007463310814243672
0.007357469310882247
0.007613014698238404
0.007878944241188278
0.008032258295695373
0.007547907336114478
0.00826894806902402
0.008147041277368237
0.008119636952579454
0.009991154404053347
0.008769756467008609
0.0089007701067564
0.00911699702311473
0.008617487372392292
0.00875226239447557
0.008660006730015275
0.008644170396921074
0.008626334976272295
0.008826431152684775
0.00991386980596263
