# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; GDP288A
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=2, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.00440175186579753
0.004477555885278945
0.007566247324648302
0.006994852041174943
0.00441783162758508
0.004630806215617881
0.005454452574418454
0.004825833032648514
0.0037428684162642777
0.004317970019488777
0.002971661732876778
0.002311964532033848
0.00406189605561255
0.006170172656917089
0.005852775196399452
0.005143908533036693
0.00541417442217389
0.0036997797181225756
0.003956752608907694
0.0053862174243620746
0.004609439590904216
0.003513476411258591
0.005375610527755316
0.005094062049576544
0.003364191662809812
0.0058549072142595774
0.00793308453678265
0.005472610644817109
0.006768020215897356
0.007510904839625207
0.00461535343841615
0.003290913391400782
0.004145364929685102
0.0020684031685044737
0.0028566599868786943
0.003980614199070189
0.003127069413911998
0.00325438884713456
0.004699446338487469
0.0033004807806070275
0.0030199523176900733
0.0034880465655150465
0.005765480495399909
0.003580475309347199
0.006338609101124067
0.004155724478858893
0.005276019261056334
0.004734346027807208
0.0037236921202984234
0.002978751905331111
0.005217411789684771
0.0021516943757586846
0.0038843417269294415
0.004601524753183121
0.00329281883335745
0.004042667883655863
0.0027697447402607244
0.0037958730493296913
0.0016301269045751903
0.0031625556019345465
0.00392081992471087
0.0028854391657122435
0.003992896929962644
0.00605683319687497
0.006528068879941389
0.008069962772026179
0.011038826335554015
0.007938000257005653
0.008512942046236812
0.007180032247444245
0.0055602206167692046
0.006230087673391432
0.004186946203079342
0.0024991475283826815
0.0030775287761735113
0.00526876807743668
0.005492349671347413
0.007353803271414404
0.011767062641446658
0.007632527489163261
0.008786808776085226
0.00826451105637191
0.005303368481895529
0.010104793283648167
0.010766863250535861
0.014638603162555622
0.013429006925865822
0.01319388478408716
0.01693433403134704
0.016093377281121583
0.01319972835478392
0.01532770258856234
0.009002797040345276
0.007333786453206118
0.012490926964039594
0.012322040869106349
0.012777707536305825
0.016864405347376668
0.015911888215822123
0.017894016796006704
