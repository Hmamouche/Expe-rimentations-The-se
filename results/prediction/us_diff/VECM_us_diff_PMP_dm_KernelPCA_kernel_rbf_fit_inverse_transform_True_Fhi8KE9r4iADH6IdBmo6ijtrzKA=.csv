# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; PMP
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=12, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.35281507301199616
0.09390986071837819
-0.01362588228739673
-0.482215438653657
-0.2188346922824856
-0.13417063019981484
-0.024386974046544437
0.257771178136332
0.050182530483761194
0.12563938655988693
0.19017602165070743
-0.03822656476149479
-0.07183239426189642
-0.02038543621450288
0.011097620648500004
0.025167463978944107
-0.05801407228042067
-0.1156782888029374
0.03200874770487092
-0.01783485084283587
0.1367891833745973
0.02183804723656973
-0.10926842507978049
-0.10569807435392603
-0.03564950095540272
0.022703421241532715
0.04234327762439483
-0.0015821506199889049
-0.06392706508214085
0.18692998622246448
0.030573384585471805
0.015359640929104083
-0.03437548666344277
0.04544075038642554
0.15762354311158816
0.06614150757445217
-0.002364789324500592
0.0163417409936652
-0.20942438213912598
-0.08421036706551069
0.042184652802662245
0.024452908108864375
0.09677535238939472
-0.11141560175612904
0.07406554133795172
-0.07175221470143829
-0.18900951726858806
0.1418216411198957
0.008276534507498498
-0.015948807315952163
-0.06004289932856278
-0.016217936243109396
-0.0524624812731261
0.030727145868030235
0.038332941448901654
0.057563942688599976
0.0484063566501714
-0.006050585914815977
0.0197302569473465
-0.09746504576074254
0.0168259763660421
-0.11163175902277385
0.07234337742492963
0.023259709034873528
0.09550703681208259
0.08557564566013776
-0.007663624065194486
-0.09860738257121021
-0.055019751881206606
-0.028242893541543482
0.044849430434338436
0.10339523042034607
-0.007293491333612456
0.06572867597390106
0.15118625937795355
0.0964380286043475
-0.05730360381948514
0.022511438944560952
-0.17385363823808184
0.14127858232987991
0.0060957743936326785
-0.06134009922211331
0.00028447006565237437
-0.07071112263859658
-0.01589889733816502
0.038140554825764166
-0.11635482155430676
-0.05968548451417952
-0.023789382526172433
-0.08307016007714868
0.1412478270608128
-0.034813343890218174
-0.06654035460257623
0.042514439420449336
-0.004217024863171572
-0.00829990186487434
0.091025890236264
-0.045357412881714305
-0.10788188535878881
0.02568565501639665
