# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; HHSNTN
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=15, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.3291575086285459
-0.25986128232145467
0.07782910269539102
-0.346385416277947
-0.03823338113851724
0.08300671065893951
-0.0754076016121999
-0.09983950744335701
0.19067357320036593
0.2500630498652959
0.008795927562883193
0.08329623167826415
0.1895886626617527
-0.08875523587392814
0.05599971079590908
-0.04544386431420617
0.03732633853633069
0.07509215918996422
-0.03112165720960336
0.20270368249482446
-0.15943722677993755
-0.01694442996950432
0.017192689225792923
0.051544433272727674
0.04882630566323587
-0.02831517810088339
0.11084740586932953
-0.037444888766863284
-0.04426822068128654
-0.029787760052543907
-0.03125276431879803
-0.2594710273208438
0.06330978976625479
-0.07242871254759964
0.17122140035996636
0.005298787184861255
-0.02335778915662612
0.09308391749292841
-0.14511960148147485
-0.06315344100263132
0.041609602678853386
-0.031644344711015404
0.0169190965776399
-0.008208942128271668
0.027130154185772345
0.09395230942700793
-0.08690155090059404
0.005431770155857846
0.164695886130675
-0.017617128589543972
-0.06295302974765941
0.011536103197496553
0.0020829799936364704
0.04855081446427599
0.09882797473131003
-0.06628055664498853
-0.03683639980483537
0.14025272428073096
0.092631229130946
-0.058575894558351584
-0.00898312443646067
0.021560042951115743
0.09202711980222936
-0.14292107825283626
0.048026953413971535
-0.0004269161568774757
-0.03751216067373405
0.11245033569330612
-0.02630778682910106
0.025664605329029783
-0.049555754449148254
0.08457653643386596
-0.025928779032943765
-0.10432985522302261
-0.12292065124678744
0.05166263181702181
0.07646539193661894
0.0517720408512862
-0.1528251496285387
-0.0867855336556439
-0.00357135757129963
-0.022174634415599685
-0.011169178234328878
0.04700634737306586
-0.02382238963849858
-0.047488609493624025
-0.02445423629406803
-0.1192205414432365
0.0016897498830886308
-0.014390645483687806
-0.05317838945618943
-0.027069022201098046
0.11449570360072929
-0.01703456237442212
-0.11537465753117189
0.14229624747899322
0.08605914756646595
-0.005175036164108995
-0.008322231050289758
-0.032689822426287714
