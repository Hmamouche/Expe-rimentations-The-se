# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; HSBR
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=2, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.051638485075238195
0.0038845823922100432
-0.004029127622839877
0.020321422751555823
-0.07159256932790663
0.0026901005534118276
0.08965397169104895
0.03926129748590166
0.027589102517138364
0.013412301856608709
0.03282110162121242
0.036770241196775015
-0.0007478128549205845
-0.034106873919523044
-0.045558693807813085
-0.031413087890984515
-0.08426329460122864
-0.041316611747210456
-0.06419549368075725
-0.06483375107203274
0.013134201067383961
-0.021787527391848333
0.0016993748625267757
0.006640821766947144
0.021546542097439062
0.04053258807411801
0.029021925732815458
-0.022969982855847903
-0.05254810930492119
-0.0360857368219895
0.005842640447683138
-0.026238737502817758
0.0026907897998947085
0.02052112671429059
0.005007208090970874
-0.006591212166606488
-0.014035266372611106
0.00040628220254175507
-0.01795697211635329
-0.01708933141720776
0.004889302781166037
0.05847599895329397
0.023220077063052902
-0.012945588594230485
0.007941157210173383
-0.04363003841040141
-0.034126766622545074
0.008987091116195937
0.041407566001760085
0.07499346355628538
0.04025206542655654
0.027692413264811717
0.014261935569788185
-0.033611232227745036
-0.010598744839769737
-0.0239769210036829
-0.011890649640170052
-0.0008333934369757039
0.04403658884190949
0.022888766625608063
0.04216616719404074
0.049853227968851915
0.03155581882649358
-0.0003311746791856904
-0.022201147459475417
-0.017921274426595385
-0.009398827348501296
-0.027793829372386444
-0.022656892404717816
0.023107561807338806
0.09104227201413193
0.07290981607828022
0.0437463851265238
0.06404510734572796
-0.024372202657027484
0.002373272587916732
-0.052326165792510135
0.03397457065879274
-0.00959932155095665
0.01849911604268055
0.04339585840556737
0.005764810608372418
-0.007130615277694833
0.011177350561798405
-0.001287826987628811
-0.004864658771428979
0.00729718105731029
0.03818173224605964
0.017982153402964083
0.009078134475272205
-0.002063746979503244
-0.04263689075321862
-0.07788702620877282
-0.07555843366584432
-0.0384959814435603
-0.051345660776182744
-0.03653902268409568
-0.0843151671822717
-0.051038315435631386
-0.03901056449863098
