# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; LHU14
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=1, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.07942983436410542
-0.0570664166893935
-0.036610919484264946
-0.036367635391066244
0.04979444228430604
-0.014087653335830955
-0.01082662208603886
0.0028669158154443312
-0.00577000250155245
-0.03381486710535047
0.0016682727178598889
0.013033041206770717
0.007404454427040314
0.009179015387135711
-0.02378846435187708
-0.014258626140679
-0.06908419547754985
-0.01616898534013836
0.0026049837111404893
-0.05969882124108057
0.0032556273791659197
-0.015693229363301604
-0.009076619284130075
0.01954569359486806
0.027697983174493387
0.005525782427809382
-0.00885656342128725
0.01052141925314424
0.01534579605980738
0.05837807247897644
0.08883744541679908
0.01934713471143748
-0.009672468557779085
0.03343287829531599
0.011925492141904602
0.02296608964378298
0.02561822075409699
-0.01763667102817123
-0.003672925386220334
0.006837705753188589
-0.02547264099002048
-0.04637975833148721
-0.03143489133469371
-0.041186232134847685
-0.023147839898866967
-0.03659884960832168
0.031429473936678234
0.03899872632810085
0.003268354648890507
0.008647573508528253
-0.010882272656157842
-0.0389675514792549
0.004205490967314297
-0.0320322007091726
-0.02243749695721045
-0.0008775234315394967
-0.021720532716613165
-0.0034246318198240533
-0.003732341638020392
-0.012948012031470113
0.009622209486213987
-0.017348281215834007
-0.027480512790714954
-0.019102228247444335
-0.02809077775666141
-0.04244891181504149
0.013281699885986396
0.006962271962396908
0.023083299305631633
0.025902563610360682
0.05761971204062029
0.01163940701903592
0.02219285103978995
0.05966326725780453
-0.021921430757480223
0.030607342965588404
0.04912732865446067
0.040165737814533614
0.03078294209302644
0.04048345212444045
-0.05854461319396249
-0.04614274833971235
-0.025495856427943775
-0.009417169668015184
0.0077711299062441196
-0.004952598674723087
-0.014634503052668744
0.014065706205877469
-0.01654610903043871
-0.028669170480951696
-0.011925466024645465
-0.009314904987656273
0.022789970570137305
0.009920142256104462
-0.010852486582270419
-0.0031527834742457444
0.025848046821078455
0.006097960896886596
0.05181874168896068
0.04005232990372191
