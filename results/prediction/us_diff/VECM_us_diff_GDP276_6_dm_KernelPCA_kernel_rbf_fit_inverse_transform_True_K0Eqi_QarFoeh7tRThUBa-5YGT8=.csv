# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; GDP276_6
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=6, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.008750440373666226
0.007300384547189348
0.007701475615366811
0.00794716548848046
0.006301251300742721
0.005059413964245186
0.005899524895096502
0.0051994440002529185
0.003977973719907918
0.004797886628608219
0.003610067641899029
0.0038762091089145786
0.0035773411267290478
0.004430886320817183
0.0027777003975322555
0.006291879695749349
0.006824594222720663
0.006620576112022344
0.00838302007971016
0.009335958476941297
0.010451194139538296
0.010890060233390463
0.011329915253852359
0.009650970572642048
0.010165459728648343
0.009888980488603696
0.010245149475926276
0.010099608583615416
0.010456372108050635
0.00922559263105002
0.007702955242975058
0.009380346756223084
0.008084721606644526
0.008302963001347398
0.0071808346928214205
0.008491925170610929
0.008250837078971101
0.008189094585203196
0.00891792696782123
0.006958396029782299
0.0073014294112745196
0.0060455909458107275
0.007744677722757214
0.00701954014441024
0.0072546601769792
0.007498361751609039
0.007401723650598912
0.005460612130216731
0.004899681425970911
0.0050828361606512044
0.0036220970801752843
0.005929138605054896
0.004784354885707327
0.005213681956724604
0.004626201381247814
0.00493693993156328
0.004473291343324151
0.003596949416938896
0.004511313952754297
0.005583177144292509
0.004865322743786831
0.004588471167140371
0.004319440506941769
0.0045539451229323555
0.005267753701613754
0.0059722331426684105
0.005035464939694259
0.006082819920026091
0.0065959676941311765
0.006560029998595183
0.009202854956084352
0.0073306590651934355
0.0076393421366911796
0.005103085749762538
0.004231708906164681
0.005033039615281738
0.005222078386923108
0.006770748412841357
0.008215872509377444
0.009778061625696718
0.010309584652259566
0.010927459951151434
0.010366912184356876
0.009426349965383597
0.008623552277372973
0.006715636934549207
0.009440457956114465
0.007608383993362409
0.008213508777264894
0.008274594222116762
0.005601776421096853
0.007607291061935907
0.0072058011329742015
0.008184042546347074
0.01190487894403152
0.008322027490382425
0.010207433125218265
0.004864653202776983
0.005841140349614325
0.005702282861487355
