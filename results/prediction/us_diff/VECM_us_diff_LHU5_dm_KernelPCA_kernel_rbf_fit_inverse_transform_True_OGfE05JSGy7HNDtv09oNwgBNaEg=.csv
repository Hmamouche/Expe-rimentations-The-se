# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; LHU5
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=7, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.008292617066848765
0.06418037632839924
-0.02490229529354525
0.007735410223780076
0.05928598686224633
-0.10943421037445701
0.004979357271703103
-0.019607581530747917
0.033462885811816265
0.0023026339426073213
-0.01646126818261228
-0.014796648586673244
0.016349792439862697
0.03945557790988474
-0.0035251784084367284
-0.01900113833309229
-0.04085113296407855
-0.006195587105612155
-0.03538054171604656
-0.019379038871200103
-0.039301109115560215
-0.052394341864959615
0.03635232155133371
0.01192437266262608
0.019655076923410546
-0.01305800083028142
0.008856267132998757
0.021781657442725467
-0.00025623410382985806
0.058046388592683705
0.0406381059013821
-0.03359020289128654
-0.037293156941687444
-0.0420743590990779
0.031902113143912905
0.05325164700206553
-0.012975269435991137
-0.012894135892229864
-0.0039028927327244747
-0.015122415401035537
-0.020595326507288715
0.008005239475197202
-0.08182104879725771
0.003859625434197843
-0.08116564227001195
-0.04066813266050838
-0.03273052181880204
-0.048341132671522076
0.021024742024225752
-0.006753964932892166
0.010535601688013171
-0.008175170922475907
-0.015726978358900537
-0.02564453468498915
-0.022703831229788612
0.017316704765527484
-0.04150486071213703
-0.0038257084030441126
0.0017634679188282473
0.014272900742259207
0.028608355813524864
0.00679781063751176
-0.007697443587692195
0.017005209855765313
-0.04019963503828636
0.013051701677226999
0.009425185852283958
0.012447049397479289
0.020337161603635894
-0.023280121417493152
0.01750773364859955
-0.012974361778799333
0.03402150368287751
0.032080691991974
-0.036138168752541146
0.06171226685774534
-0.006484215598853755
0.01355694559978959
0.038531385791927504
-0.038768420440647225
-0.03792864360912286
-0.02003207435765317
-0.056931861521277875
0.04217850532693501
0.005104642354251336
0.010405499877559662
-0.003299124795628121
0.01174327799928645
-0.022164888276573133
0.029838845417650146
-0.04534701468485215
0.05545945850185091
-0.0034033684021073088
0.002101809462598213
-0.044728401084044384
-0.01235881233635274
-0.011770741746720353
0.035461569202267555
0.02230901228829396
0.016509887328102015
