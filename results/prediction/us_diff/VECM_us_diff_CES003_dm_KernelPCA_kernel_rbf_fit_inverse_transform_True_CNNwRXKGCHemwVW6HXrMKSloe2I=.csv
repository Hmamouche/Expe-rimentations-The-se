# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; CES003
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=8, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.04034283664670805
0.06809554845919662
0.035843953033233365
-0.014562151429440999
-0.0051819484526000655
-0.0182107488971602
0.03798328078023197
-0.00015496108568254993
0.009568778524695763
-0.013857800544081145
0.014640445163846971
-0.01892553039803719
-0.03295347603929891
-0.015095016352120209
0.011616095588731602
0.03006750924433529
0.029407434262808695
0.009333053294442614
0.014168798716811936
0.020726487275090407
0.03262526333739198
0.04160768018787573
-0.020010778079474604
-0.02493622387325208
-0.032946215260763474
0.004243504008549466
0.005958926945682933
-0.020788206592974163
-0.03569355109620552
-0.09073430619977127
-0.07451505678201019
-0.01775040614793194
0.030270435481079917
-0.01831813108927422
-0.029481050565815736
-0.019034503874867395
-0.03622649326486842
0.02001018899946123
0.020661576454404265
-0.012345265555806216
0.023704369274997132
0.025931804874751378
0.030199147833404053
0.02452237496068216
0.021647526011336297
0.02650033355914664
0.005723811012065906
-0.0030992060267401077
-0.0026079296614237267
-0.022323560583608043
0.005407179483884721
0.034713127914702914
0.01452736176588569
0.036822314401011386
0.03185826531468276
0.006834492050256394
0.03075731163967067
0.01287121129969369
0.013435682151479972
0.016589196427381284
-0.015969753811054935
0.005257086310582694
0.026244081113111474
-0.005249032570815232
0.03653132410484273
0.010737902072458071
-0.001173167574681739
-0.020475726077209787
-0.0033236865591353124
-0.012662806299271292
-0.02753813935084346
-0.04842838513791477
-0.08423328391545198
-0.0827159991744494
0.022467598979000237
-0.03407024756403561
-0.021703119902431234
-0.041934821274509454
-0.05103399748570254
-0.020449657259386935
-0.005055512793494464
0.007740537141060877
0.0044967611347327295
0.008735678280124053
-0.007024154077730993
-0.004066360104640972
-0.003524136016279971
0.009272376848649174
0.026249966474091262
0.009637114668440038
0.04238751057549631
-0.001726459932262075
-0.021184676272067705
-0.007479793961339328
0.019069172416356986
-0.01543545881640799
-0.030239347276718655
-0.04593612319656071
-0.04087943759274501
-0.03444121450726302
