# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; UTL11
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=9, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.12220596330486223
0.05998939861732855
0.0801555480499165
-0.05532611640741211
-0.03960874613249986
-0.0416293637998178
0.07853390653698299
0.018170962225451316
0.020338054875439528
-0.017496662684147672
0.058163701631889914
-0.03724997742429674
-0.05187434665797151
-0.0014975099479062454
0.03466369060911069
0.0877476584904964
0.048368536680634705
0.016356980806631716
0.027737617029720697
0.03500509742969053
0.03152231784684177
0.09680966666252393
-0.05206198937313693
-0.04886253279968521
-0.07054355325197814
-0.0005661313106016825
0.0034267675059140718
-0.013394841442019603
-0.022675939249558415
-0.16015606971882698
-0.04661398558825612
0.027191133182695903
0.0360479975528369
0.019406673479408863
0.01578132280963833
0.04477704711565303
-0.05484125338292591
0.04328727190198817
0.006143890274901122
-0.051448728002439326
0.03916601499800175
0.06087200604727785
0.06265634349931613
0.04768611043824779
0.00936829950232436
0.0031940565934352053
-0.02799245594347758
-0.06856636871522291
-0.048058173309439935
-0.0451570493161935
-0.008510568691453769
0.07601662284119205
0.06731033997214161
0.05115262021811369
0.02616708533438325
-0.010804187551935275
0.04840069834061177
-0.010736918287109772
-0.0064611502348963425
-0.008022627134770113
-0.07357658197906009
-0.01282686195713484
0.03735911624213628
-0.018520152790808972
0.04445009753444683
0.025931142740924687
-0.03930865694054373
-0.07113112235312065
-0.05976544512589649
-0.06299043620780254
-0.09467586280283967
-0.03622397564431233
-0.0907085383581253
-0.10041816268693152
0.09693801776207854
0.014405593252796304
-0.0015584958297545776
-0.04508135156126798
-0.04558068705334048
0.04392593494880763
0.05566823733840914
0.04691526447551296
0.038164690059949574
0.022613723724405774
-0.011105892848979488
-0.0215795306774011
-0.016851097874795384
0.0036607777728327653
0.03861765879698967
0.028165350070826885
0.06847517533949983
-0.028829592548990368
-0.048525677222877595
0.0004029368389738171
0.04557463024722228
-0.026598919811039012
0.011329128958430232
-0.06193348360296451
-0.05887173362217184
-0.010145840114743827
