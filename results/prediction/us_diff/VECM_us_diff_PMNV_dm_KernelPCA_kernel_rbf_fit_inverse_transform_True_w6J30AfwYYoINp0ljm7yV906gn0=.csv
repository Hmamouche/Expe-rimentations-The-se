# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; PMNV
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=14, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.39554416346615273
0.34755701083950047
-0.15168591802566572
-0.3853419289843641
-0.10434381956975385
-0.06986960663788579
0.05366028017123506
0.24531733255014282
-0.1342582462133459
-0.09221667190157684
0.32835024955411096
0.10230452297884837
0.10336667804155322
-0.22152897149048678
0.07345317788647299
-0.00980296017477738
0.09139191556020948
0.09428539726863869
-0.19562387308655005
0.06296517438263557
-0.20563440122559085
0.17095026545023626
-0.11190988722910913
-0.09053822708479856
0.0878637837419264
-0.19486001685988077
-0.0008810424197326916
0.15204157297978094
-0.04939061384504687
-0.0646899140785607
0.18593811056466908
-0.06175597045199406
0.02736772439500368
0.10842172007118353
0.11742865574788296
-0.0745058820355464
-0.02684371030361718
0.021077044175780983
0.07230200567999086
0.013201421870417854
0.10507961373299454
-0.05476111011264196
0.06681349373779219
0.047075760841071894
-0.027993990181201522
-0.0804206853653607
-0.10039274088908072
-0.06860487755947223
-0.08006501888546019
0.010432035194285073
-0.010752492876312386
0.04537886917100672
0.1626737909848581
7.157912297013104e-05
0.08334857938298117
-0.015222743123555066
-0.03976711823427143
-0.07055762035186597
0.017426201239810035
0.012756763497867285
0.008909985582394295
-0.10627533318360288
0.18845628786073765
-0.07526935698729847
0.22204443774147042
0.10248287872743886
-0.1288467007698694
-0.03670045390320297
-0.05729407181133675
0.018133717914591306
-0.0021755562673020784
-0.016514936456916943
-0.02869966595844803
0.05639275556572114
0.1887678615669391
0.0586162067575869
0.016432716918573034
-0.03068322822176925
0.015148249579176732
-0.06071921034554867
-0.008491059458776394
0.09518802456862488
0.10092373835518278
-0.0641293040734513
-0.04772820794469724
-0.17256363114804776
-0.020773291524247867
0.017710018299988275
0.06320857116759314
0.04855205240998322
0.05670922664581744
-0.10786459999216395
-0.12197696876981501
-0.004106650232353959
-0.04325940503522499
-0.06355573183303109
-0.03533925387306366
-0.07593553171735666
-0.12873235043177633
0.11799062414983072
