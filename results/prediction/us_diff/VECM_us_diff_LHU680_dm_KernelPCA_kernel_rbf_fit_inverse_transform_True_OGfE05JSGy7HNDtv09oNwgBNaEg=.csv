# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; LHU680
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=7, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.011066353814886003
-0.12738068800031782
-0.07312986132909177
-0.06395694842948525
-0.037993832274766826
0.01774855273031093
-0.007612433781814032
-0.0080944368693255
0.002412224805763568
0.006913961629707222
-0.03479796100806652
-0.004793212607331671
0.008153711458266237
-0.014369300843750946
-0.0122250856968132
-0.016268384007094835
-0.02454086643014173
-0.046422583361460705
-0.020310325364283732
-0.03495040920917847
-0.025807700563438788
-0.023297498880928774
-0.029385312960841424
-0.019671535579515925
0.0010195440841492369
0.0006794018192178373
0.003603006504330262
0.015192090383498981
0.06971824622619746
0.0293115903916112
0.07811048077769725
0.07642451245698541
-0.01853623394866249
0.01561454312894662
0.05854204362814314
0.05044610823535902
0.056434409774261424
0.05437874097837087
-0.01311258156904057
-0.01283091835636143
-0.008964300205731258
0.006568639513325619
0.027432359405397018
-0.010571182472612043
0.004917291394806481
-0.015513351740607284
-0.06897814066566703
0.02567134932314965
-0.05358440502000705
-0.0021650545919416724
0.01851800662138081
0.032195760032046224
-0.02986953937141568
-0.030094638416106087
-0.02440340095603203
-0.04223390958103751
0.013909451083880885
-0.03919320952674408
-0.021667527372654375
-0.021130572168871733
0.0008184789423218881
0.0012053980887972242
-0.03209570709428517
0.009941980116108088
-0.05803459498531338
-0.016161875108683537
-0.05858301639696721
-0.02310203835173986
0.00603369008332109
-0.002415049350357096
0.06106484375092804
0.04591089487079716
0.08463433551126898
0.07245384377892403
0.038616939078788826
0.05260058082103236
0.0015189214028108779
0.0636228439039883
0.009099048796005971
0.07387351608338777
0.051018010666674024
0.003045645854768922
0.018041911709888243
-0.0017402677716160738
-0.05961399352585418
-0.010576728353230497
-0.001609036030496296
-0.015499689925583246
-0.025198254870296137
-0.025416098362153222
-0.046919551912157874
-0.02549568108332728
-0.019629723896466698
-0.029025172556297614
-0.0016992478777518262
0.008392499705099239
0.03365426195885569
0.029562936228235477
0.010029724965088279
0.0315270635477727
