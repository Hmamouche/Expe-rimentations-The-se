# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; CES151
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=2, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.07390257246163984
0.01902411040102309
-0.05591387368007276
-0.03244686355302968
-0.023168481996437704
-0.019998004450026363
0.05344666390177551
-0.01642687934808019
-0.002868025335522599
0.07822067943222474
0.07925333034785073
0.045252105493762865
-0.046103202356838714
-0.03489069745178312
-0.01994019816850207
0.06760056824496055
0.1018188634860603
-0.02075805313488898
-0.07000432760956199
-0.034979925917395745
0.018297714881955557
0.03286705933309912
-0.01904702867775665
-0.06497740819756054
-0.060871272544999355
0.029612258314476714
0.09218700846073641
0.05567282858757369
-0.03850693532920217
-0.091767399660891
-0.043251541554384695
0.04502714337080941
0.08113574316269961
-0.01697729771685014
0.013149159732629138
0.056588711324959774
0.021032641485501113
0.043417771171271366
0.016524179578499627
-0.0484937977205361
-0.001973245518782179
0.1033220416623843
0.10460319511524142
0.02378600339628837
-0.037501707897808156
0.0041074434193560105
0.003763023306090338
-0.02652460737251256
-0.05422451039969957
-0.0419732439290246
0.022976733310790184
-0.006030499470177534
-0.031063720060911827
0.027624528957372194
0.040718122835364995
0.05385740683455457
0.030733074703056196
0.011123816112405748
-0.0325097366200387
-0.009397402054583767
-0.05594176916048922
-0.054276190752361106
0.054239840255826836
0.03688784492755562
-0.010425940817417899
-0.010250453326128582
-0.05596913615412277
-0.032519054247636146
-0.030556773110784016
-0.03611058516817696
-0.04336245937207621
-0.041810504663690656
-0.021137690422193926
-0.053166378463854275
0.047188871073381294
0.037282926014247986
-0.053377932440082626
-0.04005404536183009
-0.06632763151400595
-0.006101027032870413
0.026976585698456484
0.05086700647927226
0.001724224987766366
-0.0007970507439963841
-0.037070076771091295
-0.035831225319135795
-0.004562633544434954
-0.06913808996145589
0.005496906988571486
0.01357894932564691
0.0041690024997995175
0.03775663003381267
0.03364694869007898
0.016304895881256373
0.08581112207737118
0.08212583791625477
-0.008758824746160264
-0.026693429741995442
-0.016878624279586824
0.07113300037114206
