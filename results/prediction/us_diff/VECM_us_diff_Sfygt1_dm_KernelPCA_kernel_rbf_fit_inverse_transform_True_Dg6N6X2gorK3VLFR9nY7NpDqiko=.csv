# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; Sfygt1
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=3, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.19177806353323162
-0.13260286405428906
0.005482829666464788
0.08811064721531947
0.006964225394795756
0.029289744666496087
0.052464328418430115
-0.053090067674524705
-0.11969161205959071
0.027349031742381187
-0.015507501870747967
-0.06542285343800412
-0.049038449298891645
0.01885026849442781
-0.062332289611862934
0.07139464023153178
0.005059079481287218
0.07696570228520311
0.10238384031966476
0.0016890685732465
-0.005446700036661149
-0.03784540999995788
-0.022269369621151405
-0.024174553505490734
-0.04703872070157442
-0.06782946860040752
-0.07177026923602176
-0.018806691508490632
0.03871727659610136
0.06661101627395022
0.03981020645284229
-0.055422572120223126
-0.07651697621510589
0.04217207160335628
0.04224965167596713
-0.05683747842531226
0.0035892599668896144
0.02092694770812773
0.0023762605741246
0.03471987850295865
0.02128687003602093
-0.0012406951537285126
-0.025621539901036376
0.06453696260313835
0.03812283676585519
0.07016636105693098
0.08244183705140053
-0.04687687733016043
-0.03820456801467646
-0.13176714386158556
-0.10207406605366164
0.009332032923221035
0.04796846739979239
-0.010408112067066128
0.06966700108494507
0.039216054394021
-0.0836437184016495
0.019386363592266358
-0.010891342812300744
-0.05648770798148567
-0.02141058381850223
-0.03159372624122611
0.00174092819994732
0.004859710494556925
0.038352597302278246
0.019556923639763937
0.020513990122723046
0.010318585120852567
-0.03251188968614427
-0.02374880549078713
-0.06393589289835036
-0.03470759342134021
-0.0346706612903353
0.04174962221292668
0.034425825491566565
0.08322159468596094
0.009944334004367606
0.06491478819421996
-0.09487943802478493
0.008016355236168041
-0.04602967356950741
-0.01730542274151519
-0.012538228548015295
0.07632884220359853
-0.002901801413303859
-0.02640860822878873
0.031719816204654666
0.018191172070430132
-0.018744705009053453
0.015451552487634138
0.018258211297011415
-0.039590092313540604
0.012862814680119433
-0.013951908396738395
-0.06348349824020819
-0.004086834839031014
0.017448620784980905
0.003588771998047753
0.008504129351448508
0.06164329917198011
