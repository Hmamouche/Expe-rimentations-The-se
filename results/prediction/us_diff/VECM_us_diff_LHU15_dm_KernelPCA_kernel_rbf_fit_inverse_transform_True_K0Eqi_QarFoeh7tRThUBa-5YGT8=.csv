# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; LHU15
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=6, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.034338305834705
-0.10339355477212653
-0.11265913508947095
-0.01790800396355881
-0.03671537198882849
-0.022129131681373947
-0.026508741363570995
-0.0004866374632788058
0.005813035226497296
-0.005413690156984971
-0.015948779974470636
-0.01823066463603737
0.027566691563637682
0.0004217925446313251
-0.007956313202251625
-0.008327102612451202
-0.06457851486166485
-0.02844486950937694
-0.018580575447895372
-0.03745279707074192
-0.025203675978753115
-0.061444160116993544
-0.018702286506481793
-0.002624337043051939
0.02344418701908902
0.02101548999699284
-0.0005102262082524256
0.019824060165362502
0.07325734098931179
0.08944361320693145
0.05432886450696394
0.040026528452154465
-0.042906483227336006
0.06070749591198016
0.05938255748293286
0.06649678812477047
0.030092048603139465
0.016811553310612504
-0.040669967626725395
0.013829037897038095
0.0061501705388775476
-0.020180042670385402
-0.03294064991364499
-0.07636655332384693
-0.004860091528623827
-0.045332234038651775
-0.040409274661223134
0.010625547807626555
-0.030588996984779075
0.030205691682668512
-0.018909682915610255
0.02966076981595292
-0.0415379577804062
-0.03570410143280688
-0.023203996739548337
-0.013290047094562489
-0.03773372783206788
-0.04547369709035971
-0.01844265691749286
-0.028220310580848597
0.02654225681048424
-0.002377285950549375
0.00031986839744593386
-0.02819379851810318
-0.008228367149872708
-0.0359026306457326
-0.035820945590241036
-0.003071042504928652
0.012237378886112895
0.01941345051427673
0.06537398410995413
0.04251199502164031
0.07223995856595242
0.08510516066277019
0.05106165128463498
0.030288090010518305
0.0015577681374140135
0.06061005470472185
0.027818820144235763
0.050079396009754584
0.012022922982459546
-0.02696842896023923
-0.049087963365859556
-0.01638857810267514
-0.026746937435810823
-0.017119218044710623
-0.011819229163535946
-0.02833305821438246
-0.03239122043008182
-0.0046933866161332945
-0.02954472190890133
-0.022463490918672835
0.0025085187355834253
-0.035800840119772415
0.004618973960481884
-0.008197418858043707
0.0465382321624328
0.025609169889460975
0.04434325067603484
0.08197689819793663
