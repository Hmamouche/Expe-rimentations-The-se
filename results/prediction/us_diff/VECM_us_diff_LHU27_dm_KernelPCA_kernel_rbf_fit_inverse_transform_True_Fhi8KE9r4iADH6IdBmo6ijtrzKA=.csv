# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; LHU27
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=12, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.0202914548158692
-0.15417039493908083
-0.11253516044430906
0.07025548343309679
-0.010425527010338644
-0.05733796825888696
-0.02233938809140463
-0.04090011432409603
0.0250910991653558
-0.05817760258236891
-0.06784835091173856
0.015471672416692199
0.03457677469636585
-0.015453503237703026
-0.021164792179464976
-0.038969556301316606
-0.07980911587394551
-0.0422483126746972
1.7251854923881066e-06
0.023321063884844014
-0.00921713586274311
-0.06555026792432034
0.018934477462708085
-0.011458811141007613
0.014082726277527692
0.019703165059309496
-0.00238167391714454
-0.006957722637280088
0.034570349462982555
0.05035110522233112
0.08360334321037055
0.06285285116554301
-0.005257351014803988
0.029173162615504865
0.10146980748423341
0.07532005963608863
0.03294876707267725
0.010951752683253583
-0.057308392863518715
-0.0160808691159076
-0.019888494489724406
0.03524945435103917
-0.027644119414546107
-0.04468425724853147
-0.047156920033363595
-0.026653963736557724
-0.08742653397320954
-0.0028125372158409444
-0.0036875372668341423
0.02143094466707741
0.02427982715070707
0.04697818527287916
-0.05078656602257098
-0.05022636407766855
-0.03889259138340665
-0.01662451996478031
-0.0020785634675454835
-0.03378220681032369
-0.015835465210889025
-0.03630739190818724
0.02850538313694089
0.010297650121668963
-0.015797089489375866
-0.025571064750434983
-0.033888626344107484
-0.035964129932104405
-0.009363249389120483
-0.018503469587390135
0.03878414446470659
0.0048940979720245945
0.04935475845225119
0.032757104664068636
0.07582578895685263
0.11031583666198827
0.057125184628368735
0.0332001869241879
-0.035680586141460566
0.00814172656121924
0.04151171392971828
0.072902366800774
0.03491664449971598
-0.02956964649932171
0.0006039978752335039
-0.006349667345480413
-0.06117211349561165
0.0030145720779473917
0.0015259253418250414
-0.036462267196058905
-0.030353142876396257
0.0008626698039720761
-0.03879992562049259
-0.023319297871729053
0.017948462727239232
-0.0463834194310943
-0.027451393799746517
0.009888689291214661
0.050890971870020105
0.02547284670045488
0.04637043936018109
0.022604900201593704
