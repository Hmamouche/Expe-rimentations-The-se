# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; GDP276_6
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=8, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.009491295824200358
0.006723270294873937
0.00806727783586452
0.008407182653594537
0.0060382812975055015
0.005380833551541228
0.004824184759362234
0.005104274836133675
0.0037132360978368766
0.004359845559622035
0.003291201594489532
0.00461119854508407
0.00355788090474683
0.004037509771008271
0.0022651791490098
0.006247633659775386
0.007045306841613908
0.006887345511343783
0.007819896638413721
0.009255595548227908
0.010152796949467545
0.01116600676572705
0.011622031492365415
0.009590770626308491
0.010224387905024883
0.009618572104599023
0.010249633530170346
0.010565588668446872
0.009793201699697501
0.009518038881888851
0.008396894703487312
0.009226189208249492
0.008074596317624123
0.008011046524748627
0.007006593928984787
0.00852004378171779
0.008699573293427475
0.008013385398746518
0.008977254766594526
0.006925215344880881
0.007412956327327648
0.006084472471749886
0.007901356258776993
0.007048366157525497
0.00653770245513797
0.007935018773506346
0.007589070658518688
0.005170651270962358
0.0052695053493832
0.005290222696823251
0.003661747902829449
0.006123668678885689
0.004661304443017482
0.004972510096055028
0.004249001194332599
0.005177105980985322
0.004096436818088816
0.003910766240726082
0.0043638894054801956
0.0055765445526912505
0.005205946303230247
0.004650244965953265
0.004364258028850161
0.004487219803143871
0.0049221047546123015
0.005961309614189343
0.004863064336832158
0.006539501325802876
0.006190755306278258
0.0066895228869395034
0.009256645824526706
0.007467117875011859
0.007855761765169958
0.004665244204688297
0.004313354928476991
0.0050700962439336385
0.005058250683500006
0.006736528231262142
0.008331881023370676
0.010292604272947986
0.009581656521869597
0.011172456324378062
0.010645507738989566
0.009058088348409061
0.008669024550291428
0.006743663762117781
0.009997990263115888
0.0076099205564720655
0.00796679424389813
0.008516364500918446
0.005596568093001956
0.007485931879107927
0.007371969362213748
0.008359791507450446
0.01148088580914377
0.008387318325326614
0.01041931170145429
0.004733838138709107
0.006186843224718314
0.005463063880047238
