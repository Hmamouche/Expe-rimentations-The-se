# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; HSFR
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=7, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.09139615039997442
0.0010667281358983927
0.033517077827576076
-0.08870513714384481
-0.018031995323531162
0.07740924877287374
0.007338192522309765
0.02077893231737152
0.019760596197797985
-0.060035988202991566
0.07517882665788117
0.07061822780794866
-0.0656025013162757
-0.013987097821843908
-0.04761974506637635
-0.09604952007699448
-0.01612450185946683
-0.02330132146536302
-0.00476053220425697
-0.11285465971049116
0.03323293282786477
0.04783654614986066
-0.0538236503238534
0.005269279990286386
-0.022303283226099396
0.06568390766491947
-0.029845879126024974
-0.04867448491726519
-0.07048716922497061
-0.07255079651063326
0.04397343226825562
0.04900887415275347
0.1006809345665033
0.011331716846794224
-0.06423898585299095
-0.03194178137332812
-0.02008652467864169
0.009361454288309937
0.07777969565708642
-0.04874105967700219
0.10418791308585629
0.032151422746791475
-0.04090315918556394
-0.059866886031164024
0.02621509492558029
-0.1198552474852924
0.02753568405925267
-0.021289884673261615
0.04292970716086685
0.07296441960883038
0.012346762380421959
0.02382322018906715
0.025094109567773627
-0.04479964591362758
0.010574819816631088
-0.020962773418419192
0.025182719233722874
-0.02318809526036015
0.03406611624030066
-0.027614438469736706
0.037519386818159854
0.08075534682749513
0.00797215706078036
0.013941284395433691
-0.029916227511085056
-0.021024015494662332
-0.08421286324099608
-0.005753123199992749
0.01780115083461924
0.015865405726337864
0.05457569967257403
0.002054941062362965
0.06543029666138354
0.02186078926406791
0.06819612274856024
0.02493576228528959
-0.0937241040608351
0.0011009822070112626
-0.05755024608644169
0.06608627118623565
0.032795165982200884
0.011474825239555616
0.020124493691780584
-0.07637358990519928
-0.017232830824482164
-0.004430605434521315
-0.0032809217532441207
0.04614544104580371
0.022286712472304432
-0.040979980017930136
0.003933237246479758
-0.035759335950482654
-0.04526276359950798
0.007819888403038416
-0.010383582387637771
-0.05782483698127609
-0.06019620156414156
-0.17499269810493168
-0.011075877829974316
-0.020009262049549625
