# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; LHU27
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=11, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.0018941098973750542
-0.1887621503953202
-0.11665414297274147
0.07549900517639078
-0.00940900926488179
-0.058439073500197045
-0.03286778744720599
-0.03430570804449083
0.03799685016606782
-0.021377976304128125
-0.03715640992345458
0.023640668609044534
0.003089575488859967
-0.025884475540301256
-0.008647110464957884
-0.015106704739682616
-0.05314260859190977
-0.033251649816255
-0.03490534052988789
0.012715409182327119
-0.022404065395800633
-0.0539120082575809
0.004293952522782236
-0.0068756965019688004
0.009398283620243144
0.012590143195844533
0.006198965689662536
0.01220421815078572
0.055285023707623705
0.03774699906981576
0.07712624252178171
0.03239173448797368
0.004576928531118619
0.032872036248308936
0.11056229658018181
0.07895275444077846
0.03304001931647909
-0.0061558644171752185
-0.06691688705051037
-0.02270397231312473
-0.014167109662000799
0.04452139228629774
-0.028707070052772628
-0.03907629225731711
-0.05585286964197763
-0.02403842424259337
-0.07736700936765258
0.0066299881170717966
0.0015582275991311988
0.020366145675745313
0.015372010539165577
0.045095583070553905
-0.05517459525261982
-0.046646897620225704
-0.037877961304995725
-0.017564347100818993
-0.001966576006623068
-0.03956430760782923
-0.012470073708103255
-0.03227062612287012
0.023294208845724633
0.015037230793365991
-0.024088141058433904
-0.020699353311200777
-0.026308347374613166
-0.038713063887483384
-0.012752265631775913
-0.02138039260972441
0.03814429090648115
-0.007507794592137412
0.053238494767205914
0.026672590163871947
0.08163297211480336
0.1110315199652392
0.06094542845184196
0.028683889059861523
-0.03279710551606916
0.008261201191807942
0.04643272539508778
0.07726786397587374
0.052669644870105656
-0.028097497270463352
-0.006747873850901415
-0.013022288951774163
-0.07383895986619
0.004846957826327641
0.009919007242770015
-0.03388657398804156
-0.03586148133357445
-0.016295389416019792
-0.03494861738035217
-0.021401719105767228
0.021402246282544536
-0.04503256725590714
-0.029055370555886608
0.008897435842614473
0.0530267258212434
0.035054886463203414
0.0347301468388113
0.025248261968304396
