# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FYGM6
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=13, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.05208377448301535
0.06985273294260261
0.1098407612890623
-0.04083973581361046
-0.07514557708426067
-0.11999894482931109
0.009630588607931631
-0.02546963970954467
0.0760645876603695
0.141131187499342
-0.09937747930308032
-0.14713817554544883
-0.09846064518514266
0.007052246969391648
0.0372460074226026
0.08506407923409533
0.024218934629293653
-0.12324905741593192
0.08111404495896667
0.010473375315219122
-0.019433439842189967
0.12167517067573522
0.023536346189903773
-0.06423532916575887
-0.03840413350710296
0.04859545639721695
0.03361247901313293
-0.01247229627111617
-0.007816666717362536
0.05049804928184558
-0.183252395165802
-0.10846608624985186
0.0484030082653056
0.00033807110452767353
0.011673812228514518
-0.02070101224874645
-0.11938084404135077
-0.05988317667746975
0.038768770336057364
-0.04220854099021187
0.09234865166936679
-0.006584681027827942
0.002658105500300177
0.04108255002482281
0.003899499777586697
0.06959798861421358
0.019762066201442774
0.02372049358083902
-0.007688488227105109
0.004838244502884538
-0.05790228308136765
0.015405249353205537
0.023765907814192022
0.0033434764297086365
0.012027858801108071
0.006778924646895753
0.004445555423613575
-0.00013196446799565095
-0.019955449585143847
-0.016316656704049727
-0.02625616198747566
-0.05863309052344544
0.06116132240306965
-0.020383218845800903
0.023014429285300203
0.08376517742209338
-0.02444419485021896
-0.045283412458896194
-0.027868846104497726
0.014756627467330682
-0.03312181234752556
-0.007079936465017141
-0.07270711241895104
-0.11453347132740127
0.08451692412573548
-0.02289829174251632
-0.024992460639063775
-0.049610250074773383
-0.061347660021445025
0.05192173367331981
0.03920982590969449
0.051159904122791484
-0.008159239164339683
-0.036128068962835214
-0.005584068688819645
0.012489315268685346
0.023141351038827078
-0.008444152040755586
0.026950345694626694
0.02071529364637372
0.04174626110593029
0.03304613196295546
0.020781223138531126
-0.01339926956694357
0.03137226833042969
0.02045029123810271
-0.008991942036384211
-0.04582372515035456
-0.09025190479936909
0.0002649266479901116
