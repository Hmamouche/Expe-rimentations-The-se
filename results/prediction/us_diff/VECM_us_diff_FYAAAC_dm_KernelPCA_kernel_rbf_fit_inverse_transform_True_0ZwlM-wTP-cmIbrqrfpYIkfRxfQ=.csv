# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FYAAAC
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=13, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.014993428036893297
0.04762141228925448
0.0900971233006979
-0.08307451025162246
0.027354609181779736
-0.03201580250451742
-0.021319146502938896
-0.020055227800253636
-0.04286049215903557
0.028372158412168617
-0.11432593730778486
-0.08843504852703926
-0.07542680901944696
-0.0024487344533444325
-0.013280198898329994
0.0786495179507265
0.05365464389379875
-0.016645135636348386
0.018393515821343617
-0.013268555577061011
-0.02864053495120226
0.05573604941796535
0.005653685873938066
-0.09552724057792832
-0.03385335389464188
0.008143025915189889
-0.012218976452728403
0.03637075070473514
-0.000647015760971187
0.025243743473699093
-0.05712222183334379
-0.07524696560184078
0.05414393616419928
-0.025760839613536528
-0.021693744088593668
0.003026801772097378
-0.07250748808181642
0.01770383233057387
0.01857173596353167
-0.030705409437801055
0.014558234082973413
-0.01607357074658095
0.02799674785657021
0.04448587346023914
0.013822676454738302
0.07311519027678631
-0.03640559878553275
-0.02627311793235675
-0.0704576975995262
-0.031224976971334317
-0.03745010013586296
0.05130627600765383
0.04606149332359184
0.021299038446034826
-0.013487738831482708
-0.008993863596503152
0.016429769688046457
-0.05676363449037606
-0.03293430115848725
-0.021605072464498262
-0.03280499087629045
-0.049672422510931374
0.07577161993123419
0.02575889942481325
0.03995504880983907
0.07704206199371971
-0.01694039832971679
-0.04887578500194654
-0.00938109444062663
-0.0367161596834548
-0.026951619900337627
0.015110490454380378
-0.021091776411526545
-0.018268125205102112
0.03325888272973049
0.029364240963901632
-0.08116114228337439
0.0026674520989120737
-0.06708981532434939
0.02463045873487062
-0.0023104493427421706
0.015151674456493208
0.023702962008368838
-0.05805951821562585
-0.04623348921631311
0.009983856135133428
-0.04011169213473878
-0.004508364490798538
-0.002392151855675942
0.03965794075998486
0.012121992931258457
0.04094655868354041
-0.03004453996065342
-0.0356491906593677
-0.0013114093071109646
-0.03061069799380385
0.034318271592401445
-0.02628826650546851
-0.013148140636718297
-0.011213381594844985
