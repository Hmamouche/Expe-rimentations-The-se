# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; HSSOU
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=5, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.0608571319022735
-0.03873662652799448
-0.01893137901589402
-0.06618982876136284
-0.014821874645381705
0.03417413495424454
-0.005216488510127834
0.04508038297824797
-0.030900044023272608
0.008701896186979028
0.04787691161642112
0.0326638188838174
-0.09452061903507421
-0.05594044300425996
-0.06919612316426699
-0.1573715064876691
0.042690506285057786
-0.0030590686607094323
-0.0031872004751679877
-0.11550588808506204
0.015217766039418604
-0.026585486736708477
-0.03842816324318314
0.01876841750293692
0.02317873450274332
0.026668141634136316
-0.0017965389000492646
-0.03233432375716905
-0.004597916203056284
-0.001889465776018102
0.042453296535319734
0.028839829978438383
-0.01595297856600256
-0.03795849512292352
0.02919707617325813
-0.013968021378827935
-0.0002855708600264465
-0.02249162389371088
0.013703460008920329
-0.03099580706068582
0.02628619564969753
0.0710708479641495
-0.018353418929088157
-0.016170249975861892
0.03785018724028581
-0.05918155775284204
0.0033332749376411668
0.016142445774132957
0.03652456346842988
0.058472357716527756
0.03643862946370213
-0.013977210196948064
0.03308435807616451
-0.011639855354205909
-0.036941516509334624
-0.0009694823781557435
0.018585840884284686
-0.010312105910385288
0.016525820761024823
-0.013019962090764337
0.0624049636599358
0.08011974759567278
0.023505812015928704
0.004989120966293604
-0.035161061173661995
-0.04848117563603903
-0.01368489766287874
-0.0361173491607449
0.01060241928174389
0.007361044941701876
0.07969042094306898
0.024408688417030797
0.07270639516209115
0.03611112962566268
0.022112178467431416
0.02599550953045487
-0.0662421914623816
-0.010425103584603686
-0.027469742876170856
-0.004118288884931346
0.034153479600703154
0.03354017611435089
0.00868235937787546
-0.05322658310175965
0.04120527005296719
-0.014392246087267281
0.00502940034629733
0.05032189242951282
0.03609579741718529
-0.0003300053698961844
0.05027497475903724
-0.01321582804784064
-0.06001372995924553
-0.007862320817530709
-0.04170556743725669
-0.07231652267764427
-0.048655844804564266
-0.1008247430032221
-0.01969894641027972
-0.07871437614099376
