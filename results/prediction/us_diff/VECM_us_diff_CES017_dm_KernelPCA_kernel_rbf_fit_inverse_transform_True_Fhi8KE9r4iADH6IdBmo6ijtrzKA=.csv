# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; CES017
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=12, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.024514854415904543
0.11798664573475721
-0.010596917335206877
-0.021286366075495097
-0.05050919907069729
-0.008982624797502697
-0.0081986011200069
0.01170277263368997
0.010250981554155048
-0.008909931029783722
0.00032197641635013543
-0.035819358620955824
-0.027669539785521318
-0.03917194205850193
0.010540982778701476
0.03343103983709931
0.005945662225223665
-0.0038318325388941206
0.02852639699125376
0.041309460123585884
0.01970259762921113
0.04052749103366607
-0.020414592614612412
-0.016681636104688834
-0.0538155765156284
-0.0020767756762813394
-0.01934275299355269
-0.014363372013446465
-0.03899035626028558
-0.060177190093572926
-0.08896048512902409
-0.0022075480412599505
0.06405565199300169
-0.02060143632654579
0.005137201570906977
-0.024673351657814528
-0.022937697302294616
0.022541593547998964
-0.004741303799655164
-0.03215258351232031
0.03778308842150937
0.00027908222262107256
0.040964278996594515
0.013156480507238371
0.02557559476554006
0.030930745225659945
-0.00959859408920964
-0.0036963744165654485
-0.014098490987983305
-0.03513103990463837
-0.0005145604759515792
0.033697243284334454
0.024171201960881347
0.04533981464417551
0.007961276755597758
0.004803066834287411
0.03244631728982703
0.01820188480614482
0.010365021141047662
-0.001587002859529829
-0.024512305348916078
-0.012023549155181753
0.013718648733207927
-0.013294509993918693
0.02928661991069682
0.016061504161865137
-0.013727418671156552
-0.0008377039400826293
-0.023028237341772572
0.0004521111152936902
-0.0400309449352266
-0.05129000153983849
-0.08873427361540379
-0.09060366860610078
-0.004926711598115245
-0.019657022288769147
-0.0013298969376782585
-0.05414693347451329
-0.07395222218881937
-0.00014527879686536318
-0.02754995250181844
-0.00047479395327779117
0.008152886723606895
-0.0037629251772206534
-0.004125198364898013
-0.002931624964876776
-0.015285778522046036
-0.006800026382705893
0.011305834572708041
0.005341462826977211
0.013619292577786537
0.0011432300197740481
-0.037330867828196744
-0.008026489202570736
0.010409398835430753
-0.024030790482592653
-0.014872563559544551
-0.04535927560941515
-0.021451949974003976
-0.03256676807952853
