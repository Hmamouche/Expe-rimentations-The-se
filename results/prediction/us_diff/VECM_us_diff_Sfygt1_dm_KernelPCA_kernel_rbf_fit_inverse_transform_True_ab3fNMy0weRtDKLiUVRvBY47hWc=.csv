# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; Sfygt1
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=9, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.1300678283803621
0.25827668152885075
0.07483270961866956
0.041145627012125374
-0.06756777677639333
0.1712732399089631
-0.10700468297500496
-0.04810276492874623
-0.18124524028273542
-0.10888887459603298
-0.03618935588452229
-0.04186913404047303
-0.07502297034105171
0.1379619752821301
-0.07103038094650421
0.09368891736264975
0.16099158997696056
0.08775891906625899
0.09946194544517058
0.019664571461495424
-0.11258494201951555
0.2052124714160845
0.07296743012774967
-0.031526168476664035
-0.1286851500488752
-0.11133634604663958
-0.0651246641054628
0.07013495423859144
-0.034894030183210675
-0.01965460807073458
0.007117650333458372
-0.056065078330172526
0.11694966291718041
0.10609310759926979
-0.14088031401918427
-0.06044787551972388
-0.0071094421411650194
0.06289521824635413
0.09697789240717652
-0.0952671757341983
-0.020413408847822098
-0.018184881836142246
0.13252170170716757
0.1404804941628496
-0.0017508536498999144
0.15917248085974392
-0.005912012982863896
-0.06121053949563096
-0.089867261097834
-0.12535923399003143
-0.08291423550135668
0.05754389396393646
0.13915491013478912
-0.01871749256103796
0.08207782328460322
-0.01415423654855347
-0.05847259873125843
0.0592865914378851
-0.05693527552226094
-0.04082522935361119
-0.03617491615849467
-0.08765907553084619
0.07053950658489282
0.05982349546054778
-0.00782502850953528
0.0800307854035833
-0.02620409951761242
-0.021599850192019543
-0.05792855766391341
0.0021989640867474264
-0.15700010188578467
-0.0169535894403443
0.06868385685212192
0.025960053711506757
0.02801833256922654
0.08720515457160768
-0.16441013272703223
0.0640509369795308
-0.16645832794760929
0.07513118166503623
0.004529271307859982
0.08939468314903457
0.06995594968051955
-0.0661886730715367
-0.09676257154869394
-0.04464192689343927
0.07122728245138973
0.02988742520688307
0.020610640768385566
0.036905400751494226
0.024277722214122516
-0.058342420263712974
-0.027605254948395073
0.04816813119677401
-0.08467735662634068
0.014769606666178187
0.003756610289209622
-0.031371953320911125
-0.10647309235998643
-0.01260704182584052
