# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FYBAAC
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=4, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.04798161122135921
-0.0025772995862628784
0.04289254344377755
0.08165039809284741
0.008806799362759866
-0.06840660235032357
0.002808850047327765
-0.06342120980354655
-0.0431611154523481
-0.05616147091431632
-0.0702998115671581
-0.037362541420930484
-0.03597966637761688
-0.027033944913937614
-0.035468619878180764
0.02417110476880606
0.030623758936775256
0.056810418389842916
0.02593526821937982
0.0029752151022516115
0.0006145794157921937
-0.017196027158895223
0.010034647748602004
-0.036878323229498064
-0.03240410235849082
-0.014817286822254316
-0.006781816568977499
0.010444717180709877
-0.009863764326210198
-0.003573820911337517
-0.02472685219417046
-0.010532615592918303
0.006637779741433098
-0.015573213603893675
-0.016109990508472293
-0.010797944312176677
-0.02089795842516415
-0.0011136137237452775
-0.031539377602934834
-0.0232840045333601
-0.044103955627602026
-0.019849253136929593
0.02335724468510661
0.05264169526824855
0.029680559007702976
0.04939845532713744
-0.005854025742671275
-0.028530530929457634
-0.023539606598978746
-0.0332971145341708
0.0013774547973451675
0.001584124173559161
0.009362079855932995
0.021573298537659654
-0.0002693460402677485
0.011713295874066342
0.014592809622425165
-0.007188014039031464
-0.012603634380069773
-0.019155518968821304
-0.016743791518265074
-0.01689874566779648
0.01728537507042824
0.012488824921601723
0.027039253660420128
0.02153582777658394
0.025421831020769767
0.012583861819478756
-0.014499048701634286
-0.013888720093466207
-0.03951372845343175
0.002410132249983891
-0.01634138764045498
-0.01850106838652189
0.005254980965002841
-0.017324313009246674
-0.016677152343910862
-0.025393942673012974
-0.04396151504477838
-0.02015083404233444
0.00526863101958874
0.007735297280463856
0.023101569549460858
0.011174821571307107
-0.03718490426220098
0.001844529197702536
-0.025006094298210796
-0.024369089654472313
-0.004141573818086292
0.013945000324981605
0.01839457007803428
0.027264587796532225
0.008853216480052266
-0.012996505168340455
0.01905258935651985
0.009141748174544398
0.016550037525070824
-0.005360157897238763
-0.0108685513966737
-0.0186526973286239
