# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FYGT5
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=9, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.14380347398599885
0.09550383093566153
0.11177303242379737
-0.020589013143215623
0.00011105729800358656
-0.0605140474922975
-0.05483364548312011
-0.05396903034150624
-0.04382427181494559
-0.07695004448414385
-0.07559706589085693
-0.03701298978986828
-0.06500152481817761
-0.0038255716280935353
-0.04322952620342646
-0.002532082228848801
0.08103770286108708
0.046538016236755415
0.06617998783679876
-0.031392977149222574
0.004503510173202496
0.06702583644951461
-0.017985476451665365
-0.034733792541411554
-0.04568500225148803
-0.001494787807215233
0.010988575865855985
0.006196026780855176
-0.04415200369185757
0.02420398480291121
-0.048747174513804344
-0.05199531552493923
0.06335355913241318
-0.003949841535577127
-0.008263931198515085
-0.03758370690067883
-0.06502654507633857
0.014670505369000393
-0.003665103089246481
-0.01602660059911596
0.041375789184536355
-0.020238105023786087
0.01963646564428931
0.014871477764354104
0.05189022591178255
0.1305284156407171
-0.035632976022217974
-0.014678834612010486
-0.03739906866827363
-0.06010644882232398
-0.030083458350765563
0.04190828447330236
0.0455635642178387
-0.01281623309067432
0.018578539276243408
0.017280662653807073
0.008215459558481472
-0.020171267632810762
-0.03645616961310652
-0.02260900679221003
-0.023992292641308734
-0.07007659289601605
0.07667458637943791
-0.024561075221611953
0.030726016529644698
0.05283806157643001
-0.016773246852718166
-0.03754456745592802
-0.016627955613614825
-0.02078612139798204
-0.05412921460159325
-0.008454921421436874
-0.039670655506272356
-0.07957239332051448
0.04133474484342026
-0.011225514291354497
-0.0612029020482546
0.011478119716918228
-0.04347927744964093
-0.015578340173960946
-0.016557073397099567
0.04349310834003555
0.06070929158471046
0.006962971527149113
-0.027213718328528584
0.013887034649556013
-0.010575890751685888
-0.01016939493896399
0.025524703801775756
0.028786362241048913
0.034250881984436383
-0.00032149577763448955
-0.0011855136773615826
-0.02251931339650266
-0.00392160924989628
0.024204284503184844
0.008167556848230513
-0.07707505517714903
-0.07670805765520738
0.02979639212672888
