# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FM2
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=8, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.011780425577812503
0.004235174329640814
0.007786056800559731
0.003273654101398041
0.005716858690633848
0.005475957516166101
0.006776133290610174
0.00841485601571363
0.005571000128811487
0.006307602267628672
0.006035581754133386
0.007957098194665965
0.006597417933602169
0.005847422803143529
0.004532091286304785
0.0050081784494425885
0.006165885275975445
0.0039149636009844216
0.006484786268053727
0.00518374320116811
0.004087290144670464
0.005145492196995466
0.0038529837168672534
0.005498930426340175
0.006740169092296855
0.005667593223173778
0.0037736863023858534
0.0032969018404347484
0.00638043333694909
0.0033864012117128034
0.006996591573217305
0.004465084046928047
0.004080820812606163
0.004525631338398702
0.003953980346891334
0.0014697808814846448
0.0012564556694442217
0.0014610407378118854
0.0005946597007745339
0.0014939849986483778
0.0006091970923276847
0.002996698996882188
0.00043333514259938843
0.0008300916222558066
0.0014521173085065599
-0.0009646423470489971
0.0022544902256885686
0.003947249383662265
0.005232182017408046
0.00407411452636483
0.004916902915664475
0.0034609058305580206
0.006310309607407012
0.005800067255971386
0.0046958497497212506
0.0060972502252902815
0.00781640278251408
0.006655699072546065
0.009657116700763363
0.008273915669152558
0.00978325658480912
0.012982515569397819
0.00945662252511005
0.00887810386368295
0.0077831197913419
0.00892032683658815
0.008720850286935533
0.010892074545435594
0.008474411979580623
0.008870011674578572
0.015486288686895056
0.012963907529633825
0.014046163006899953
0.013933673265856886
0.012537932461758324
0.009187542547036503
0.01483826955156186
0.013647255701278865
0.012226604830519683
0.01371813293559722
0.012745478044427518
0.005593862863770921
0.0128479112669673
0.007856010668972188
0.008152909032817086
0.013140801623450818
0.003630530717591951
0.01242620192742936
0.007984439090695983
0.010836032003302534
0.011040585109000719
0.006204166208607085
0.01248573761058038
0.011642807737602573
0.010545569317546924
0.014384746224205894
0.012761151147653232
0.014347119481759648
0.019601630344648076
0.011141926993369175
