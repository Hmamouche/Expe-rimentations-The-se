# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FYBAAC
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=13, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.04632108032348603
0.06241056851115036
0.02727727555961801
-0.03306611874318681
0.002987380489041902
-0.026665210694192587
0.013439722506521846
-0.03318874497068431
-0.07007883231565896
0.0013155347014042838
-0.12395267185646025
-0.07253874566670987
-0.04340369744907514
-0.013538087386401828
0.008788777909795955
0.08155374725446005
0.03666778511279937
-0.0015288628691491161
0.002498054257656781
0.024278540747822437
-0.03498799841561177
0.05453558460405479
0.014880905144148068
-0.059535409711120035
-0.0548634310269163
0.005742347521907135
-0.016245973743160523
0.027855886847939264
0.02277736492806775
0.0019644876417590947
-0.02763691321168765
-0.060123896869480414
0.030821300353571845
-0.005843604020085345
-0.030011795320000492
-0.005992412072110218
-0.09000148109914434
0.002190412248309438
0.011884365419594818
-0.03373150832225905
-0.020914390412761767
0.004088049165383319
0.0022684468617604966
0.056924515436267434
0.00031340895902449983
0.061083921728363896
-0.007061351877792839
-0.042626559547555655
-0.05238437479434099
-0.03074382775422528
-0.03226743613869906
0.05198485947699283
0.027329307508107678
0.034700647885919464
-0.013977064745404952
-0.004243413621351087
-0.0020724544347390916
-0.03358007347379016
-0.03823264091553481
-0.015427514051402594
-0.041191279812905615
-0.02319973117213224
0.04875811296160733
0.025371799495277535
0.04237493154710568
0.06306080245496873
-0.007466565594796765
-0.026497795216925888
-0.02980439079833136
-0.025504721706236296
-0.026561193266035524
0.025108004603741498
0.006695355590728072
-0.026463439569501715
0.030731458442544546
-0.003156111623761248
-0.0834998628246183
0.017941081940910337
-0.047246596781504266
0.03391829014237123
0.007593109680329229
-0.009320866786819284
0.03290377753696572
-0.012830413337821612
-0.05609608256437812
0.01842106032091543
-0.03776717009073915
-0.006348165188378788
0.003399773936870602
0.029570654539754014
0.004024280734534468
0.037857060674551746
-0.01918108451749778
-0.04139497740591085
0.022345986798192
-0.038450038480632526
0.015535958500027232
-0.0044396206370753664
-0.019806462189462155
0.011188270638581164
