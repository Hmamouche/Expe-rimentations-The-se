# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; LHU27
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=2, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.10118209496427419
-0.12024812517299285
-0.07486993316608803
-0.0459814670084369
-0.030032887422565484
-0.024567224311659145
-0.011811455717218216
-0.005415110578007403
-0.010442684120724771
-0.014451674199329998
-0.03414092762835005
0.006558573842720855
0.012161376072345267
-0.002823310490674408
-0.032113784150217414
-0.02667673476994549
-0.037722414276386125
-0.040580098712610535
-0.012488254930591466
-0.01453982149590992
-0.01793839154911436
-0.024364650469867327
-0.02036542139884702
0.005307376119045322
0.014727342526785923
0.022904307526266668
-0.0025780218592821825
0.001400691126908653
0.032817802562935004
0.06193113550670341
0.0569580139079368
0.04265280151633451
0.020555343295039972
0.052506058612196996
0.09842772153792437
0.07899064951306116
0.041788572553385334
0.008252327409421799
-0.03941307205559326
-0.028178690165195763
0.03621546056487895
0.00466166596149283
-0.047177960081097536
-0.06474760956765055
-0.048153115474635175
-0.0371886239372923
-0.04834888158803577
0.0158930223012863
-0.004081238355280034
0.0005974369730336413
0.0028335223217942247
0.02970647762747604
-0.03112264520595158
-0.047387048275773516
-0.023226482301205272
-0.01095386222936972
-0.01703470873887451
-0.03936875622035486
-0.015865415253599895
-0.018935184027585282
0.010274160333215225
0.010330181054620191
-0.028202568243484747
-0.02675924927235878
-0.008037911560215983
-0.021389853441075923
-0.03285844952764116
-0.003654859587248713
0.025267437644546396
0.0058911782759725724
0.04108178244102173
0.03372805684261183
0.05525131542596833
0.09515387188686115
0.06555294532401053
0.047073326574199235
0.02776263484491398
0.049341966886133175
0.03556666076211902
0.05498805414960524
0.011020965807169377
-0.029413804118175915
-0.03493373688001251
-0.023181869776913736
-0.038395209842905155
-0.01083471538790102
-0.004934110199830085
-0.03130240109081446
-0.04273845403965664
-0.016587962609938327
-0.036666869920831124
-0.010121031444049406
0.010780498209706555
-0.03126439130402135
-0.00346534533436859
0.015788101040105335
0.03384290313769361
0.030488990850759307
0.026844090055770944
0.04546516673042124
