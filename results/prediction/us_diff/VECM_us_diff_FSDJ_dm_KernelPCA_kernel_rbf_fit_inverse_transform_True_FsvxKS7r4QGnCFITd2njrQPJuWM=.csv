# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FSDJ
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=2, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.002325162719691732
-0.0027964509550458226
-0.003818240101547177
-0.006088446187023821
0.0011995565286020706
0.0020487380135986665
0.0018740344929434021
0.004390733478823603
0.0029035198050587534
0.006396062935467729
0.010145487284677545
0.00954858993663179
0.0019271993414467956
0.008846782256489933
0.01743565597317155
0.0033989005690033347
0.010117957746376494
0.013828385851322662
-0.0030968550474136033
0.027909744509210417
-0.002807828284043516
-0.021903150292057475
0.0038241225457538385
0.008395035878219064
0.011060848154015441
0.010330472569732147
0.007166145884327978
0.011133649003398293
0.006023901580793233
-0.0019465890196492099
0.01083185429341557
0.005985807892724532
-0.0007026457479055506
0.003743528373182139
0.00963924266549094
0.005800507848210913
0.004168253141288058
0.005694946981066913
0.006244907750527929
0.004012809544044171
0.002934196189874603
0.006769106811874035
0.0048567156962027146
-0.00400159591209902
0.002015108217026171
0.001488311248712076
-6.64615303973934e-05
0.014574842411806328
0.017733149866151442
0.01882989772334815
0.029212527661073576
0.021319249880606843
0.015334188605088579
0.02447081187318595
0.02579730352243535
0.025392242258176
0.04571120426288821
0.030912286292333082
0.03453176569779966
0.025548559769167412
0.044268673823151014
0.029584568418975003
-0.005539625431537513
0.036044852349306114
0.006055406628010495
0.04479266613181713
0.04616751035695695
0.02849193064007802
0.010249112909706391
-0.006768511503097272
0.004746145296961217
0.0008881700923401587
-0.01838686702756769
-0.00918454035063334
-0.007554010994115897
-0.013926045925324967
-0.038433789504853766
0.010752054580654313
-0.045259926831579186
-0.022597144059095898
-0.037783966949828086
0.02091915542391785
0.033814516980831974
0.017182438836794506
0.022862054298370967
0.019709715456206103
0.017124893108225785
-0.0051739958107877935
0.012507386374753706
0.0004096879283719732
0.01665140344414746
0.003652623902552581
0.016258829170490284
0.042523987142575576
0.024035841607635645
0.04603003647111806
0.027404768133522783
0.03431601305773073
-0.01862095917787191
0.015019642333977649
