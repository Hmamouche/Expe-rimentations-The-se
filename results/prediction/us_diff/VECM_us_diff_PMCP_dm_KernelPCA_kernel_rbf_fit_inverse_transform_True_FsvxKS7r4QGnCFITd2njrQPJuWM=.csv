# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; PMCP
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=2, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.14630114334432154
0.07740722263649982
-0.0423705614658612
-0.009486803553841015
-0.13297813989469212
-0.06738656817785986
-0.02864895327057638
-0.06107479617471675
-0.03722455305515317
0.029231093092724838
-0.007605084115876144
0.0014124139126016587
0.004190236597204978
0.008172024030239573
0.09941926388861543
0.08164381331428341
0.08857527804757323
0.05623408035789279
-0.013877019457749547
0.07900158621851636
0.015387931954586478
-0.006800599598260175
-0.04509848985438791
-0.1378885177230925
-0.1103813992673096
-0.06729403050592722
-0.015040305771696977
-0.040471322436966425
-0.0029835356202307546
0.04254254179665117
0.012061930347533547
0.1448738966684665
0.03541336595826261
-0.041394928242431586
-0.025449034382555906
0.032196463877153636
-0.05771584816805542
0.027504916281768745
0.010518264025327139
-0.07104201690084874
0.0331486624897018
0.047414043300663664
0.07483000179129917
0.07525422938532998
0.056677251136608583
0.06131566525509863
-0.008034635767103473
-0.09728955365846367
-0.030142105462652773
-0.10183909130728888
-0.09627577103854103
0.07149344049590434
-0.06521189696154955
0.0027147672783935026
0.032805161665118744
-0.014697580933194627
0.03514252787832493
-0.01829123790780963
-0.04676866505792044
-0.07267503465752519
-0.06437923858982479
-0.046733790147210116
0.02987353546109047
0.07777256907194832
0.09183043516346434
0.12731600055877967
0.06802965761495479
-0.02559147236600242
-0.041073976552979855
-0.06670322344758635
-0.03523626826643096
0.00780116184812541
-0.019460050500104065
-0.08413477790997856
0.14004282453235695
0.0690906930180563
-0.04630047729794229
0.03080586712643801
0.02146607275161739
-0.002193264756785617
0.083275307707781
0.07891928021735894
0.08862986521513141
0.009532157295451538
-0.04840560785543983
-0.03057080249310594
-0.023737909738957324
-0.09742276312380796
0.007849548203869277
0.036359085013218904
-0.06560419881041316
0.04400293588813695
-0.06350594501191281
-0.08132407798543234
0.004714847406316874
0.05275631049534041
-0.013810020251121186
0.02217779531266538
0.023802969453129295
0.09172622919957386
