# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; GDP261
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=4, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.012849875893393575
-0.003981231942478403
0.03737330895094641
0.0023615524333639028
-0.01597095897799157
0.01543261410141486
0.043589913422811016
0.032211564305403784
-0.0037758536085178577
0.005505175467243132
0.025321735316666292
0.024204603033728758
-0.0005216786266184876
0.00314930804650792
0.017925380491455187
-0.0266840366795346
-0.004970798366259198
-0.018689615735629814
0.016126361214662627
0.004202510009576163
0.01116552097764873
0.009527176106908092
-0.007663737485137645
-0.01112693038949778
-0.005672543485707173
0.005382109593021591
-0.003926558936306186
-0.031572957585031265
-0.04319899652143743
-0.039698590571913085
-0.01483651361198428
0.0025560050075639964
0.017294434384115823
0.01371031682561702
0.027564678233999093
0.0016083348292033912
0.009600955343449766
0.01303684874149152
0.00826210530697909
-0.0015979101060888314
0.015189874095627593
0.037631790024803984
0.01777238430094908
0.006696114862747925
0.019641573394028812
-0.002525465581040352
-0.011589961219214594
7.958831173781288e-05
0.011462801911572984
0.012098081093961122
0.011807214697984441
0.006239751669611192
0.01457518571189214
0.013283723153906891
0.004032436139882427
0.007018543960784485
0.01612970720597615
0.008199313421877606
0.020958732394462255
0.0002396395179045164
0.014041107333554494
0.030435867593126473
0.02444889169527715
-0.0022649309717878794
0.0007971162013511183
0.008404894544739666
0.00814861501995594
0.00016489873428136067
-0.0013274395651314212
0.00030222743918444615
0.0129473007980416
0.013583701259649061
0.004516489907885583
-0.000764936646894166
0.0174880771418858
0.007252410038254202
-0.0022752990624877074
0.019970823502356207
-0.0019462251624746008
0.01967553838110487
0.03604358878787992
0.0491488492045487
0.021557042871807208
0.007088479882525048
0.019067947800126907
0.020762666146692876
0.025978837577576915
0.019968091654039635
0.023858196005590842
0.004869953447281982
0.021893798822411967
-0.03906215447794519
-0.05162445241095981
-0.05153223517546002
-0.05103509307932169
-0.037445121349294445
-0.05274021714710705
-0.07052000359649148
-0.06119856631479426
-0.052957769254788414
