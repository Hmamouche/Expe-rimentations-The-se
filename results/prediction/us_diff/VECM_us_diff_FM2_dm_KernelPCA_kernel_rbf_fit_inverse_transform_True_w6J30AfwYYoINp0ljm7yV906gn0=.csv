# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FM2
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=14, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.011828381046818024
0.01022739911163938
0.0031472110469315108
0.003995841498437759
0.007962801163641276
0.005818120448070167
0.006662417981531593
0.005534753533547191
0.006522216788738822
0.0063430506013623005
0.007131110849019073
0.007709541841517053
0.0069117516156744436
0.005790795392332069
0.0031395695210241353
0.004485657751323317
0.006376452753319455
0.004357875821051381
0.0059433424393938285
0.003386590614017123
0.005631000440017107
0.006412014793752223
0.0035735951565094284
0.006047099991406483
0.007103728889591211
0.004115465362647015
0.003702554018887585
0.003985937615594154
0.0038482593422366866
0.004247610250624709
0.007729371097279215
0.005388703069298669
0.00473792744635408
0.003955360013642354
0.004272803778595737
0.001383843672036054
0.00373615296391544
0.0012903587970329966
-0.0012126148142762112
0.0021782220782829626
0.0002490793032655836
0.0015756414748504892
0.0018845465815891733
-6.336003224150676e-05
0.001198709075116979
0.0003661330700034638
0.002023414999157398
0.00403439517033796
0.005999983250547628
0.0038341772491994374
0.0050065961813880655
0.0018660890242216811
0.006133151357132374
0.005295734341430995
0.0041839276050476765
0.0068970514086347644
0.007673819527308249
0.007314441375756088
0.009897190223981195
0.00785805654302725
0.010320432128208731
0.01229049138514634
0.008849648287091475
0.008506736475022918
0.007891513843382216
0.00818788749999965
0.008019759739610038
0.013125892084700806
0.007653297946446643
0.008790738642263691
0.014898470876814604
0.011885359655434396
0.014630866812503725
0.012593812935204361
0.011719467012792636
0.010700964685450625
0.014694308052835316
0.014026673975088041
0.012580982977614969
0.013607840674821348
0.011255411993038356
0.004170370538876921
0.014156144596544747
0.009785916695958859
0.00985869569676208
0.012063183138013849
0.0026293811395437495
0.01273836271667595
0.0086974051814756
0.011814111358731465
0.01012299621422586
0.005073115923468114
0.011598437756299308
0.012724781431609725
0.009619656770057878
0.013840713417316013
0.013122469536901991
0.013980080459507108
0.020341763047339537
0.011726174391048637
