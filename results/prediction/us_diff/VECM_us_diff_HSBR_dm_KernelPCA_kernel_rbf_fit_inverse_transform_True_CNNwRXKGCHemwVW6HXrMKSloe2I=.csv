# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; HSBR
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=8, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.06791761203811998
-0.05326092379820439
-0.0327906893408714
-0.07112322134327353
-0.006638652177994388
0.027025166013802518
0.06748112216468512
0.07139731899417512
0.01206669231603368
-0.014611770956052525
0.055289742798647505
0.045747454309813126
0.006522293423680934
-0.013528741734808641
-0.06527381455304866
-0.047907978290842354
-0.05443231636493807
-0.09174583477737538
-0.02173529265587619
-0.08204541648301171
0.049526903171755114
0.012546397282605749
-0.037549798034755236
-0.01654728162017449
0.025176085854392138
0.06847349245916699
0.019686756214155103
-0.06805663255175826
-0.0891332191396698
-0.09579139788740353
-0.021124732672308032
0.06827841306027657
0.11402785757979667
-0.004494593204501164
-0.05820629561060464
0.0031802817379507596
-0.007312940658813008
0.01847101695259406
0.00015648416196961754
-0.05063614108435867
-0.04458684355907603
0.06149847788531473
0.05244809765142
-0.05009267560522384
0.03973760646428087
-0.08417517592010101
-0.029351942071174558
0.0403722961582946
0.04611270026463209
0.07027469459985676
0.04292252310199441
0.017525967191494927
0.013890933514945296
0.021701253046946097
0.011040599198019946
-0.029860459086046454
0.024570487831747194
-0.017253457897546738
0.03608037316355611
0.015416912914070562
0.04434926700205264
0.037542788887072416
0.05486394441794259
-0.015313401762911894
-0.002539199774018581
-0.05489151010937136
-0.03719758042905152
-0.0260844708557081
0.013180044702384631
0.023232845646643546
0.026936726692054817
0.0428508729338201
0.036359689322626745
0.06433791398128647
0.03163558565372432
0.002045950139465128
-0.08108212072335573
0.037398357896104184
-0.04813544018836656
0.0354131749524787
0.02491330994246874
0.05863904870207546
0.013200599838878741
-0.03964182219117069
0.01653357547631123
-0.011243347299490291
-0.02710805575582675
0.03579901031513409
0.008261807760765471
0.01730597722940859
0.026893131580430166
-0.041950280991186
-0.04968487523376578
-0.049408179015419315
-0.032686960732203764
-0.03988107683067853
-0.06715708231655927
-0.12949930919913372
-0.0206512811774599
-0.10464559556361955
