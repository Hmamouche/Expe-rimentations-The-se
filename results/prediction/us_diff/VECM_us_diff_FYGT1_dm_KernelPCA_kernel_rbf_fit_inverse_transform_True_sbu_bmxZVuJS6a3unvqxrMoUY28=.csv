# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FYGT1
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=1, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.0313212994264862
-0.07672097542043738
0.03154748956512618
0.009818399917205886
-0.03782809458607154
-0.007996508455936935
0.032286784796031996
-0.03204167687402472
-0.04202703109155437
-0.0707488617605582
-0.04305962447368415
-0.03918569916076124
-0.04108368641472163
-0.045713681199953
-0.025748159817357872
-0.012337724916881464
0.011579016865371933
0.019374866377747296
0.0067922135037334925
0.04029077020738778
-0.014614240487549003
0.03152930839262035
0.017429915070400624
0.005481811453644889
0.011799523059218893
0.0030791310560592845
0.0041704244547383276
-0.0197599335337924
-0.03175721279360506
-0.039455428058478
-0.05149197692369521
-0.0018295374940969743
-0.014517528948085428
-0.01978667353418931
-0.01544668915786497
-0.06260836940054394
-0.05335155331283192
-0.020930475399591836
-0.05156338126940661
-0.012308756990894613
-0.00972303137703236
0.006310773014205994
0.028913946289708004
0.05363709822446147
0.009101150404678394
0.07620014834410523
0.013607246361282643
0.02065348996436408
0.010869652518446744
-0.024748368768031243
-0.0044622021480083505
0.006148874229134226
-0.01226937655599239
0.029830643053226084
0.004859113595647239
0.010020990367834853
0.003089081779378445
0.008315770950821423
-0.01363691011333256
-0.01741605220542829
-0.03800412124717635
-0.016704735284642797
0.01690757774221804
-0.01030812026939403
0.013934137801044678
0.013222791683751064
0.022086362380561116
0.003082020906232029
0.0022360138736476007
-0.016407933974763846
-0.037390126810805814
-0.018479426729455337
-0.05659015933488201
-0.07357758589674873
-0.004183966772162682
-0.050325502835556854
-0.006886207130242976
-0.041997720702205435
-0.048677863629546556
-0.012704463744055072
0.005186671820089221
0.02529017564879807
0.02212834689909149
0.020686264277621286
-0.026329816389569986
0.0024723451452861455
0.02027828414420091
0.01638872742886332
0.049594877446886496
0.03466343869593223
0.049765740395858
0.023247442206731885
0.00498910976852837
0.002065696919273278
0.012231267000991908
0.014352763633195967
-0.014474367254388582
-0.021479474554208892
-0.06594448682966804
-0.03258581729300813
