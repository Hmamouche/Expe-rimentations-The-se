# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; GDP276_7
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=15, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.005414753432672137
0.004546221528255292
0.0010394057562820188
0.010276892989334742
0.004624917992521481
0.006123871594079795
0.004299077875007414
0.008145869469105603
0.0006452033093935481
0.0026956726960672174
0.004436994901927033
0.00570826050126057
0.0036838122772856687
0.006556344698183477
0.007241650452341558
0.006772960931957922
0.008173650819411389
0.007630620667347607
0.004792533620684664
0.009742577028906687
0.0011733760462204882
0.006528150301363384
0.008800793060413254
0.00502145268711074
0.004758968007047231
0.008209409678044023
0.010549227325033743
0.006196161773232222
0.012108556162759819
0.007093766434091901
0.00425736829481634
0.007506591989807604
0.009345936418255143
0.010971692856012691
0.007362423505357715
0.00865682515075988
0.00687928366387457
0.005068289472227759
0.004139898960295611
0.004609436444662069
0.007583532892483514
0.0049220555326362145
0.004863579462738931
0.005725656911974246
0.004567105499441411
0.004163223297145332
0.0055682804995439514
0.0025266365980807226
0.0045666230035347095
0.004537571611240158
0.005829262046085772
0.00771554006315787
0.006054593541985203
0.0055731100040917305
0.007364153582703262
0.006373992205194082
0.006501754353548452
0.0060477575981995385
0.005934785070715377
0.006007383830610307
0.0054551032117150094
0.005791773847326074
0.006610608756063487
0.007942667779104663
0.005927725459160321
0.005931312887440443
0.009856069325292604
0.008291655801969132
0.008526613117243996
0.009311790938177423
0.00783376414059403
0.00953415071384096
0.009166768010394535
0.0027586706966495162
0.006719177532342519
0.008422763730899166
0.008661796062089323
0.010014929036900802
0.005700689079631201
0.004722656999227327
0.006279955447983489
0.00612906134689842
0.005748056302295556
0.008390378256634256
0.0073682086614223815
0.00620552935535501
0.008722831882436735
0.006963887073757781
0.007093421157150948
0.010480020690944431
0.008227451536170877
0.006415255782702337
0.010369626646913186
0.006374426149673789
0.0009450419773533417
0.003820328266963156
0.0075761932282991
0.0042610895989451175
0.009671716605877213
0.010626595920941256
