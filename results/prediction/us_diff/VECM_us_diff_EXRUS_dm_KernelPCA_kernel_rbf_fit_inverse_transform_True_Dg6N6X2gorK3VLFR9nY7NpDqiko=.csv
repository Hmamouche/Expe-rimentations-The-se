# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; EXRUS
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=3, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.00911184773222929
-0.010886793350647704
0.01017095295030633
0.06193695647651784
0.040026060503855695
0.040710169934975574
0.07781314000439689
0.015904811211671612
-0.008077108045819903
-0.08861767445295707
-0.09479165392806993
-0.06612076350448896
-0.058120541444655976
-0.025543047543924814
-0.06896798133089864
-0.05884754812519022
0.0012316409706932428
-0.018310099664129043
-0.06182765024071975
0.008469030861407868
0.019423896365581943
-0.03793593322278859
0.003420051167152825
0.03544983090431311
-0.001229205405192368
-0.012191068009124532
-0.007042948989881259
-0.008876665358975851
-0.06450997754281672
-0.04736717732176418
-0.017457556836623474
0.04067038389931803
-0.013866406209859614
-0.019122811995834477
0.041989644370877925
-0.01639088365292596
-0.05336305016110462
0.06900025843944932
-0.0031831713029446436
-0.010632194803130095
0.03536593495666296
-0.002154076692575979
0.012534616214680053
0.005086268646348033
-0.018297739507735125
0.011703287396890605
-0.026390291252506567
-0.02906127636403599
0.023805886794711965
-0.030838328780106797
0.01901817955169388
0.006731118532343379
-0.029583897374685718
0.031137819503840064
0.03958922171773262
0.022123602994371244
0.03265652225391937
0.03488548757125621
0.03146201290176398
0.023967156829159782
0.01047224890941181
-0.023551308016959628
0.0018085432570982135
-0.003466752278271104
-0.02354655328756507
0.0019709069772092455
0.024036223745721456
0.025845731077241592
0.027093971768977057
0.049972016098324856
-0.005016231541153861
0.03351285696775109
-0.024377921490695224
0.02028053571185551
0.033733378704131574
-0.04471893230809176
-0.018694534360517816
-0.0020051650313526667
-0.06496306572464257
-0.05632569706372812
-0.017349946344675127
-0.08232853341171262
-0.01613663577888368
0.0015661428451156732
-0.02997376916693066
-0.0036857502253228574
-0.003582261378769873
-0.012235498061870376
-0.000272867691473987
0.008714468454500718
-0.00636090960742846
0.0076446566726662045
-0.027513017446824783
0.0006693781850307425
-0.01106059178499359
-0.0200820082136852
-0.029028625166842695
-0.032182464224101495
-0.03245546489781511
-0.023878380617059453
