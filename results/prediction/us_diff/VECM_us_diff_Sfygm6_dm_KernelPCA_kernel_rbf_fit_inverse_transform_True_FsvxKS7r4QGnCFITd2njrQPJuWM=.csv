# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; Sfygm6
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=2, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.17484353805692243
-0.08223662299456401
-0.007695444107873893
0.03585012073078436
0.002336345417474378
0.03922701957586088
0.025049985492923574
-0.030429958779326773
-0.07727795834364765
0.019023435757365894
0.008186788925522166
-0.0031676545815141066
-0.025417020952671607
-0.030154448007844625
-0.04243400463021947
0.06279694381984324
-0.02666044733791558
0.0725008221559983
0.07069527711200937
0.014711058830633514
0.06165131810935589
-0.05816172105581723
-0.016411538777710878
-0.01962925553350715
-0.04243123843471892
-0.05727898276258355
-0.07767791666021967
-0.028822969588479453
0.03630049639078461
0.05665745179483833
0.0527924958830154
-0.02149541714048327
-0.01875030644495896
0.025321212585187172
0.025429027070650978
0.017689951147569632
-0.012052886481271095
-0.011640877066324093
0.031243754401227217
0.007853345693918892
0.012337091048899842
0.032227758518888594
-0.02501683619037077
-0.0015857728761827367
0.04014049429039414
0.037027685020157464
0.08176390876431995
0.01934735111342802
-0.027647090354117172
-0.08709194859405645
-0.1034996556870649
-0.039928229670582
0.02703867081983067
-0.00047704920728304685
0.05429074364732446
0.01830892933325496
-0.01381409439413352
0.0066807718374839835
0.010784223040106903
-0.02179940388931411
-0.012557534920313014
-0.0188467558812938
0.002805978177114701
-0.00417698946233443
-0.005111551470365993
0.015603924998208209
0.03686459962610029
0.009481703719007886
0.012284113126450596
-0.010621114389836778
-0.0020369894597721226
-0.045383385547738256
-0.05061360368874809
0.004921779310706767
-0.023434850822776236
0.05702747661655311
-0.021003577191789954
0.09104272979032173
-0.042974687753384234
0.007729199250263842
-0.029168143710108806
0.015130669434583766
-0.01989672352871938
0.031149082133689374
0.009404374054717961
0.030267101196232202
0.011303077364936069
0.027185571343448605
-0.009506745637257551
-0.005524989832283389
0.010638758711475236
-0.02259509358247399
0.006435595008661046
-0.054384148758331634
-0.007773149585338344
-0.014787953325036818
-0.005321177498035469
0.004374582587083561
0.037811816924205224
0.021943626023874313
