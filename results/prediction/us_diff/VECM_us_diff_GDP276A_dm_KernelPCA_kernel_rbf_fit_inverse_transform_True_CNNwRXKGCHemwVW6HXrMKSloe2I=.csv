# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; GDP276A
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=8, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.006972433473950349
0.005042856271866948
0.006504243074244559
0.008242400478063856
0.006287936491437916
0.005326964372357856
0.007242045182400063
0.004628076567292883
0.005952176302457477
0.0054518553416093895
0.006687174125464611
0.007178558678515216
0.005812983363794737
0.007025920904732328
0.004041023158494167
0.0038682441050917624
0.005905404865186116
0.005158982448427887
0.007922583506944188
0.006834655085703057
0.008912729062834456
0.007395136072640834
0.007902120974865812
0.0067969684316324785
0.00587686775466629
0.006796571026024345
0.00695261791890105
0.009691036087173168
0.008532419366361298
0.008573512566732723
0.0075843494677230346
0.006008181672658281
0.007213327358288122
0.006755535843008383
0.00797535211374766
0.006162164607223705
0.0061973727763048395
0.00578252491693799
0.007649882235554915
0.0057388351852272
0.00738485135031163
0.004321399871288356
0.004861106858772183
0.005347371197260704
0.006706176892188789
0.005091102662549852
0.00659985244372762
0.00457271019954541
0.006717962191960648
0.006392745304691434
0.003933965752032388
0.005570515267039733
0.005588485228878334
0.00485781151263247
0.0061083910194863885
0.005993692491417498
0.004324195948292435
0.004972137944721004
0.004399225706160657
0.003513770405843065
0.004701781920584545
0.004346643594601495
0.004123158888860512
0.0050393009924237404
0.004173655298384774
0.006243934172042348
0.0064283638298888645
0.0064428065635658265
0.004174831167341683
0.005834397528351187
0.008753616887371703
0.0059277195960543184
0.00667973644054932
0.006597625028534328
0.007928548957910859
0.006251544031008853
0.004724506031827373
0.006892893446927293
0.0073391404957402675
0.0068018342352814875
0.008244813219815855
0.007832743286197428
0.008391938654392613
0.0072935165406619006
0.008228981335805194
0.007598685450406496
0.008598733333424318
0.008708710166229426
0.007856221913923218
0.010527828422894972
0.008327658922851481
0.009678085942310989
0.00873580264439059
0.007780992522619858
0.009582078710932576
0.00909227288376192
0.008467769883905976
0.008521577161872556
0.0088972973143565
0.010122878657415615
