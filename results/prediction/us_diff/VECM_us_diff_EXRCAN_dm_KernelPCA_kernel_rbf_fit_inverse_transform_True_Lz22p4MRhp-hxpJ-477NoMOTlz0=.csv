# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; EXRCAN
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=11, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.011747769177598098
0.01659623718529966
0.021525029712917608
0.056927484081831396
0.032054166332296456
0.021817835982844577
0.0128870451444634
0.0353298733877669
0.01594042753784232
0.00842294844657285
0.01090371551863072
-0.0023210168987957934
-0.005774241868889012
-0.00012554792807014173
-0.04121501664083445
-0.0003118167266515904
-0.03926257690983322
-0.0030357129520471207
-0.02485410041871268
-0.01785816754430191
-0.04070609704478403
0.003900807721385823
-0.03093335371696362
-0.025935317810780553
-0.02580368481884483
-0.009942109193434209
-0.020130730054060707
-0.02033530433998941
-0.04016279402453901
-0.03451820663554115
-0.021258745438757224
0.0331380916344184
-0.00506167385049908
0.009532919473803844
0.033813617415730815
-0.01030163001839712
0.005331645602667055
0.04321260987166904
0.016484844814605125
0.052264086407641346
0.06537577205009248
0.029899846880131267
0.03388783662877215
0.03914483304823231
0.006986616795891486
0.02854959646759871
0.028571261987853487
-0.03917577680401824
0.04351638868734216
-0.034191355870164294
0.023811608641872255
-0.0069146532240974175
0.018474147354177045
-0.028712389711393886
-0.025384973761982198
0.00501787166661285
0.018724352673152922
0.03622877074799336
0.007097565289216498
0.0341046228476099
0.06929659307770375
0.049244631055069994
0.06216525712019577
-0.04367306610051335
-0.007902792052190181
-0.012173528955319923
-0.028739040538376462
0.010103598293303146
-0.01765911628634948
0.05500558447006358
0.001134236444919055
0.031451540571785376
0.017926607211714768
0.039694708588293104
0.03206406319997816
-0.01186994469516571
-0.00891254579712933
-0.01452309605177704
-0.004104483458754559
-0.034182204394433535
-0.01711811378735218
-0.15101607237729944
-0.054560356239218365
-0.07305774181897542
-0.0449480159661106
-0.029892739852906164
-0.017473173062076807
-0.036925359289927184
-0.03933576850536276
-0.028397290877296526
-0.040023221596141786
-0.04428019543627154
-0.024218110054823112
-0.02731500370122801
0.035146828860292814
-0.049081509881817025
0.003571420190555003
-0.09330851101432848
-0.04119662045873848
-0.07436005540992602
