# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FYGT1
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=3, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.15853657261392173
-0.0540209536009265
0.07884554465221791
0.023013403563820256
-0.009826511035205411
-0.045881314894762555
0.0805133058340911
-0.05989165236195058
-0.054562614667099496
-0.07066363437533046
-0.03617896420568789
-0.018242715863956226
-0.04529075256871112
-0.06869626469121597
-0.02958778893755301
-0.007405681589936534
0.027726454005567846
0.00242104964493886
0.05202561625020017
0.010583604579414908
0.009768913944307839
0.021463284709790025
0.01949462873660704
-0.0003962971311875004
0.000586946296345054
0.00517821094655822
-0.0008899072995754466
-0.031159065008088682
-0.045886986021758086
-0.04569669975384576
-0.0517443350963244
-5.660263441507711e-05
4.4181100135404217e-05
-0.03088849996508934
-0.011163343908762988
-0.05813162507205865
-0.05126078662567104
-0.018169766483794307
-0.03957624999451409
-0.028286746105748325
0.0023419286274046474
0.009246582147295962
0.04000572659359389
0.05320811157822304
0.006979012499678534
0.0876770163310418
0.019073133716812504
0.010733147929839914
0.016802018858789784
-0.027702490553251153
-0.02038677248501637
0.011667604291458413
-0.004928930561739795
0.033162869387860325
0.015400018394563206
0.012829035457589101
0.010708912962383113
0.010654109966861376
-0.0022622383152458053
-0.028727748363123234
-0.03585769462583131
-0.02267471352818462
0.025450522825211368
-0.019304239122465122
0.01970302287329743
0.018864379225072975
0.03384866722685163
-0.00083096458459141
-0.005915350765595161
-0.023830338617008954
-0.04377009875745093
-0.029431667114905465
-0.04874387875317277
-0.07598881024917956
0.009055259686026045
-0.06442771976399352
0.0166477023006089
-0.06001099451788868
-0.03538316055393604
-0.029928250634843966
0.01726844411949894
0.03158989519435036
0.009027435684143409
0.029249392638346593
-0.017990615423857027
-0.0010409829473299647
0.02694995389424914
-0.0005967136715650683
0.04247168421675904
0.03410351689810133
0.04744147911400271
0.019544345553021814
0.006905274813554097
-0.007938031913154554
0.03181648300472841
-0.00013908726723009003
-0.014585628903088687
-0.01822586556942017
-0.06612137989402096
-0.01679877422546617
