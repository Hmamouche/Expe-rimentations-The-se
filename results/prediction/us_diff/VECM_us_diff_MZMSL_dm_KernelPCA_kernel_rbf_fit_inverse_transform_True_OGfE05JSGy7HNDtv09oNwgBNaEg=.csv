# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; MZMSL
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=7, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.018261455131599123
0.0005450588597873105
0.009867338030895918
-2.0741522990610573e-05
0.003112812356320759
0.0038285983678050324
0.005489306655275804
0.008723497808633707
0.0029057595426331306
0.0058542066009284
0.005240798005446752
0.010604532571082797
0.007017394321680395
0.004917149785150253
0.0033469305303480243
0.002577655220457272
0.005566977122778827
0.0002460259912952928
0.004247771832721911
0.002299627191317745
0.0001528441555093559
0.00158963905232012
-0.001467718817233165
0.001747792592467047
0.00425452978015843
0.0033846767424040368
0.001381474036481461
0.0029622533175860887
0.005777589458631287
0.004142348850772777
0.007222612106046928
0.0039964635403786485
0.005141105601622115
0.010109301204482175
0.009352086867758277
0.006683174691817048
0.00787147733499901
0.003681297873818606
0.005829117765673277
0.006044061998813953
0.004526746560214137
0.004565926259069446
0.0010850530002539176
0.0002652311400494205
0.0005083319497819896
-0.0032366567924854107
-0.00011901230023543899
0.002666777690892022
0.004531258808601968
0.004341083457999008
0.004603440115541154
0.0021574531980537003
0.006449208626482851
0.00542438722972974
0.005051810002270002
0.0065246226976328325
0.009213288911928555
0.007285114372504868
0.01101671209413873
0.011763287319168276
0.01310932687738515
0.018447377790495056
0.011902265002501095
0.01107150787145545
0.009840586565574998
0.009686676383926137
0.009499973620561982
0.01131173592484246
0.01087318749735003
0.010684752145631655
0.02641785288845863
0.023998042369459825
0.01862933304141168
0.030082065292639442
0.017874665863575643
0.015957275074982836
0.012931151224451461
0.019550639853843622
0.013151913115380299
0.010283680901897204
0.015087059103393375
0.003131065231169904
0.005623420898549382
0.006261274700319537
0.00956086449831783
0.006505358869154848
-0.0010662560515209353
0.006751959993839011
0.0043026576865030945
0.008409999798711782
0.009424574441029536
0.0025359580178177226
0.010649905001880825
0.011767223010991468
0.010913203330238963
0.020587488752887857
0.027452050805011275
0.03211104359710452
0.03784976679722778
0.030397313963307238
