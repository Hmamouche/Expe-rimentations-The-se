# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; GDP272A
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=10, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.004767882278022384
0.006181757562567331
0.006792549213676657
0.007375123523852167
0.005924780084610156
0.003865474777791419
0.009345325905655883
0.003753148509544145
0.002957448219696797
0.005029883368346968
0.002158481017711697
0.0018109771483226576
0.005582945129579055
0.005263723468558087
0.004980226934521104
0.0044035399643372105
0.007119432642787674
0.0056332727505707635
0.004955651552430027
0.007147229395686189
0.007427495929020057
0.008086041607779341
0.007334025115705595
0.007770467377929732
0.004790006205723423
0.006062600514659475
0.00756017905782148
0.008376632374310014
0.007918555380989433
0.007052318499966031
0.00813323264293513
0.004523007202122665
0.007832344892229651
0.005510213512407138
0.005136333315916236
0.005535049835881231
0.002372681108411469
0.0038350746678065912
0.007581682123243922
0.003496692613564881
0.005797301970977786
0.0038612765254473877
0.004477665579462496
0.005263977560205553
0.006437672915209696
0.004693336547665053
0.005293523181347459
0.0038067893276552433
0.00438379307326495
0.005242085668454153
0.0025477142613875604
0.004155836182876792
0.003928746443554121
0.004312105675448234
0.004434494173162735
0.003933602094973955
0.0029732661938186595
0.003893159150816668
0.0013962861550912981
0.0021663923323226152
0.002865701366616885
0.0033820155931050077
0.0028475432695596577
0.003964079107134954
0.0035764226477914794
0.005326669665202613
0.005987101936686844
0.00479707399639132
0.004590523527385759
0.006127108181106015
0.0055398579355381995
0.007089493807007444
0.004892248172905286
0.004815742866048648
0.006624507894533532
0.0018802856450318263
0.0036838023810998244
0.006645506512878161
0.005835524903415339
0.0029459687842251807
0.005824939444189236
0.007944231930649685
0.004940944096768197
0.009687945322067196
0.007942926388823863
0.00880966110642961
0.009938655692547042
0.007025859976761355
0.009885082935665299
0.0099555095253489
0.008840382079151513
0.010373914248724372
0.00735535486373654
0.007879398031937117
0.009163794695429953
0.00604743159724418
0.007794461502347129
0.008399103000887129
0.005420381037276007
0.0038551844964892436
