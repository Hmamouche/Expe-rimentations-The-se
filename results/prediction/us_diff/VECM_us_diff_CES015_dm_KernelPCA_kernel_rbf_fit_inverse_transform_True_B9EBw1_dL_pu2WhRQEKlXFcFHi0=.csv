# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; CES015
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=10, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.06684587849367707
0.0422911169407162
0.019841637532351467
-0.01343683103082777
-0.04255855729623095
-0.024372905851663684
0.0254636700379565
0.004175376654121505
0.0059655117791141896
-0.004714421834264532
0.018863058768073447
-0.02211359149486441
-0.026816550911836497
-0.01782827666786809
0.030564639685433968
0.026276920216033957
0.008114240258539249
-0.008863609674943294
0.017428452396275128
0.02023407860890017
0.004191234565546632
0.03038869383143245
-0.02933044294732961
-0.015535899635158935
-0.03532724658278698
0.009096952300866002
-0.007342552866490705
-0.025081551347225714
-0.01712835570487317
-0.06359214049872344
-0.06724009795442039
-0.0029322696455055808
0.03562664082594639
-0.011395979433655487
0.012891980271417539
-0.009285726470659169
-0.02502128165585787
0.016746516632869725
-0.0008271949486003977
-0.03061048779282592
0.019224349301886164
0.018840149675571176
0.02347124779037051
0.025084275979065568
0.017670384934333948
0.018546319941991962
-0.009217685766534721
-0.004731654916658572
-0.014591418185408924
-0.041737819158265896
0.005838476711758804
0.030429201334797592
0.015520288129215888
0.039789356599930194
0.009425387843578025
-0.0022126834453608333
0.01803050582821512
0.004153468671974966
0.0035521168207851015
0.003479175552078795
-0.03248824533692901
-0.007137286686300942
0.014142567781383227
-0.01475818417586381
0.024112325637394696
0.004471484828352799
-0.017733853810992687
-0.025929322522302976
-0.011346436884386722
-0.018065263788646854
-0.037555018937296916
-0.04902989529779001
-0.07695961535582709
-0.06710331461952199
0.01276744630144019
-0.024077873093645594
-0.016665912998890142
-0.045056440536894046
-0.053482835047267176
-0.020630393983516543
-0.011895945363142039
-0.0011848367426914915
-0.012696511302008252
-0.006448512066998516
-0.011867992747406832
-0.017148468885816073
-0.02009017846773136
-0.006238117827871745
0.014266940043924571
-0.007299024013813082
0.0172704843739712
-0.006973951078297712
-0.03142300007476076
-0.010132275162765342
0.014393210823815297
-0.020541431577753448
-0.025865564096189293
-0.026579159028864735
-0.033667083218151206
-0.011777436291630685
