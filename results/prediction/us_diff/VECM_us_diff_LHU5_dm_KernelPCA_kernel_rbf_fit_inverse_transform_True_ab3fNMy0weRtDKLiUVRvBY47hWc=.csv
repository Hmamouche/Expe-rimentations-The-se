# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; LHU5
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=9, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.026750343171596042
0.05535843434145839
-0.05506952767289677
0.012297915258403607
0.06534308096688765
-0.105791495267729
0.026526270197011623
-0.04074468679024328
0.027783628387669245
-0.015072682545612365
-0.01700933882364243
0.006228327547843672
0.0026095177055592625
0.04293780286648709
-0.009852730428541451
-0.043878893177897074
-0.0187850345372149
0.00883276459135976
-0.06250418170093687
-0.004322207201369416
-0.06388222341577939
-0.039692072973508775
0.032651735643194006
0.028914846290793664
0.009577602487687204
-0.008370874648150057
0.0033280744507315184
0.04083674485660658
-0.0283572414156837
0.06734597061577233
0.056956067045092526
-0.03971765184747361
-0.03937824800686382
-0.04359270135603774
0.039181709923458936
0.052230766700284786
-0.0022080484492502323
-0.02119979505537215
0.001995145747306158
-0.03202523119349464
-0.007338176795334628
0.0040780087490854405
-0.09178460293626213
0.023903124436918194
-0.08788570724554287
-0.036051716787677886
-0.016601736408701148
-0.06839242055450023
0.04055412335245684
-0.031039562602653697
0.004814066498663694
-0.027484037298555794
-0.022634138222895187
-0.024115406047099264
-0.0170742529772838
0.03419751979005975
-0.05017705745794465
0.008497311903290318
-0.003132923446784189
0.011861671869382891
0.0232043586607614
0.0009294359755181308
-0.027485314095089232
0.027536415393502753
-0.04152307955230134
0.030575860106244055
0.015667614140187233
0.03529481772188457
0.0021070890745185367
-0.014174158058129771
0.004888619448756994
-0.007614871170031892
0.02537518183134858
0.02060485639638881
-0.0461302159400754
0.0598357913916792
0.009904620774910392
0.02295714631098471
0.051016715114853534
-0.047651063705328074
-0.04129250815691218
-0.022193743042983875
-0.0553636150665955
0.04796126263576333
-0.008322758203396115
0.017352554465607416
-0.011546162479379977
0.02660409939942488
-0.03172494977314998
0.040652722524274355
-0.05356220223509838
0.046038206984488766
-0.0010417115403579822
0.005472644431486462
-0.04590257766102176
-0.014007111648733722
-0.015510449591091695
0.03338842764742067
0.03785905539365973
0.011362915696252012
