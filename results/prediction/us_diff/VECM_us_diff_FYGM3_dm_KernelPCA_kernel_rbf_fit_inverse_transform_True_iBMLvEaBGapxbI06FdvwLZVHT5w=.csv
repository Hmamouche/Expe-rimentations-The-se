# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FYGM3
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=15, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.022386147597301863
0.09577931764321035
0.038735315491668185
-0.03605261896485561
-0.054043010005066436
-0.13648216355131265
0.057909113339401824
-0.04321223941040514
0.1072944803521567
0.11968948188557078
-0.0790466398417265
-0.1337058059493202
-0.07814413008691928
-0.07927860828488915
0.05588254558326848
9.853939164079784e-05
-0.07869851004310748
-0.06733384143349272
0.15662845295763686
0.0179473417062445
0.004264516848327599
0.07788460747616649
0.05500322945582006
-0.0697914941178775
-0.03916834506327922
0.10144495656961691
0.0071348905473758925
-0.0506523425292059
-0.03125934806230865
0.08036899160313023
-0.17745061623711644
-0.10764938110891291
0.0637990921008645
-0.0678680435225405
0.003767582433763683
-0.008062483318092964
-0.08942041681829149
-0.11191424531623065
-0.02022335760418472
-0.04876253970684607
0.0985105559799487
0.012661941704856142
-0.031174149794755416
0.004071019469484903
0.03123486154705543
0.09405719472474916
0.005279606892387531
0.06899385850160891
-0.02058617065458857
0.014314896816144718
-0.05615365346041426
-0.010009417963289406
0.0011483697081613488
0.02931631494458555
0.005982617477567344
-0.019792289591875768
0.010671551232890214
0.03000535669388711
-0.026107973861918453
-0.018808927408897932
-0.04843861810698816
-0.055632631816370105
0.0371412417256546
0.01782545306067252
-0.012663125287513066
0.08537214845866958
-0.06350226022389381
-0.031116083967958143
-0.03140553324028282
-0.003306523596379854
-0.06452623431609969
0.026044994426672546
-0.054952194480327426
-0.12615715853575868
0.012490261345807838
-0.020068733001872795
-0.010315540281496033
-0.038059972461157845
-0.06433176497178154
0.044392526925148314
0.030613102330899887
0.06086155643862477
-0.04270444364757363
-0.008566442558659092
-0.006886941781331665
0.023263684205960266
0.021857046253457045
-0.05077968810032195
0.04022922290187261
0.0020004054972374563
0.06458686706832815
0.02832236277061689
0.05237010345387971
0.027355437016994666
-0.012396338921818337
0.059177065792881116
-0.016406332717111696
-0.019846335416143772
-0.10226667109332246
-0.017591626565017702
