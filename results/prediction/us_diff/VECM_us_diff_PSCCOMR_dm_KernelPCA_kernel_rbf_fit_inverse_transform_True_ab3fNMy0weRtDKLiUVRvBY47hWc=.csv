# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; PSCCOMR
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=9, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.015851293511346273
0.09460007525201561
0.04427933679703971
-0.07064406844972601
-0.04067887188074068
0.00665557173090426
-0.023138496723445735
-0.009440601149350403
-0.030145298339293938
-0.03668942475666905
0.01807894344092562
-0.04649912095160156
-0.047354504042425666
-0.006778086604994232
-0.014549597631987064
0.05538382816383154
0.04156595412458175
-0.02513083368768896
0.047053467673980255
-0.02362267811197243
-0.01669638464806861
-0.002748952842117885
-0.03777049300668711
-0.010959234399881619
-0.01875633510816803
0.01789890402926956
0.019403221135170873
-0.01834454128339805
0.0210772667979856
-0.0004142857399417714
-0.05664989777442813
-0.08162806723792483
-0.03443913716726965
0.034311465263240346
0.018613974130403096
0.025026641934554082
-0.05889840538724056
-0.030632761503809638
0.016998001713454794
-0.03175797467923672
0.013367177814191174
0.050605609828962284
0.02256109514185463
-0.03838427047501989
0.023814179125402835
0.04157790140294234
-0.020110376959592433
-0.016531672009477978
0.009822440917635643
-0.011229185570502672
-0.018542403092552378
0.04443623406179917
0.005412565026472535
0.008899600844561671
0.033440332272360526
-0.03368425700956584
-0.0005970476026987631
0.001032264432198617
-0.04131341371800037
-0.0160620474516981
-0.03429113373277666
-0.024446148267252943
0.02652475450023098
-0.03265218557555713
-0.003070072612577009
-0.007863873420415593
-0.000911765919941924
-0.057415037773807756
-0.024917535151689416
0.00537798217936153
0.008116807143893904
0.013664719171967026
-0.037009033547890646
-0.036853742237156685
0.0389417177124582
-0.0033289682539929463
-0.017249520024799167
0.03635243652672449
0.04482665055051914
-0.0026977808502817308
-0.024664804531405564
0.06518952967382205
0.05530930721944683
0.02601813006926933
-0.006051661520955866
-0.02338990653987369
-0.03452468234386033
-0.02039928709769554
0.01949054279218171
0.030288073329956023
0.0014036371084759731
-0.010010959566299596
0.0072180037008832505
-0.00572454554432994
0.0381253235872116
0.05777047005755474
0.034753927568985536
0.003764813752447426
0.024915523460497812
0.02404919722432862
