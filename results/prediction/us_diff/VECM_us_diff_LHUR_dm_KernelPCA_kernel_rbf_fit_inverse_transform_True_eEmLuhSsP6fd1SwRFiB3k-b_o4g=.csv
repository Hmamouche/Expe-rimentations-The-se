# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; LHUR
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=4, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.0015231654807347766
-0.051879047347458115
-0.06899737085119556
-0.020457392399054086
0.014545921720019988
-0.015054858320741493
-0.06312012299814626
0.0024416253592343447
0.0029012793524309938
-0.027030486439929204
-0.02292971635296085
-0.009254527522759903
0.0027039959154461753
0.024617327940280976
-0.017086415053106263
-0.03014669261558602
-0.07211935007731729
-0.01538190705656109
-0.046507749001120584
-0.01637594850061042
-0.01839862270272751
-0.03516580766281946
-0.011290621830263867
0.016417335804321646
0.014424771294261155
0.01153484861518558
-0.0065767487214259245
0.0064189368184500465
0.053978291730335956
0.08109760732239321
0.06354939667149925
0.00965840624006959
-0.044063338509119795
0.0130117170562558
0.02756767471173097
0.05175000454679092
0.024100737512661362
-0.01390755896768017
-0.00700465831505909
0.02294601205126183
-0.036857085329809344
-0.036345153878769916
-0.03972518538063356
-0.05299151829968201
-0.026677769190966413
-0.043381098528475916
-0.01673378580226385
0.009852448197384024
-0.007868030583035606
-0.004870681194036898
0.004088121624516595
0.00026842433671385414
-0.03205226163788309
-0.02197514646811883
-0.020437495682794668
-0.02922241110464391
-0.04008088842542583
-0.024345965411718355
-0.014951455046581514
-0.0031539488050903687
0.026094770293992677
-0.005056486688340828
-0.02448207596608126
-0.005454275687350203
-0.018422999668651636
-0.03215760845548271
-0.008730270431313393
-0.003509430188940969
0.01501632195333676
0.0069483379086852735
0.048541671300950634
0.02928495863342086
0.04474450013610964
0.07600833290268968
0.0008254711855016474
0.02689115344814605
0.0010049957390811937
0.03942461631602021
0.03802845349241592
0.030979924514691025
-0.0245897443074277
-0.06036029120702785
-0.032585486199469396
-0.0014417964624815535
-0.009217549964400254
0.008083235430421252
-0.01354061117174539
-0.003769866133369445
-0.021375498035046542
-0.005785031655319383
-0.04317472447725151
0.003910409576670288
0.006759108901985106
-0.009809803205859995
-0.014857112929779398
-0.014071242532595934
0.0180775632253884
0.027721889179570195
0.03714041762330139
0.054408668607080324
