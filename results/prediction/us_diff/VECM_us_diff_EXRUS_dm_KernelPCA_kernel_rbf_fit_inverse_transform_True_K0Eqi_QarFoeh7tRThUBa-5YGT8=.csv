# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; EXRUS
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=6, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.008349633702691761
-0.03412628051669817
0.014026249165972272
0.059483718001313926
0.049806056859523926
0.053776612912624214
0.07031309127996087
0.004820631558063226
-0.03340247587952089
-0.104868436915064
-0.07663792646345378
-0.014288629642712894
-0.06845946439170872
-0.02261872503423914
-0.08559851036727241
-0.06642764489396687
0.003703251048858436
-0.04047874129790004
-0.03721843925273621
0.005209444782153494
0.02972882087239799
-0.032405123472298514
0.009536522300667795
0.05118528685399128
0.020068882999118638
-0.008506953759539955
-0.030668022038152512
0.008530925255785754
-0.09368025825247026
-0.07815733876751732
0.001342957666818535
0.038090410581710504
-0.04875436195407351
-0.01298559371869417
0.06996270809264662
0.0021985137806917743
-0.06123550158003957
0.0743371195910615
-0.011504300121313019
-0.0245773935149076
0.02951752177744021
-0.0067612702810145725
0.03197562458124433
-0.017596385422836588
-0.010505805478722416
0.018133258081655147
-0.012567049248843026
-0.022945261030778867
0.03031615697649482
-0.033700825199034994
0.025371373834887023
-0.011141149205541625
-0.020246454355434568
0.04617666107781013
0.02682179043085131
0.037467118699383184
0.048032557905931195
0.029323824117361775
0.03647999753382234
0.02531273534163027
0.0007193900981316316
-0.03378063337961405
0.014507701166377982
-0.018141446509562176
-0.037312470628725454
0.01621341927290864
0.02366344422661872
0.02066734478705458
0.028954121232772558
0.039393690029367975
-0.017841207862326666
0.04306695037741925
-0.02910016084872331
0.008832293568697601
0.03048556084817674
-0.03633365190999849
-0.024825572690297008
-0.002533823288376036
-0.07454811903008449
-0.03824120148800071
-0.028741868623459076
-0.07267689769877636
0.0027503682054823278
-0.005824546710340483
-0.021504868608049788
-0.0007969086657743166
-0.012059620683857134
-0.02515072090615665
-0.007444443351659902
0.013521140404932025
0.010009234763612367
-0.002036022335009294
-0.02381167217324052
0.029880524949604685
-0.027758578598463014
-0.022337894730394308
-0.004730635771451072
-0.031079661054849377
-0.047645402364629955
-0.05533070912759545
