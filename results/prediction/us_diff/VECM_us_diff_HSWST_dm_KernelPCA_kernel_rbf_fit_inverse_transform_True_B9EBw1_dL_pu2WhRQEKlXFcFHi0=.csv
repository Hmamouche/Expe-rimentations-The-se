# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; HSWST
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=10, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.10585897550223512
0.08589752273797216
-0.10544191110112419
-0.045607657936652855
0.04840205138525219
0.08970714729570556
-0.04836202201723417
0.147200132036779
0.05561489151572294
0.0368710346899033
0.11007845241162827
-0.006689580682965685
0.003926683120680268
-0.007617691218935959
-0.09199633408899358
-0.08123305124558804
-0.005290904927664605
-0.07513693927574622
-0.0809632680848828
-0.07919518230137561
-0.009892943293425443
0.1208876606584274
-0.014585129232950779
0.03843120738980163
0.00046634750828083646
0.04685729652992035
0.01831079196880231
-0.07794584779062702
-0.033020182882379845
-0.12823816950995529
-0.08057425076341952
-0.05665335075180878
0.03782596324334066
0.07280018993938844
-0.006558264105875913
-0.002032673737878035
-0.05413447816144257
0.03554767147217211
0.03616020475651092
-0.1569582969197215
0.10356591957862336
0.03675617895582661
-0.014449687893140141
0.012681033099332117
0.03345325753553287
-0.09980218928716297
0.08827076010028878
-0.031878735674295625
0.12049105801596657
0.04058335141721487
-0.059137471416173426
0.044088536605650035
0.002035330896129048
-0.05058980580507465
0.0367612871465869
-0.024500878588004933
0.05343134370623114
-0.005459016612824157
0.06213238467183607
-0.0181832033070173
0.004390037699710296
0.05982736829901532
-0.03966619881148074
-0.04332223782844449
-0.020750583906995104
-0.014511562896623677
-0.013995398484401116
0.0075826874652328916
-0.04902354221876365
0.04170114929948704
0.09410041568649356
0.026625711965035145
0.0158213900434204
-0.020425781894944074
0.09510657933872237
0.004062853360850181
-0.0962135638782459
-0.033483525491626955
-0.03777028179187955
0.05162616943806357
0.20532966470574127
-0.035510259288530584
0.016477681532987384
0.0026124536835084103
0.0010814265951526042
-0.0055886723682189755
-0.012398242930237408
0.06239297484029116
-0.06237284339160088
-0.016009916564011562
0.0026961238730290396
-0.031961333721194786
-0.05991577195720676
-0.0203401165148151
-0.044834544533681596
-0.04873881376689843
-0.009392337814108108
-0.14831310695752947
-0.03058870638344334
-0.1474319159055401
