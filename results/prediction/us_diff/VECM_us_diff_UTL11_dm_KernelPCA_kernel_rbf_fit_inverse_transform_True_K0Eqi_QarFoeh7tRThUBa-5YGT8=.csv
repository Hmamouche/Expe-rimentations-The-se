# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; UTL11
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=6, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.16269010113221705
0.041651065427970715
0.13061025295459486
0.020919644886893858
-0.03638490164032604
0.019414388548231278
0.045683814666526684
-0.014815540609001955
-0.025126969292489386
-0.04147751257914463
0.02788400701124029
0.02956793395454752
-0.06816836936564195
-0.05636689122841802
0.025380656692267585
0.036471164607161044
0.060395921101462524
0.025943260687385568
0.02534969915590215
0.05465987025867328
0.02571611820471207
0.08813141181613843
-0.018201540470256402
-0.0301254340058044
-0.030467557095691197
-0.03963606010904234
-0.02660851357042994
-0.010769420082132847
-0.08703780407335018
-0.12908533606976055
0.01395065169828175
0.00875408511612392
0.027202860700670836
0.0006751339872800369
0.06364638758184998
0.02842263060352953
-0.021395551551072308
0.021071897176766146
-0.011570639276662211
-0.03231217346540634
-0.01213668740166962
0.06518720397531601
0.08671792851217291
0.03968943811810322
0.033234121024889686
0.0609274420114667
-0.026981019683707565
-0.04718170918444603
-0.015382836544118325
-0.019347974539641148
-0.011808728613371816
0.009503490793687255
0.03657191138459638
0.05611069936230891
0.00296208403972165
0.03308625023281665
0.05625987892143379
0.01760072042291714
0.008930724536816953
-0.03089115412962495
-0.06373333235843703
-0.03763350540169551
0.01981896571328078
-0.029646511118616815
0.012555822730543321
0.04942625628973329
0.0016953772456430524
-0.04290237122888385
-0.052238928469409474
-0.06678771122400357
-0.08439516673509936
-0.0253193265412553
-0.07038993248953193
-0.09737066845914738
0.020865556585850303
-0.010535234018336677
-0.012467397266553173
-0.030014752413717007
-0.044800991864533374
0.034490508284455886
0.06668458430484649
0.10087961146337007
0.08058371957210797
0.0026109630775185593
-0.007072649705378721
-0.0023684534657662463
-0.0041296092384293
-0.009620672043878235
0.027081677126462853
0.024950664948293254
0.06752112949911557
-0.041066773960462793
-0.03457093731756952
0.013625355358514004
-0.007216970401550449
-0.0012895405711736384
0.020143707423952845
-0.03350016799899657
-0.06619169548136528
-0.04347109631205132
