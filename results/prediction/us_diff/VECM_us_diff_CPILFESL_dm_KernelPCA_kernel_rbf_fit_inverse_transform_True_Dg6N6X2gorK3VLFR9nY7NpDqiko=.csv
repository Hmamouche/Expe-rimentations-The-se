# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; CPILFESL
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=3, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.006666599542869668
0.006441383503477538
0.008373260857931731
0.00851921360732984
0.00704120461767902
0.0052366160459225725
0.005818119409192064
0.005467230431377058
0.006086055675362309
0.006645638976837323
0.005956931641049414
0.005584272198844601
0.005970017462652638
0.005236495430966085
0.005694687874895147
0.006452421674850525
0.006215046646878182
0.0072843776346633304
0.006780370324801456
0.00916163127212101
0.007160321407683089
0.008958220284543999
0.007591943965504298
0.006966930595764342
0.0061781630190728895
0.007482661947392095
0.008472422193943546
0.009202338446638727
0.010544205718005252
0.009062624728781199
0.009567968730393075
0.007805178983787147
0.007942250968029455
0.006540614908853514
0.007545287243586211
0.007013274430453452
0.0056364559209647344
0.007818767535100649
0.007596913801100225
0.006875323236951503
0.006332229224814734
0.005108972160371804
0.005027994417785862
0.006185925633236231
0.0057289233632403895
0.006778552390167742
0.006629388490331474
0.006754662280534441
0.006802075102824701
0.005295701840273264
0.005235695483338735
0.0057895559165388666
0.004639131781237528
0.006226286142962004
0.005656235511164724
0.005434440377628382
0.00448489946865596
0.004805925756053107
0.005008551675456633
0.005441578141632321
0.005252505871422727
0.00516988794174025
0.005159698399996237
0.003906941920880863
0.004595268761703185
0.0056494339777304016
0.00603403106981673
0.006355705184410311
0.006933559173739463
0.005944907675796613
0.00567335306233385
0.005557237337715527
0.006008670682301872
0.005900300649054673
0.008122427507322566
0.004806379591055847
0.0065736879170793456
0.005148189292380709
0.004630136552619077
0.0012312592929523934
0.002915249556205375
0.002855528681645803
0.004469237896928414
0.006337715112177314
0.005766179355015073
0.006266106555912699
0.0066464182844021506
0.0057475331816914665
0.004589453344041463
0.005742183758442479
0.005962122108838784
0.009357953561003557
0.008190311931423397
0.00613058104626271
0.006155349163069948
0.005325957191114147
0.005140259024223135
0.007639924857412853
0.006878008358784969
0.006278273161232404
