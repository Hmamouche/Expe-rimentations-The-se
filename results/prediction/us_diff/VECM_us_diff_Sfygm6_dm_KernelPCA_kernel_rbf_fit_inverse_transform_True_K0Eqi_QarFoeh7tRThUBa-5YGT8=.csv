# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; Sfygm6
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=6, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.15291604330798292
0.14745852940116
-0.007851983469326432
0.08719980518906711
-0.03204329324469932
-0.004613405593091632
-0.10131749750250693
-0.05498131588534784
-0.11368166759178268
-0.005330580453868028
-0.09916649620188261
-0.00157300317688467
-0.02257713380376297
0.09077888184991925
0.013750447827573486
0.12492778405441556
0.1156974588925803
-0.02333561068735745
-0.04089247458261781
0.08653450120226294
-0.031483054421204866
0.06556200899962727
-0.02247378885709741
-0.011025898675084626
-0.07650657913664075
-0.13861213753827062
-0.011373463465245329
0.07107406684717725
0.02727689729026398
-0.05935746397465157
0.0988751018174586
-0.06629781311876254
-0.023232151065695886
0.08747740216853539
-0.022767539313448288
0.014526350492985472
-0.10096731527657989
0.05444532550225223
0.06538157866413677
-0.021486597628519104
0.061601584720700465
-0.01673319016779131
0.027409847660303018
0.11358129911711463
0.02434892713748697
0.0342935416996578
0.02540979907450707
-0.08729980288308374
-0.020129591306818035
-0.08565355894906768
-0.05282773104615731
0.07017994095159535
0.06859525003339974
-0.10571056483654094
0.03314166798827879
0.024054223141296176
-0.053856894887456666
-0.013852880663042407
0.013442660897036128
-0.005010911342460957
-0.016010505566833207
-0.03254301499754714
0.02688524143865071
0.04798041375201996
0.004679638470242723
0.05851733948570863
-0.02882392894576144
-0.050831827228039486
-0.004485268590521495
0.007353571298997047
0.05441779803967756
0.02114619407507871
-0.03788208646349832
0.004550243115612709
0.008413464486466057
0.0014907315484077657
-0.15800049158521506
0.03389745355026518
-0.05718002835705351
0.060246936090799875
0.10389605739932233
0.009117584233693635
-0.023282913359455357
-0.00867676803809887
-0.010356208187240751
-0.055450305045325186
0.06761853847529191
0.0769037981112858
-0.0239592760532634
0.008808165507773257
0.04561003978402922
-0.014578862303113414
-0.03286537108759136
0.01978212301857575
-0.11814488149813898
0.024272446801643418
0.014512156296912528
-0.05603098762481161
-0.01685558665532004
0.04410227500368682
