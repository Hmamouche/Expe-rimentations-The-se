# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; GDP276_1
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=1, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.005160665214470742
0.005158244734353135
0.006188520360654138
0.007360539268802895
0.006976867011370109
0.006652530541216022
0.008135896523444274
0.007988481055710497
0.0077146485346180765
0.008224006063884851
0.008440266849844755
0.008718015490440845
0.0059493418973235075
0.007625059720087089
0.006555160050922889
0.006234218285447165
0.0058248030980233186
0.007198052894056912
0.008199069194647631
0.005209809995403356
0.007655201017513343
0.006360813902052864
0.005486165643417741
0.006754800486109352
0.007140161442909179
0.007699636220765581
0.0066329548289227385
0.008208838973929577
0.009569701423690118
0.005394469307025644
0.008005408025313809
0.0055523419551950165
0.0027620659146470493
0.00537482678645422
0.004685216673581196
0.005039103198056831
0.0046209775486871565
0.004801433885037763
0.004898880791344521
0.0055719503100734495
0.00433864080017358
0.005330229594942512
0.0052770628139320345
0.005366174545597778
0.0048073837221668265
0.005644283583718821
0.005504400860892633
0.006242277436723742
0.006116251999219782
0.006795938238489673
0.006017847312389855
0.006164364714833057
0.005999082517520245
0.005323430079430902
0.0057301968199139
0.006122947928647134
0.0057720602567912635
0.006383081405628963
0.006397594903242253
0.006977317909725769
0.006816801290992646
0.007366690948005615
0.005602107727107261
0.006456470765748448
0.005609885868856669
0.005435116920188283
0.007351811670763561
0.006529525739778368
0.0074920145321681125
0.007910539391362232
0.008021448577218055
0.009226493980693204
0.009313578055683601
0.009549566565671795
0.009912129289381602
0.008627511879425573
0.00807497599966233
0.007725436389851325
0.0068550307622812735
0.004991066399283922
0.005779762401048801
0.005928631798883918
0.005041580661417047
0.00760499918592747
0.006522881648873774
0.006513047714860073
0.007289597471398801
0.006228692828545902
0.006137358458488853
0.007133840955153607
0.007823407682164704
0.010786295429916489
0.011294752108021146
0.0116453555677486
0.010490743508342327
0.00937479230243448
0.008735494621177533
0.008906232292872227
0.00837355957808741
0.006699897100876524
