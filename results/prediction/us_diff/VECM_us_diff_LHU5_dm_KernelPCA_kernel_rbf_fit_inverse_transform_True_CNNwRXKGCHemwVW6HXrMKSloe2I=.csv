# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; LHU5
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=8, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.01441571680710343
0.05628178657782581
-0.03331948340195895
0.02245268742153288
0.046861862761829146
-0.10193014316613178
0.013424867254481202
-0.042279017676274705
0.03446126658045516
-0.009796651432016718
-0.011970829499619726
0.0017458892876854558
0.01322535819409881
0.030169919946291784
-0.00014815947162679114
-0.020674554916566362
-0.032701517693838916
-0.0038008157225864567
-0.04566437439893724
-0.015618891252293236
-0.05263635541542674
-0.04584106285039817
0.03778646679364548
0.01928587610072624
0.017515640164763344
-0.014918115125437456
0.008313058834543632
0.04144362664924765
-0.029734174746477934
0.06686062184170626
0.0532101720155415
-0.03763192068119428
-0.033712512607934265
-0.04412114222043062
0.04296144248686878
0.04413229395824895
0.002994746080954543
-0.025517684574420527
-0.005374988859551667
-0.018381058073218444
-0.017978318760318104
0.007587936191119606
-0.06911558529588295
0.01582398194375495
-0.09262445432718486
-0.03762552607030725
-0.032556014341626444
-0.050495398777201536
0.02244566812953831
-0.014107462908950144
0.005233868480370328
-0.02788919954167941
-0.013375928362331967
-0.028150782869985413
-0.01947366334722691
0.03448276717484226
-0.04834883831740315
-0.0009484391910657974
0.0032439860159168407
0.0056424326007529875
0.032561197617226076
-0.004167107950304862
-0.00983361855253878
0.022169827813827564
-0.04077423833120659
0.025463250220609294
0.010212789474936718
0.029256161950872814
0.010192155393524353
-0.022408155378406062
0.013050667309018969
-0.01700098162591173
0.03614306326849499
0.024280364446349057
-0.04125667331091127
0.06894021441905097
-0.005923284843437132
0.02507259518847136
0.03224261786178173
-0.023676933641751774
-0.04754972887137783
-0.02173856068229795
-0.06115889011943047
0.03885598927671838
0.0031636448738038263
0.014919055201540554
-0.004162208761396302
0.019514643310447798
-0.02962244780699191
0.03279867675618128
-0.04446064330723273
0.04942267421356532
-5.277142153936251e-05
0.003580314848568699
-0.045548825844756044
-0.01439576443766084
-0.016592192481625762
0.031417885514012825
0.03293876970534585
0.024873639693153635
