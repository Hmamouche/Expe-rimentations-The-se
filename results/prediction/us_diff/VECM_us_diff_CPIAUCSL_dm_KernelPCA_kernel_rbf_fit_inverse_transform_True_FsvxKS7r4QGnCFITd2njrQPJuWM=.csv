# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; CPIAUCSL
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=2, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.0025602180933575703
0.006922117636930821
0.008212473621079165
0.006407567164418809
0.006510245815463097
0.004017107727878663
0.004088347346721239
0.0044424270759956435
0.004690079133331208
0.0061328183996044745
0.0020090089853348606
-0.0016930882712698193
0.0031387829356424876
0.0008307555473722994
0.00910398372877389
0.004726597929687309
0.007048559785402242
0.005942361298636136
0.005451679964840016
0.007943567707371122
0.006561463373040276
0.007359945633340662
0.007465479096427145
0.009196016657840403
0.005948155507240875
0.007483882422953881
0.0085169891631489
0.0070274101488112944
0.011818649579924529
0.009789556070216686
0.008829884941529165
0.005162958318519884
0.004152847579656884
0.005922704265503432
0.005983683268864422
0.00598142971453023
0.005455128410456888
0.007281129626699022
0.005874659886034787
0.005880555372819574
0.0041732598258165275
0.005854395054287283
0.0031185568389174977
0.006287047864746345
0.006225065983024507
0.005968900165760177
0.006665537149437912
0.005152277869811663
0.005183989346444349
0.004049669053945082
0.005522041974260837
0.00689876859184514
0.0053324334649970245
0.007478846915431658
0.005114949338009734
0.004191692934441778
0.004138665420697626
0.0038176056216999163
0.0031415671876469193
0.003143244871124457
0.003099250730964652
0.0040591948503926846
0.003899437012303681
0.005249744170936994
0.005472133489860941
0.007699338759256494
0.007971489953175082
0.007006981296004304
0.009384096931355251
0.006096860500379812
0.00768495314363256
0.0053391735053395385
0.004730646451655139
9.351242955393801e-05
0.0036182556267066257
0.004300444552415518
0.0061244165829863395
0.006818662911274968
0.008159638995703346
0.0010229771418106641
0.00720553603292547
0.0010594167093956927
0.008231861670085858
0.006935343519322072
0.007871182206699936
0.009196070033965623
0.007270082003582239
0.007142000457780429
0.009909698453339614
0.011109054955335158
0.009576069831510513
0.008911207874097755
0.008545532688583019
0.0033139945934269003
0.0069275792630955
0.0036517828631987105
0.00876250880795712
0.01214606463968847
0.010103855110322048
0.014414569641589603
