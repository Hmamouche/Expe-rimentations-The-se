# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FYGT1
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=6, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.16877497382055412
-0.06236249248745407
0.12018799687587833
-0.0017489945441962126
-0.023373621447115835
-0.04426435212216916
0.08245128237862945
-0.07422887176012727
-0.04407755265513792
-0.03939290385734062
-0.03359466903559581
-0.03201484442517376
-0.09288176097481204
-0.002511575000833363
-0.020744479963421904
0.004972412012707785
0.024185831480334007
-0.015840667753167607
0.07223256414957031
-0.023650737622644535
0.007099131107135882
0.01840206690883109
0.04122705975317957
-0.010660632734414636
-0.0022128086587842426
0.00735749634901392
0.004649730845914659
0.0072148136858611045
-0.08472513317347705
-0.008658088375347395
-0.04678196066832089
-0.07486515879680941
0.02251800457328978
0.026800453947156154
-0.008387629165335194
-0.04021966626299626
-0.08523880741766662
-0.010084653837136422
0.0110398389378636
-0.04163501694490236
0.05003516558462663
0.011435853778759567
0.004448796500579801
0.016295179311956003
-0.002343693012851216
0.07972784648172467
0.010866802117038884
-0.008448804441016436
0.010001308379799349
-0.006476334479886487
-0.0240702763643056
0.018322681211387744
0.02087790125436226
0.00552021717042793
-0.003057481414335577
0.011347319590685422
-0.011428793644561583
0.0060416285848811785
-0.01339663565842578
-0.0381052920844904
-0.02947125283973373
-0.03462401877830672
0.0614657779537173
-0.036238414450294514
0.014307203371531258
0.035137695961855744
0.009437882033140254
-0.012382625674231634
-0.004691032037324601
-0.007796815135415537
-0.0250825341526589
-0.0237855759663604
-0.046801799555775375
-0.10837926678876758
0.03629727036174275
-0.04697333918326646
-0.005557560870011562
-0.02352303576061162
-0.058233647427909284
0.0241656769025049
-0.002622795484340617
0.04178117770050066
0.004688813530874922
0.006445156302891053
-0.031459591838214784
-0.009164104500253734
0.02145833073888578
0.014326678738655924
0.039151321439045525
0.03326091040190196
0.04121446109901645
-0.0019183431554769224
0.014697683612326711
0.0015870217860284698
0.025914866152947504
0.02479837494079464
0.01174701023267061
-0.05111546318399756
-0.06980924562290416
-0.009617077936348407
