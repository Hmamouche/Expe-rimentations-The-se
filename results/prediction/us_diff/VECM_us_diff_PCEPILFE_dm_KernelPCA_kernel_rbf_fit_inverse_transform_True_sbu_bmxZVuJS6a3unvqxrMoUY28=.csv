# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; PCEPILFE
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=1, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.008062513516692175
0.007919054158081083
0.005795383552907916
0.006775351389011048
0.007387509274837006
0.00573874666907991
0.006709175826460323
0.006221910972458433
0.006150607522543938
0.005655945119440887
0.006049973633220064
0.006730107641696977
0.006856104582820961
0.0069230927884054334
0.006372441347002575
0.0064906642957386265
0.006528518439117207
0.0072550514059534195
0.007913655661960731
0.008749622834675043
0.009106030219271744
0.008677315946300494
0.008753738504975777
0.007700018921721604
0.006556577167180096
0.006616967918036076
0.008054506518002548
0.010122169572775423
0.009558758510450797
0.008058820771955938
0.008713388643791092
0.007546337290150093
0.007935067569935741
0.007577383081430048
0.00788463307095337
0.007865343081927205
0.006650799865465773
0.005951621390898697
0.005687780498106316
0.006224516363861685
0.005212584261349445
0.004781751867766826
0.004533203813387267
0.005075087730008312
0.006699528364476268
0.005790590392564444
0.0052822079501615785
0.005022662145188984
0.00489322058350298
0.0050341200294554385
0.004475808613551607
0.004307150640469775
0.004232776508959932
0.0043782006821324266
0.004252452316722705
0.004438235524179015
0.0032330613352965377
0.002876632122623523
0.003169679993415105
0.0029752799280636736
0.003442602044823072
0.0038990618283052648
0.003585096811506099
0.0039820294056009805
0.0036753594178299074
0.004182002243066421
0.00536916840781569
0.0044275167694128046
0.0033443451555170587
0.0034854404786096634
0.005336647576211422
0.005450777635233949
0.0049382137674866735
0.005923956921800907
0.004391976781438907
0.005071850983209479
0.004414239031903991
0.004003289656179181
0.00387754677167288
0.003418020162486921
0.003980241846334633
0.004308471993428753
0.005417429593349005
0.0062682406153388905
0.0058265210458926875
0.005990375333604437
0.006330318565205624
0.006138374323260526
0.0054134999775909244
0.006214044150077741
0.005807466736157779
0.007095815897405885
0.007115394412656478
0.006334375630259674
0.006490333459242956
0.005898511239079677
0.0060398550725727276
0.006382740247123496
0.006846341743121286
0.0068338000565219045
