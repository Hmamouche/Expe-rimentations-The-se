# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; LHU26
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=5, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.04414580542628613
-0.0562930157110762
-0.09533494409320252
-0.010405292805455352
-0.006255693638846548
-0.02103633211765306
-0.017749939155097524
-0.007824940871432505
0.0007457456517996589
-0.029425350107428097
-0.0010230340442739308
-0.027836176485313596
0.035265699868535325
0.020945107813254853
-0.012714492841063504
0.01089847185007984
-0.06548210362781662
-0.03780332526154537
0.02633849737050902
-0.06020722484877638
0.0035256532538221683
-0.06791509693151274
-0.009089048275991808
0.007069197560724333
0.05141358002237302
0.0013833234433576558
-0.009814460754834888
0.01608851414052104
0.05002178315293475
0.07174635022288459
0.10701671215963177
0.045068576538751154
-0.06317776947006143
0.04656929166916408
0.032521291746243644
0.04754247230463755
0.021032368379162215
0.03356750359364272
-0.039439627415295365
0.018272772593614164
-0.027774586074614728
-0.04702401725371718
-0.021424458846094042
-0.08400385960230336
0.010138561731591177
-0.06051713711730639
0.032785587879587186
0.005152988252689908
-0.009414938999043259
0.03709282397010957
-0.02059087231329392
-0.003524378752593465
-0.019345819942432278
-0.0404695641176886
-0.01698253302060554
-0.01682460400944936
-0.059179808669882185
-0.024705573565880747
-0.009974740111506787
-0.0095399187815415
0.020740422131087115
-0.04029508271013039
0.01481299197085717
-0.047719836241153606
-0.0005525472639217356
-0.0428440760185545
-0.012414122109427373
0.011346415419105887
0.0022798873221423393
0.023405340758015404
0.05623999010570821
0.07158555179590242
0.06654011174974496
0.08755301261860515
0.03542031157942312
0.004568877972947739
0.02069934178894526
0.05939546367227863
0.03859521368214207
0.019723858239084933
-0.008408034086837973
-0.0668852128193096
-0.040976347843209304
-0.021591403604674233
0.01751607020047042
-0.029886393725541582
-0.010584669766394985
-0.011091764730661431
-0.022822960965866054
-0.017769126164713482
-0.014124313771060245
-0.034830453476111564
0.03580356926615176
-0.008242381664500418
0.003667016640633426
-0.03999464453276373
0.01672752583336837
0.04997928137269972
0.035581631746416854
0.07994555165761855
