# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; EXRUS
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=9, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.044582373463544364
-0.04114417270032434
0.010332666005876505
0.08300531631513472
0.0616665557162989
0.004067401438462697
0.08642778231059761
0.0013165812676465202
-0.033358897097403084
-0.10381770474369843
-0.07697505438926097
-0.019226482158365293
-0.07557613060646061
-0.04681187705291262
-0.08491104923668484
-0.05294232506666309
0.004954657534181794
-0.041398038822493634
-0.04163819424921272
0.021458677883059694
0.058338725608218914
-0.023590088764736134
0.015908455068052146
0.060693301466488714
0.015062108155118131
-0.00874633031437096
-0.037807125116881245
-0.0016451474933886177
-0.1101112281691323
-0.07765717747694309
-0.000938073741752728
0.012100269047955334
-0.03371783913799202
-0.008990038774642904
0.07252852981746874
-0.008710418887703153
-0.07404371698036431
0.0694493841282907
-0.02764188174355297
-0.01055041029704449
0.05009686363362622
-0.022895066340061415
0.024792767646595226
0.006759977599668052
-0.04107535532698736
0.030665213179128948
-0.010383874243634593
-0.040556397447849314
0.03646149006360411
-0.022530285265196813
0.04299590230389779
-0.0032373622304417878
-0.019919371166441574
0.03488561305485503
0.012657972480847317
0.03643342409547516
0.045038395314074976
0.022455114659262587
0.04817490552318138
0.020745005581184126
0.0070254716809321684
-0.027449361582834335
0.00885751406099502
-0.024366892643337093
-0.04417641568370127
0.015538989358174
0.022198799860419297
0.027849500562334414
0.03785833077363849
0.03984980791761458
-0.011326925551410214
0.02371027419858545
-0.036156619913653466
0.015340903409869409
0.03802961281894516
-0.045687015973655216
-0.02821983171357753
0.004625910674791532
-0.08183602032305873
-0.010799577602649501
-0.024141609433012638
-0.0945054369526506
-0.03567104651882653
-0.028527814961823918
-0.022259987940956752
0.008653234606803211
0.012323253438978074
-0.007120565417583715
-0.001981230402804716
0.0036182596820725985
0.014064712382085197
-0.007600142705641985
-0.022701450301626406
0.03877547372194919
-0.03168004468343822
-0.03911268438767714
-0.014512832245117006
-0.02193307593243541
-0.044352263659500415
-0.034064705130669545
