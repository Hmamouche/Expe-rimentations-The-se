# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; GDP252
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=10, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.004698071739931682
0.00272692974397381
0.01331713090439983
0.0009666260360446672
0.0002588387414305862
0.006910263007465384
0.009200997371563447
0.005633826581812545
0.007328579450998817
0.008370231008643945
0.009568160697979557
0.004133992119454356
0.0012681837029084307
0.004425331743219248
0.007042874278367486
0.005164673613497277
0.005821914223361611
0.008891436798183527
0.0038663699630788508
0.008077512332765808
0.00707695467036331
0.011052458499792537
0.001983743492103461
-0.0022211385558288366
0.0010984056077495978
0.005297815277713464
-0.0012810886625339891
0.0039034173517074927
-0.00042576429593713095
-0.0044221576885367115
0.009713667705350476
0.0034859975657258856
0.004008908997325189
0.0028989894908308025
0.0020960617401372507
0.006084274048165883
0.0047059775183690596
0.008823403008697803
0.006040146523578326
0.0001120348686666447
0.006945713163387209
0.00920829458427399
0.010512215045238776
0.004789009919752743
0.007589908969239397
0.007989444477868054
0.00588071595425937
0.006341945718520037
0.007139800996049599
0.00502810367313039
0.002935324301448695
0.0020026963546454835
0.008345643016961262
0.0071868785724662505
0.007450167017402447
0.012096975846297929
0.008416212835137207
0.006605166203877268
0.012900979282537311
0.00877559073486298
0.009870692384478555
0.013328142894583629
0.010413424953956436
0.008361433081504728
0.009909986692649626
0.01450098116600856
0.011320544320544539
0.013282340695734866
0.01072398765444263
0.007463840794226584
0.00941056446326037
0.0083966420957216
0.0029184266994754992
0.007458812985846895
0.010561410167506391
0.010486929833511453
0.0034211276018572797
0.00788214443657972
0.0002557111790693699
0.00661703931638878
0.011368635676688698
0.007720775891758361
0.013925847621921571
0.004952172465557288
0.00827512460252933
0.009562720988626799
0.009487410986692315
0.009342609614952119
0.008594900577218382
0.008576238994493366
0.011715298218660913
0.004378841904055616
0.005718440034348375
0.012733916575724195
0.008270285223593753
0.007924788506788263
0.012541575198930439
0.002621323055049381
0.00035894529200106596
0.0041517163487629835
