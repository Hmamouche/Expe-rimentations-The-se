# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; LHU14
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=7, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.00766070335075332
-0.02405986216249551
-0.047788469548234756
-0.007411587058657559
0.05575266452957458
-0.030052216689870202
-0.046962010980543666
-0.02272803397540108
0.038865105052525245
-0.014112349479166406
-0.040330597336148896
0.0015437434779890721
0.05824574213568233
0.038530211527765604
-0.02834939850233713
-0.011852146241068243
-0.10452914240903931
0.009275816180219873
0.001432091403467407
-0.08603909051847393
-0.02919640464689142
-0.06746017315657198
0.040978051402244045
0.005807397842184588
0.02578029725540342
-0.012399011947371438
0.0255732597404504
0.01863463566808095
0.06947406449738638
0.09612958514866628
0.006656091401758295
0.04857384188785941
-0.025677741246064242
-0.020242530701480722
0.02979167499639369
0.027911672639010864
0.05556333484004895
-0.03412565893960909
-0.02855173487225772
0.04367112127096578
-0.062045118679910374
-0.05537269830698347
-0.02999415662016395
-0.009081849105671348
-0.0022203529816723928
-0.03745679812520002
-0.01220789482863268
0.027304280270421307
-0.024428555129260312
0.00411176670446162
0.023860617097507295
-0.01617001577491937
0.0017438440502080856
-0.02188634627083557
-0.03226443570723156
-0.025427007710538962
-0.03251768533609191
0.0020308153740809624
-0.027104657600450013
0.00028916020599469017
0.02611695133702943
-0.012652880928855696
-0.022873752573501133
0.00862915410369412
-0.031512374244314384
-0.06664864259204867
0.030843741895798155
0.0016883029417379507
0.01808750313455272
0.004094858257667507
0.04865509991885938
0.024632927022351526
0.048175834714354666
0.09090086839697732
-0.07026639021361714
0.05112255846414286
0.05094361787489018
0.02306379267954117
0.03862492827015771
0.006777315225997638
-0.04172106341116793
-0.05165947286443146
-0.027509612662459708
0.02728399179835938
-0.003976629688217154
-0.002043396234702505
-0.02178760458878326
-0.009797693792291758
0.009641682308192385
-0.018347272025047462
-0.04827048632206999
0.028316434972320947
0.019705622045379904
-0.03253382543070493
-0.014973933211219138
-0.004877517160219762
0.01584658985493824
0.05873406618566541
0.055847588309915076
0.04467380527763768
