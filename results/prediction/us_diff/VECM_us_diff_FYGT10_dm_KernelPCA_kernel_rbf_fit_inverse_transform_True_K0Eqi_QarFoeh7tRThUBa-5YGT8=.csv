# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FYGT10
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=6, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.059778928711759216
0.06248915908756295
0.11429497002229438
-0.023612886829645248
-0.002659233588746093
-0.051103483309488426
-0.035481084356431436
-0.059322443955221314
-0.006697632445372182
-0.045968944771250714
-0.06746003994980036
-0.024189075630185252
-0.09003578002167781
-0.00412366183833086
-0.03324456573125495
-0.013840257228101997
0.08336155380975295
0.057609279043518764
0.05627082201960151
-0.04564677309563019
0.014785000565656781
0.06501443178786316
0.013952644847929763
-0.019562943278010014
-0.03094413389747178
-0.022874162631365415
-0.0022359565192484523
0.005733271528684553
-0.029667583574435042
-0.04326395331960504
-0.00873114842597663
-0.0577502305399768
0.042427728280094654
0.050447552299799185
-0.007306470486454365
-0.02218712687247829
-0.06027373228793845
-0.008554830387659994
-0.043741353954568046
-0.06046549376183326
-0.008411311204433405
0.0004857068717943943
0.03377097257192516
0.057513177464731884
0.0676281578835995
0.06673630252652005
-0.009779878963475314
-0.02628213984938417
-0.012180810996315652
-0.02721842941654577
-0.018075184732222895
0.008180353015909206
0.017903386197244735
0.0005519012919336132
-0.009313001658236217
0.022193857403493007
0.03272617211066669
-0.01910708051128874
-0.006267415651982619
-0.04524985392245809
-0.015202725098962578
-0.047567317064269296
0.02271793622452086
-0.019010827462062615
0.010620836290604906
0.05972278011643751
0.021172582032694953
-0.015133625540925741
-0.007653418958165994
-0.030706309040069428
-0.020214248343116525
-0.006834300387390062
-0.019996690863637492
-0.054307178651600126
0.005364113541041517
-0.007953630764912671
-0.04177788689506201
-0.015857253663004472
-0.059998302391539846
-0.012602743802329446
0.05330453474928856
0.0317594869334175
0.058903218650200734
-0.014877103720419711
-0.030822381574313763
0.009113952244442752
-0.009171140942916246
-0.009375080149335085
0.0001573046503372845
0.021702905972520718
0.041493584146862515
0.01424923667571517
0.008263796909191838
-0.0027607020585038236
-0.02041612809051828
0.0308809187361113
0.032968578753167076
-0.06442441940666412
-0.05122083269265133
-0.010912907889187737
