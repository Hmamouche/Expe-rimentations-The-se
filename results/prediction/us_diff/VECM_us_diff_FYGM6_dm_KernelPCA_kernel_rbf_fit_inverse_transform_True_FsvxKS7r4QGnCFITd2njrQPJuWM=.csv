# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FYGM6
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=2, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.049691398608805205
-0.055972637749902285
0.043113103489277785
0.008519121539727541
-0.014375576884503152
-0.02676474052400673
0.03949988630449615
-0.037430708089799467
-0.025740635042448973
-0.07606854761157139
-0.045830260934648516
-0.03532534318892863
-0.025342075340850707
-0.0360570742003099
-0.012295901268068609
-0.03780266059800129
0.022418566720714867
0.001673347761804232
0.013944713638108775
0.03450579386032933
-0.015941333246111244
0.034081428973755996
0.0030426131370110514
0.012901325198949093
0.021374028800492467
0.006427521616822269
0.010371988349750689
-0.01976584103592798
-0.0420332535530911
-0.05013398911162252
-0.06589140103743674
0.00016501095994975613
-0.005536752985218546
-0.038051812444018875
-0.010392726942364497
-0.04950290220540722
-0.05213127717876659
-0.018094429684301368
-0.04618950684947204
-0.020362814238131046
-0.0024393157379962727
-0.001191621120539835
0.03187111820967421
0.04828075356456568
0.011380621701561676
0.07378557008796918
0.012605612827435258
0.017623975632112134
0.019936070314457092
-0.013181228312175865
-0.005112643607715753
0.01717064358003797
-0.015543671440326957
0.030749700347053776
-0.0018336201485998939
0.01605833951411106
0.017431262351906424
0.006434598224824258
-0.0053460583199679374
-0.015995679566915607
-0.03433554730535669
-0.01146311988697703
0.015001351179859922
-0.007168946681622013
0.012493600339363992
0.01157260816506642
0.016788231269925756
0.008063224729601661
-0.005613572090392219
-0.012883998324158357
-0.03434880935222902
-0.017038816522528456
-0.050364906217550236
-0.09534411062904878
-0.004039190769803497
-0.07441285743324502
-0.005855845291561808
-0.0732156918862332
-0.025737377261933397
-0.028378777993600095
0.0238370523349319
0.030723334071935938
0.03232127513300892
0.017839516549841793
-0.011823428298623453
-0.013477443378907664
0.026110095436009034
0.012285336728550842
0.05216677799075944
0.04343841684103687
0.04731025655746239
0.031547219705342966
0.004746372079276754
0.006366403820685079
0.008676953210912988
0.011604713091861
-0.0136268257555507
-0.023509916898827005
-0.06482690192217513
-0.04002891265757063
