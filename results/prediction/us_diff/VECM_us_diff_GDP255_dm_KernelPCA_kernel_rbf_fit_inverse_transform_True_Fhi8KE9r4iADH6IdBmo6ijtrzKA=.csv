# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; GDP255
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=12, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.0031426348551372805
0.007333569144445128
0.00331936273757311
0.0031494460289119095
0.0018961176105182524
0.006265360742499613
0.010968011204218099
0.009744225113598211
0.00905256651612859
0.0048477803987255035
0.003203749222642159
0.00528770528031367
0.0048349084693536295
0.002123203876325224
0.0073039055875551715
0.006673923548494382
0.0052076660885930545
0.006729678905625789
0.009663901284473714
0.007355030369000273
0.007438530080486889
0.009482799797614808
0.003109636684239392
0.0047070872090707256
0.0037561353394433864
0.007380683973492809
0.0022464428532732762
0.006472464368711549
0.005814120289244543
0.005608981600162093
0.0038741808818902367
0.0020217608250298457
0.005769306645682527
0.003816743289454709
0.0063878561965567925
0.005780972656618868
0.008752620242954052
0.007167196746316857
0.002577026116658047
0.0018468558640769408
0.002664413676144258
0.005169214922784274
0.0073097338055386095
0.007136205233908545
0.0038739820435347277
0.004676848769374234
0.0021012702528687903
0.005122839165052748
0.007053386254168568
0.00671068910981015
0.00826378526986588
0.006292664476041223
0.005055265752466618
0.004975363032412857
0.005820136503209586
0.0097058077024614
0.007404299192107531
0.009615143840893212
0.010237375648894381
0.009369664295880372
0.009614542139732838
0.006720596284083397
0.008197102132830706
0.0075928373941786175
0.009756854384596263
0.00935745275883815
0.012195343400310314
0.01087953372238541
0.010508960681021313
0.01035117784807078
0.009116905409591643
0.004676810285721361
0.0015074714616764026
0.006867612329429955
0.007221558889035094
0.005395609451115892
0.0027299276199277333
0.005842675474967171
-5.1701877996384064e-05
0.006106648091473971
0.00284396570909871
0.008644586992263843
0.010846447337638806
0.007786562346145605
0.008118682781953357
0.009222825379829916
0.006418309273099085
0.007214936072095305
0.006225119807912962
0.00624166343280894
0.008511253482679898
0.0065305166266322635
0.008155722242956104
0.01076628638958681
0.007793214538401308
0.007859124476497965
0.010040560103716099
0.005541100413878827
0.004994528685575297
0.003908848706268245
