# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; LHU27
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=5, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.01417796600396179
-0.14718976796968317
-0.10651896310064667
-0.038112653680334146
-0.02115309968258903
-0.02179958948556438
-0.03465548211443132
-0.004218303792532589
-0.0046005293481384026
-0.023886147475882073
-0.04174931660144121
0.009380166405081504
0.012558413485002971
0.004239646061583361
-0.028609827458779265
-0.024107639098300472
-0.03887641788155929
-0.03053652468499854
-0.03829932445368377
-0.003377174198705205
-0.037894058534302444
-0.020214299390527365
-0.03919022234471135
0.0015292001526226928
0.010500867700006591
0.01828019866069632
-0.0028514373307569982
0.0087798869704989
0.035779861082845886
0.05892336433030167
0.06493374318306969
0.051080168217482874
0.01209486581748557
0.04195479731410781
0.08562564298150462
0.09002757633843096
0.04811040980131789
0.017514109642625283
-0.03608268382576076
-0.025326352727827496
0.02803398165642241
0.008754779017518678
-0.04861762707216131
-0.05815570204072979
-0.04603988894087596
-0.028941392511465873
-0.06038074663348201
0.023190386593556732
-0.01812551765678297
-0.0044059156636614295
0.0037503380905052133
0.03796288925053516
-0.034349688770388846
-0.04568757452271478
-0.028154211542488546
-0.011458202524310415
-0.018534625268936532
-0.049028745010304985
-0.02212218484191963
-0.018289300519622236
0.01036414911095668
0.013734793831473415
-0.03227410940343197
-0.02004543358889662
-0.004093445466366434
-0.030071941897427193
-0.03936255851794154
-0.0028542070510758724
0.02802278602124842
-0.0016381004316757168
0.03925066884848278
0.03881599701446068
0.06057287256090804
0.09689462869787867
0.058032477980766065
0.04683223223987317
0.012572496415924832
0.047751001935690165
0.035043399630618685
0.06843694364443623
0.019912607035373606
-0.022880058184051765
-0.03064467244664562
-0.023069475715000915
-0.04343021857778408
-0.013480773443164179
-0.007202372770619208
-0.02715067786513033
-0.03998966778349697
-0.012007570762315564
-0.03704378508012062
-0.01016653817152512
0.009981897596854184
-0.03411750566666678
-0.019972866195296892
0.008106364111240789
0.03222441783428614
0.02658611044304323
0.02455731012909959
0.04192553620087304
