# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; HSFR
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=8, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.07458813538754702
0.01246678001642388
0.05303064505546748
-0.1564089016458185
-0.05709384807055238
0.05691221575400182
0.01077241409873635
0.042521248818881946
0.022636669959968114
-0.05798708679201291
0.07815805157478349
0.05747321747241228
-0.06539374244640839
0.012019097036940448
-0.06763314708302406
-0.0640942644438029
0.04863074182199066
-0.040037597497029544
0.02171236177696977
-0.13115436526499524
0.005192232188562282
0.046178877822395756
-0.062006944344143736
-0.022771883883303326
-0.0349341158512243
0.06314151026256205
0.0045601630491826015
-0.059215844043800814
-0.027024348655887643
-0.05380872038690263
-0.015538364519822217
0.04892616181724124
0.08995448635298259
-0.005342716904188791
-0.059242491917459686
0.005183626500015415
-0.02663160856489296
0.019853010040189696
0.050495081371546194
-0.08462074679958798
0.060741298180141366
0.05036700235884686
0.028114997904680587
-0.056881689205190135
0.08553783033733164
-0.08545665909701569
0.02050911175610632
0.007135864869554795
0.019934718981023218
0.04474867284628663
-0.006573325147556297
0.02309536621502493
0.011099523991450075
-0.029152241856963048
0.0487214253317454
-0.024677551744477277
0.04437716056439201
-0.01640259473008121
0.006394623756412069
-0.02148133211568122
0.026645048000265136
0.03657924732122695
0.019569445477132644
0.016214233191028264
-0.013658090117227418
-0.006386979816746324
-0.04621796018105735
-0.008636209125732711
-0.0037405223133803525
0.03046125251258696
0.02327941789543947
0.007409101169068405
0.038035287964440334
-0.0027195062048084445
0.07819744811925607
0.050883615410941406
-0.11299382634381105
0.010787776846263433
-0.04127835871542467
0.05074820793388395
0.03395875419585631
0.04599716512236351
0.0570148849343783
-0.04727123566973561
-0.00871544505402612
-0.031455398966601864
-0.02057893234941378
0.02770527700077294
0.025339447106189676
-0.02787894225169701
0.01478107768917248
-0.006272531332571891
-0.04653293404285399
-0.011261172261038866
-0.00795808486858476
-0.034021284268508215
-0.07378943221377916
-0.17111227662339834
-0.004716429462662735
-0.016849674742733328
