# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; HSMW
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=14, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.4305431746738754
-0.056994812224397805
-0.689151249269711
0.10522507271903891
-0.22391688388689668
0.3508563125551346
0.05164150643428145
-0.11653214465045054
-0.10460528754981518
0.014393879533829285
0.10112614600549123
0.15421491036991858
-0.01817552404912485
0.12569632335785633
-0.2950156042473224
0.10423164759267908
0.23898559218754006
-0.15352703948264895
0.07841810653857172
-0.15986462384160444
0.14040808789622258
-0.03670308284837435
-0.05864615318344267
0.06513415389161414
-0.16401910868844788
0.09660815602462007
0.15358904819884484
-0.1278701096238115
-0.06597753925432363
0.04505359267786024
-0.27984270424739394
0.11637263282847721
-0.03144846520151247
-0.005973254438383049
0.09058184787638224
0.08961278043565196
-0.029980008460821196
0.03748981395966875
0.05656016216228363
-0.1520719991129377
0.006414483838123167
-0.08180783959074046
0.13738922720672375
-0.15357030720260198
0.1784249780876466
-0.06530526816088253
0.1241164253004918
-0.13564223787160115
-0.013505920696500529
0.04309117543084319
0.02680060568903584
-0.05755700631496098
-0.05281520553873979
-0.07864940396178097
0.09899416988448093
-0.05012653236093529
0.07099348248151796
0.04360720024809267
-0.01201437491534895
-0.011484419209801831
0.04759352756065158
-0.05433514914058943
0.08891903888119661
-0.06842577199781681
-0.12905697759662893
-0.05243580165642741
0.034492110377776275
-0.013673176898010203
-0.061677302548815593
0.041284567586369975
-0.013848049082361397
-0.028870247256003657
-0.11911282288490262
-0.08221274285552217
0.09385810188342807
0.10011703146000557
-0.1667265399406852
0.03310644551936161
-0.05479599417806526
-0.01884806566687376
0.033381096964676255
0.049433181270602214
0.15116940771447618
-0.005810650637408401
-0.02843340120490511
-0.0848350020119086
0.01320047471253984
0.05367407359259738
-0.07752004311831023
0.025162524976530404
-0.05537107077192545
-0.09465727238893043
-0.025598832050663784
-0.00041605428137007905
-0.024878236034042562
-0.05468352725364323
0.01237060568346049
-0.051105318610132094
0.13241435312924926
-0.06289591017559139
