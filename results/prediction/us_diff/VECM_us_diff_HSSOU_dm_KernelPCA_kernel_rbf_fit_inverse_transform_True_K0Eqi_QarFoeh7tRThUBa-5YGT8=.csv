# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; HSSOU
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=6, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.02809842854838572
-0.04569005228670427
0.01619370705215792
-0.09222566178695286
-0.013193676818464709
0.04241273052493855
-0.02313480397887627
0.052842889804153095
-0.054095892904702715
0.006937920074585481
0.026206124617284155
0.05139496415669234
-0.09781972100295988
-0.04190300283392963
-0.08575888744726953
-0.16543688865578407
0.05010788679806896
-0.00527789984278957
-0.011690123626529752
-0.11086478260145607
0.026507538975711183
-0.00041200736263097195
-0.0430349949033256
0.021252876191941935
0.02459194823980338
0.028829465699227404
0.0008819766059648126
-0.047219671666084796
-0.04073544274350115
-0.030992256240021852
0.05780985213586356
0.05678173294777883
0.05135263270942265
-0.03411090172507987
-0.03203737875838555
-0.029756329632586766
0.0014223892054916016
0.017179618040673635
0.03885390602322197
-0.013406593524669207
0.037812109052200685
0.04602231412462223
-0.027677944757409395
-0.02423708550294792
0.01678028411722006
-0.08117667247680982
-0.0038045591638318846
0.025024151781771392
0.0648967527205552
0.07837320114529794
0.05232140666238712
-0.01505690855857158
0.00916216444710107
-0.021545131661659347
-0.03540169228701551
0.001253243180951818
0.014932336763290602
-0.0054317815238073275
0.008642214393523313
-0.008413203571077957
0.07511897131384052
0.08775780309902849
0.019839419470642286
0.00016061691038370476
-0.05537212604666415
-0.0705755687580362
-0.026943311235069815
-0.02051048675459221
0.020583463116078806
0.009655161620016502
0.06931977009287325
0.020148168974444592
0.0724712287082573
0.046533092965919634
0.023716135187183997
0.02183942596759078
-0.07275120961687079
-0.02179195126132029
-0.02573478274528642
0.01509605511068297
0.03889812696680738
0.03716797781920207
0.0029133352147200434
-0.06876087500635879
0.037705360493790446
-0.017226415288255303
0.02711365658023994
0.04923084647555294
0.035622217471007606
-0.010610818101712733
0.03519433939046289
-0.0010179140448541013
-0.04532475402588682
-0.016060298520498895
-0.017031164583706557
-0.04774187156468122
-0.10230828251950658
-0.12204173231109146
0.003820793948213913
-0.07491665226472322
