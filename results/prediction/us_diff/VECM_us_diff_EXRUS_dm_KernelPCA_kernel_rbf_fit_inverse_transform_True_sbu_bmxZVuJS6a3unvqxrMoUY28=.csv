# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; EXRUS
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=1, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.0460628213142472
-0.013991473548041998
-0.002696674535130634
0.0199033262054858
0.04891559814827948
0.025701591252318842
0.06664011809789812
0.027805250490796705
0.020615682738734546
-0.04510383252569936
-0.05349196832480924
-0.07957821127262447
-0.08168867951392139
-0.03920430331076555
-0.07483051812397604
-0.0635896339914257
-0.005387411142297177
-0.056982713868461474
-0.05146544426718981
-0.02018062840433358
0.028812384583670955
-0.03839947321261948
-0.004723758424539645
0.031202676640236605
0.014886462162554301
-0.008460877515538444
-0.0026403434281283863
-0.003538794356603706
-0.05067361732413508
-0.050422797975124736
-0.009989250854802682
0.03914388741570343
-0.015685711178338717
-0.03322868242841842
0.03121764141804652
-0.0034714630515879103
-0.0398934175563344
0.03867385335620266
0.008556512821459111
-0.007112858173568475
0.0336914491581629
0.012701629606517943
0.015699062114394122
-0.001835161166079428
-0.02345637815490654
0.00304670665333665
-0.028945178520567535
-0.0415706188310703
0.011323137942084805
-0.019003996747301098
0.020521039162748583
0.0050596145902183955
0.0026246173487349004
0.019991075825609033
0.029393515159702604
0.022894371317584563
0.03252543784016702
0.019602124605997546
0.03544938405338667
0.018896069360080352
0.02467482850912839
-0.027660489017270876
0.017686190460068245
-0.001356425364897138
-0.00944097699162356
-0.011536724084398334
0.020051818075465005
0.01730114144768116
0.018994104861827
0.03854388154142162
0.01184467339234804
0.051377545391551246
-0.004662160415997871
0.017116033415460412
0.034704973637219534
-0.0369118336901761
-0.012535522458455978
-0.02084446422176922
-0.0546305366682993
-0.03961673549637569
-0.03200853779419047
-0.06771417903402559
-0.03976641627680917
-0.012471682265629867
-0.03288674626807937
-0.023105553679329687
-0.0076090518836097
-0.008633595565112857
0.0006669355681098903
0.007270040152730348
0.009870493473376275
-0.012969657914343003
-0.0009798688294535406
-0.012008288843867185
-0.0032204609546463026
-0.02311235899670583
-0.02288295325883887
-0.034748015113723864
-0.03625950160086269
-0.027326975262574903
