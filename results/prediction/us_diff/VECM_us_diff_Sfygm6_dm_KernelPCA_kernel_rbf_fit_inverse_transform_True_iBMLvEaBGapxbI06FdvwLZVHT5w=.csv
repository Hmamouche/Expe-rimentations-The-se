# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; Sfygm6
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=15, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.018146305587975003
0.12514750689017437
0.5033803471630148
-0.11515972506079565
-0.12504044193469202
0.13339308831138297
-0.029122474806534127
0.3391659632584504
-0.43582517285478006
-0.13058624789790202
0.062128736106965524
-0.29167498117708995
-0.20007220254729907
0.11230979596785581
0.12474655645400258
-0.04385181188949163
0.056138611473134876
0.1855406184925934
-0.1832861957409409
0.05334699600765275
-0.13831285936029666
0.0504338198311968
-0.055281261796760316
0.14539088793466856
-0.05132485037021223
-0.062184131395046374
0.018599581396677015
0.08725367087897723
-0.0506728194342067
-0.05597572519138968
0.07474794128288938
0.15391952615376583
0.06046680908745514
0.11178195404711994
-0.010662314210275171
-0.07978337311240437
-0.11876911076339146
0.08842909978335624
0.22430899830976847
-0.05972293018644424
0.015611528365637714
-0.051015189540452685
0.08774683357050672
0.052661000083450524
0.007867474059205726
0.0591927574736114
-0.0016815622973537295
-0.10428788384443995
-0.0950333184675069
0.024722330980744556
-0.03540077947948921
0.15542421815019564
0.07164727016788239
-0.07040080396639713
0.10372329390292688
-0.07784481766079686
-0.17353909330849493
-0.07165582932372558
0.03755357805026559
-0.04881233555025442
0.04847830867146103
0.021386924021763627
0.01815060244389436
0.15678785018787833
0.0221499386442014
0.08892891516986914
-0.11217953157584688
-0.05123755203565843
0.016401053290831875
-0.013891716219875585
-0.0031822278867301643
0.005567516604936228
0.0658511134491824
0.16333547703600843
0.08100008838941682
-0.032776877790662046
-0.17321502410220868
0.0975758864195677
-0.1011163145958908
0.07458159149494917
0.015133916414211482
0.08797980156408342
0.09678062571188924
-0.10486813525842051
-0.06908709101107001
-0.07423741670364542
-0.00019927018030562252
0.12599070047824362
0.07203349998948554
-0.006588534702228956
0.07424120360673828
0.06384238288153482
-0.12264863521138665
-0.03580088765486425
-0.09685589470457828
-0.020384743815871048
-0.05916975224990196
-0.12284802746990489
0.09214959943239046
-0.039702210851665004
