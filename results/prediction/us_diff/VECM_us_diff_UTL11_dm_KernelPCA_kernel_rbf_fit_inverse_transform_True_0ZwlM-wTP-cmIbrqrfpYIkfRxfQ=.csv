# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; UTL11
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=13, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.13525713163488234
-0.048658686156724515
0.14959033735194044
-0.16927417395418914
-0.040709151556279605
-0.009525555964974203
0.005148805369951756
0.08712248738781914
0.039006686270748686
0.10023763704229693
0.15354481876800094
-0.06721545747276032
-0.09269502480568315
-0.036926706970901474
-0.001597439141020035
0.06040692182037535
0.038906557384666904
0.013932902908560349
0.005887435695899369
0.058229002855853244
0.0691836366753542
0.10971563856392602
-0.05330654094339998
-0.017530954652067332
-0.08030676047049772
-0.028435131868372286
0.02812759061671847
-0.00514788033225211
-0.09019896314635675
-0.08765974521329772
-0.049203637626056194
0.033547323211531054
0.08874020620395934
0.006930607783574357
0.015263092623371808
0.0161467024311985
-0.004237812064337437
0.05963134771779395
-0.02905997131081438
-0.013367061820598145
0.07089067914080982
0.03394313611240357
0.09530761722651755
0.007514311691969855
0.02410278360059334
0.02930474754765651
-0.024026675282404424
-0.006853315570290437
-0.05727737681174309
-0.03922456107888158
-0.04144927477090155
0.05280707399978409
0.028350383629244774
0.021318884186841844
0.02530195090514805
0.02460267704341209
0.034379972458196746
0.00811441929965738
-0.003977144743681533
-0.05046931340717989
-0.04899992901659051
-0.05588666780475743
0.0032346176259977263
-0.016859873637530783
0.006245536419073144
0.01904191277524376
-0.03441586072081886
0.0074547160002552335
-0.08756116699325592
-0.03884237616662524
-0.0915848520701972
-0.06535029535037114
-0.08107547116430208
-0.12046638578910034
0.03365942453370894
0.05480195724871442
-0.0025600781123356568
-0.03327643932257113
-0.0609389067316414
0.10146904595275748
0.004974451459174734
0.07474823594939613
0.053366923743483376
0.015734913457273386
0.02916213059642144
-0.01601843738521852
-0.018814737058792483
0.005562528328933499
0.020613577720554055
0.026313126168555995
0.04732668094624852
-0.011488500789974779
-0.04251242964334454
0.03811812513729209
0.03480485618823129
-0.010746495644094956
0.005523002633533874
-0.07880583429244133
-0.06117592978982926
-0.016430269898856098
