# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; LHU27
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=10, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.017727560861450213
-0.17137860752525463
-0.12396034432406108
0.0037863479100664725
-0.005131452058744818
-0.05307866322386033
-0.029955679075121484
-0.016558149939561652
0.01682675340189596
-0.027529472549626514
-0.033734781892270464
0.014189680518714992
0.016513273540380978
-0.01999083249847146
-0.012326599570102584
-0.012508272223753114
-0.058317843966515764
-0.02580874513010652
-0.039080322324454755
0.004191431516545812
-0.01973468165982205
-0.038629032441588516
-0.008594921267037411
-0.0163350517223083
0.018986032103410592
0.009976759191370244
0.006747527771709329
0.012964438282796613
0.05219064893581281
0.049918593494823006
0.07004835248159873
0.027080874523740864
0.013915486090949242
0.026885821889016018
0.12120621208832619
0.08627879334387728
0.04186644851325214
-0.006996458372811425
-0.07177819309259843
-0.030873018549609832
-0.009947702450006379
0.027643055230496642
-0.021469069901210044
-0.039350275944695104
-0.05860113840387695
-0.019817756560322225
-0.08789056854183801
0.01660279560367988
-0.0017644004246444448
0.01904719466942865
0.022776743843728284
0.03661103914957074
-0.03958617749074543
-0.057557929272967635
-0.03524142057073918
-0.014283994870261954
0.0009221211098406649
-0.046192311947522456
-0.002976410149257567
-0.03221647338804656
0.019944058533731657
0.023340448914028034
-0.033250860670993955
-0.017193001910150656
-0.019983845258760485
-0.038386935149495205
-0.020839996254398994
-0.018337852302094258
0.030820293971075836
-0.0017251041545262033
0.05087012364212855
0.02947082731033169
0.07913769327529989
0.10950734222560458
0.056842656284630616
0.03578828066918377
-0.030176559359780586
0.00552791452902009
0.05328014512137584
0.07761832345680672
0.03802887796646694
-0.019140610133257203
-0.009593503717782823
-0.023934413471604202
-0.06750082699008444
0.0081380280821238
0.006570689175608529
-0.04004378195509571
-0.04496086211962274
-0.01494549580160946
-0.038852152960147754
-0.012951988788918397
0.01578377486439467
-0.03440746877559264
-0.027377256242965615
0.007435295269119138
0.05756611066059346
0.03245148387064688
0.03330066847407667
0.026748060026269678
