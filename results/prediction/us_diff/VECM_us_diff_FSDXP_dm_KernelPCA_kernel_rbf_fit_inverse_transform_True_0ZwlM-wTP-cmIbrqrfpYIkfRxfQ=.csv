# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FSDXP
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=13, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.011657135656191392
0.18408098683987578
0.1090002509195049
0.16289127062528835
-0.07155415617030106
0.012792340862498253
-0.04396745101456781
0.13738691044581045
-0.11457651048609321
-0.21403137260814895
-0.23031275652254754
-0.10612914061548674
-0.028846125597060943
0.04203922861759996
0.007254107528127357
-0.028727300039599384
0.05950280987764245
0.07417340077971528
0.05038299681424964
0.024752666038932378
-0.022882284176706347
0.06478574570637444
-0.01373410950886591
-0.0006668769728598693
-0.013400467825965573
-0.018887758677564474
-0.007957877917828645
0.03697227673085266
-0.011416432637464743
-0.023846327092711107
-0.10735323050968901
0.05831377673438502
0.032560565137332714
-0.001420518333063725
-0.02833105526665891
-0.014548845397223184
-0.08323775960856354
0.020750939758639446
0.0026375226550888788
0.011666854610805508
0.010934943983450972
-0.01039388785555673
0.0491401395458559
0.06104470323791649
-0.0232971379986546
0.017763132337754856
0.029105960678400912
-0.029887891851207587
-0.07956396968954364
-0.0337760591061437
-0.020389288437642757
0.04715485897352184
-0.019917323363305765
0.004195965125474675
-0.07511085176180328
0.043955882790938774
-0.019407331298524913
-0.05388962996923715
-0.022895803224026783
-0.05185180164550429
-0.0017044009924266808
-0.06854013502770599
-0.010661355006554074
0.046720699222378106
-0.054520294982148526
0.0056157258916958615
-0.028029672652376478
-0.00716422038414518
-0.0031502185377150684
-0.05396940133593861
-0.0015665222662343825
-0.03513106740095694
0.04750141204638132
-0.0887840607191531
-0.0471712366048127
-0.003510803369982972
-0.054438454407326586
0.04823587917086815
0.08546501990851549
0.07432738163864472
-0.029669939030969508
0.019589880872201777
0.07300163966865128
0.055235883589941574
0.05614216149583317
-0.024266694411226038
-0.028566844713541945
0.003386302849805352
0.01191049453956628
0.024978234509370784
0.010486467102143847
0.015630728428433466
-0.04282709078215534
0.014538864464133475
0.016568244877237324
-0.00807357656572329
0.012729141579958903
0.008500653222303103
-0.02375481153850861
0.011982141940347173
