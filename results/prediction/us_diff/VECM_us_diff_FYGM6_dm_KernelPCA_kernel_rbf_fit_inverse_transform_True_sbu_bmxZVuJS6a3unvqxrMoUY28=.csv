# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FYGM6
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=1, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.026388962417169697
-0.062435817028830456
0.035373299191936144
-0.006381544370392364
-0.034301890482882305
-0.022388339511125285
0.02998409560573393
-0.035616804451464104
-0.027304833834440184
-0.07311356585792156
-0.04193804899518454
-0.03471228015973139
-0.0358452611308128
-0.0409865040713901
-0.022051916393640205
-0.021349494809611052
0.011164125657669922
0.013009261941497343
-0.009252204837721139
0.03908522991289866
-0.015296640402342278
0.03442466788220524
0.01709095410177018
0.008241351724930299
0.023800844803413265
0.009725538717673888
0.013183307432145314
-0.01656394689064109
-0.03531740571891802
-0.04965391029545994
-0.05929778842786807
0.0023553451324711604
-0.009798333211132465
-0.028817246338188364
-0.02066590279040565
-0.05908960754193283
-0.05508088668703445
-0.028373082790249644
-0.054825004478806075
-0.014620498147763094
-0.01414917614985748
0.013428523060917576
0.02784831469901687
0.05436798503978873
0.009601875488229838
0.06818166176222382
0.007381050759382866
0.01986333751019902
0.016586372406653106
-0.010746887538649275
0.0027162504775830112
0.016825383328375763
-0.01201692277808811
0.027203902006727378
-0.0025173857636834347
0.010790993132094428
0.010457707298187267
0.008258652418405033
-0.012978335596203247
-0.014034687252470871
-0.03422552260312309
-0.014048643778598257
0.016574108036367594
-0.005640329169469873
0.013143928502315619
0.011753212673237957
0.01604198683092516
0.003195320476724801
-0.002881314459443887
-0.011240075990883693
-0.03366544782734153
-0.006591204925858525
-0.055861022972905426
-0.08842567720279315
-0.014194292458704592
-0.06813986733101146
-0.019854943227270758
-0.05768658506978099
-0.043037933344770296
-0.011111414840491149
0.01253225915374643
0.034010393950560505
0.026012704560947168
0.015882832890878922
-0.022935457072256152
-0.004922684822177033
0.01777545057918834
0.017944560280585817
0.05787337542124907
0.042551920648424946
0.05508953981076267
0.02760128648950881
0.01133879211547098
0.0038267172533175374
0.013216439600944803
0.018491591806457257
-0.013375646738555817
-0.025499466152717146
-0.06466472781247604
-0.041689559823966466
