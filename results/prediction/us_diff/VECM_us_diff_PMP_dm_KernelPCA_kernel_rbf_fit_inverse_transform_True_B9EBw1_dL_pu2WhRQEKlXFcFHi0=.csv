# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; PMP
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=10, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.235653417129093
-0.13934398899051087
0.0981868118988
-0.3628205754710765
-0.1223658047889391
-0.039658299892470834
0.06808540751013208
0.15801285609251553
0.06246623199742324
0.04130672135415919
0.17837837364661538
0.03673180627213423
-0.04763753641118336
0.0037341180678969743
0.01455433789002022
0.0020482094340198605
-0.03629614549407932
-0.12445057042757042
0.1268133159591487
0.005670738403737677
0.08204480454766651
0.1011820684564017
-0.16454693950686908
-0.09208955452208661
-0.022025874616244234
0.02110600279710311
0.022693710867149962
-1.6403339047664847e-05
0.0346451036627161
0.04808760114679806
0.06084010514327445
0.031126858381729704
0.01170186068587756
0.02975401132402673
0.10749478419905717
0.0726648716004043
-0.042317092527110495
0.022383286870323454
-0.15692132851140803
-0.1391815947361846
-0.028974056666584667
0.08571974472222219
0.07194303316069663
-0.04974203679412208
0.060731338720973714
-0.08064769923836648
-0.1661498715980109
0.12600044389558535
-0.021852044268180028
-0.011791872622020957
-0.06557326846485673
0.004470134426610022
-0.05660964310652371
0.10095672650516477
0.043089881024781
0.057955298245520225
0.05832489493519359
-0.03124796184846622
0.007710717964303856
-0.06796145239042185
-0.06361871412548378
-0.09366036851587733
0.10038850788427343
-0.00043799352597433117
0.11657923666475359
0.09813816831560146
-0.005100365074863708
-0.1388000139108499
-0.020296439070346107
-0.1195860965100787
0.004492020384730809
0.08480048915976858
-0.011726030460490519
0.07885858309458106
0.1978082786960501
0.06823628749310405
-0.09612259694205907
0.037874236365606814
-0.18218501143861976
0.09511831482876418
0.06551717633552757
-0.03270200234813625
0.004870368293392317
-0.035939922483189446
-0.0483468848783357
0.022088676137138
-0.12969961352820106
-0.049728832993982834
0.0019163145169404754
-0.09349945999321238
0.15714399001396034
-0.04485186246243365
-0.043957186718401646
0.0280866189007596
0.03352795256775523
-0.009193769139808773
0.07561563119594153
-0.008124520255973463
-0.13675494508259356
0.057402259867557585
