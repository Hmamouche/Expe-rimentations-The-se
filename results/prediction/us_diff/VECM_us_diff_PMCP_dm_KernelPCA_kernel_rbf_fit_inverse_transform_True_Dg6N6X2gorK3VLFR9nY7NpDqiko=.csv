# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; PMCP
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=3, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.04475132801793104
-0.009872218938336327
-0.03141676340855884
-0.013409076675719982
-0.14190863533118675
-0.04502940099560988
0.02535365799621129
-0.046380294092195684
-0.05682872767316841
0.03505944957510538
0.02023697639230193
0.027465232126080374
-0.012679079724919702
-0.034616430518933275
0.07017717827383198
0.04519276564403044
0.07178401904338431
0.04826814939627353
0.032297398896103974
0.09501501782174968
0.044475525293698207
-0.014831228529243184
-0.04078583250250549
-0.13477615279952615
-0.11361433129306983
-0.06644175939585786
-0.026576414444001956
-0.06466890986737006
-0.00772653502667711
0.055896288595137296
0.014707134101597328
0.11826005986042078
0.02749991072579002
-0.008557758933409626
-0.0008863830703087078
0.026002863688155325
-0.059782032588577685
0.007598257902486985
0.011895896695746346
-0.08143321733854628
0.036241019154100856
0.06024562812705934
0.07431855401143554
0.061352387256572535
0.0560867954798733
0.06440093863697732
0.021447304195467302
-0.07019173488718905
-0.027945583343689344
-0.10983534517037735
-0.11507585064266763
0.05641971676084905
-0.06462523954610852
0.005863848171525691
0.05114232161449741
-0.013944364421030017
0.04197489262456522
-0.010470626117675921
-0.03278661261220124
-0.08503935761758782
-0.0648238595900393
-0.05063209325054833
0.03264011741439071
0.06880029871449578
0.08927926418887074
0.128760197729001
0.07607047133859783
-0.028135759446912895
-0.03777679459268395
-0.06385302868715774
-0.030052826361751564
-0.00544098430872136
-0.030215736020878767
-0.09124070865294477
0.14662278915791588
0.07079272316333665
-0.03032552988072281
0.027810669962247823
0.01783163254239504
-0.0024233393531654657
0.0802092172707452
0.08405992642040735
0.08578967074028952
0.00985661375421763
-0.05569451459339002
-0.029206117608807633
-0.01668756797587886
-0.10405239327712282
0.010295948001300676
0.03736043721192733
-0.05776226467169161
0.025896101740869062
-0.05226368781230622
-0.0944339323885586
0.01590815012939853
0.050552485215898384
-0.01893986659018993
0.012231775477236674
0.03063764925477762
0.09255337265369343
