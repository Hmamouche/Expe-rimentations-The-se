# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FSPXE
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=13, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.025215599791734482
-0.15048967742362912
0.005234781296738621
-0.028389014800494904
-0.012029415040808285
0.0005794603439305858
0.05280460160583629
0.0035037993300816896
-0.008309758259596994
0.1256521927657606
0.10630306662365062
0.05289052071839169
-0.01918741658148977
-0.0021529637537533834
0.07092211742212687
0.0010402574347776715
0.028579128219841515
-0.06871661508853288
-0.08551449487749505
0.03630077673601112
-0.06407042666064505
-0.03813300739324454
0.0022954786833774728
-0.02110722225978967
-0.03142932098137306
0.02419572308753025
-0.003378369189003354
0.011434875034614093
0.017526683251730266
0.0022931498425924814
0.09152063056897429
-0.019891503201930227
0.004208649219096625
0.028538520499785093
0.050638630970945966
0.010274896355349072
0.047847535811646136
0.04510308740760616
-0.0031926690484306453
-0.007317074180873007
0.004711486172650928
-0.021038697563752762
-0.06234618211153156
-0.03692168960305112
0.001133181821559118
-0.057302874724212215
-0.04815987807085273
-0.021075478366925523
0.025869295262497122
0.011421250746900225
0.018975798302884598
-0.011924641873397819
-0.003813832056818137
-0.0025832564290876846
0.03297448429538305
-0.020378488987150143
0.040626686121202765
0.030286577774818246
0.033485306655918465
0.0714056966442601
0.020645777696524724
0.07043463683545277
0.05888395287481951
0.0026847688145007816
0.0037635347587214292
0.0173472421124244
0.007514011998184859
-0.007712589365366859
-0.0194024536004111
-0.01877965945020576
-0.004962919818527662
0.011613843928186035
0.024871187042334243
0.10045001769344675
-0.09358552785939256
-0.0834378138818451
0.31511848366428713
0.26069950604059067
-0.011411524461974597
-0.01868883085718151
-0.029871061113459836
-0.0034075634262203253
-0.07752630582327612
-0.13897968205936506
-0.10208346705762861
-0.04440147946367247
-0.00017452096925007002
-0.04281935009091872
-0.0230826970459527
-0.030217518003696042
0.019091671505977112
-0.07578328730745279
0.0008144635641951364
-0.016351630041848576
-0.04854300996231469
0.00535088968321471
-0.010836337057460562
-0.027699344573308624
0.017356054686596907
0.01590357862119824
