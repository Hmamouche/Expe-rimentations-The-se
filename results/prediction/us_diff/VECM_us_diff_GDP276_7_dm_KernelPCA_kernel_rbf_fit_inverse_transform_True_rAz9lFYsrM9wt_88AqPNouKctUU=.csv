# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; GDP276_7
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=5, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.00517900514754326
0.006506084136952547
0.005306235039595517
0.007090632705321611
0.006090920779959971
0.007142248027302879
0.0027357652344995537
0.007032749944126434
0.004402357907269687
0.00397130951139905
0.006638462349108399
0.005531545041749541
0.0051556261439212655
0.004923579471972077
0.006129291455257398
0.005931901647938984
0.006669016270264037
0.006434094635132067
0.0038354940026856426
0.0070872009067737265
0.004925817286711641
0.007639759557021428
0.008465323699508056
0.006837619422898069
0.0058598609409100865
0.006802891524128413
0.008214912494532906
0.009077255517809472
0.010536597756852329
0.006778952147820068
0.008954431372686946
0.010164636407199456
0.009557832779056485
0.005679905145044116
0.0036861197040789304
0.007175580827825205
0.006887364600469206
0.005571152409966979
0.004911617792776978
0.006015487526168513
0.006445250270619937
0.005755240119948198
0.005040043760916999
0.0032467547571981515
0.0029702547912564973
0.0037906062004084183
0.004440250728756256
0.004440692427578013
0.004641431153843499
0.005567560455168836
0.006196397210053849
0.00766349338542746
0.0075548307082205675
0.006195531220646339
0.006111151716785453
0.0063788498307360934
0.00658275628771385
0.005291126897576493
0.005264455716023085
0.00633089161709058
0.0054381037934415006
0.00483441715994464
0.007119170927364795
0.0076262323940136465
0.006496437253131705
0.0075493973609004755
0.008122387216477395
0.008399529536903459
0.008777607881651403
0.00897472045370158
0.0072214165581587254
0.008445910949777044
0.009425243477650952
0.005335281050823605
0.006948335617066297
0.008687952111228013
0.006976686172143994
0.00849724762059827
0.007700265673805957
0.007768790858819055
0.005762582033641252
0.00468887352504301
0.006253635584112996
0.007208977803639566
0.007591133906487833
0.006908977872687296
0.007553730051936842
0.0063423142213808415
0.007699449139309914
0.010219701910094789
0.00780436163319233
0.007553767448874714
0.010475132906103246
0.006116452405362717
0.001039374830894382
0.004575617050372149
0.00396797928612789
0.006835025879778286
0.0073080544867495795
0.010422419512587548
