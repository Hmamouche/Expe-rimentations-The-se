# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; CES017
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=15, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.06806045798928356
0.08332379342340893
0.015487918964499262
-0.062112910100165855
-0.042813003200384205
0.01931617333997164
0.025008935416994768
-0.014201746116497823
0.05642212983298919
0.0048804902642652385
0.0538995810690502
-0.04154654507076624
-0.04980744060943239
-0.05908788519870314
0.014756182956929194
0.027609588359557926
-0.0028398533480530853
-0.009201254715927362
0.045354429015490846
0.05686850922473162
-0.005883818686565593
0.040800056463023066
-0.010872950355887276
-0.016436948213730148
-0.04107574477651947
-0.013467260378911203
-0.02182489331053443
-0.004887057888062825
-0.05879626065336259
-0.045733831017584395
-0.05818106198145664
0.009043950941670006
0.017154245392391774
-0.026436009351856533
0.00038199766393445496
-0.008954030357732342
-0.03779906945962249
0.027651809346542194
-0.021091112010496644
-0.024551912446338888
0.04310624613143968
-0.010090471151744652
0.03452333243654949
0.01138422106117748
0.012310656760164873
0.031101110892226124
-0.004805106448174695
0.003181049084846877
-0.0016446254858294056
-0.01756262026566803
-0.009397857254216012
0.03214194155007296
0.020209761446534993
0.02894308855999274
0.014213154423540007
0.009984221254908622
0.02543988186691918
0.019354822199071836
0.026041774808475415
-0.002147796841458597
-0.03357233081554586
-0.02105069581567755
0.027738584361856597
-0.02165000815945583
0.013566112793896688
0.004195433558483335
-0.03956656467503111
0.021374016812076478
-0.016786416111309623
-0.008625146196667717
-0.03574356608382084
-0.03246179135914833
-0.0742837864985113
-0.07903935135440526
-0.033274526129902934
0.007169953197222842
-0.025291073308960205
-0.06524935250584746
-0.04815228665284571
-0.0021637259047047035
-0.015057759818398566
-5.786084327775651e-05
-0.006884358249678338
-0.005077350995647928
-0.013702454997321981
0.004747708188754375
-0.009759219349301086
-7.57753131625568e-05
0.010165998799985235
-0.006304561216549033
0.011148519183151111
-0.016603658206129072
-0.017162051275839592
0.002893483551488066
-0.0010292209721392733
-0.016585832285267887
-0.021381792823968133
-0.042499955028318596
-0.05370143877288729
9.946251306324622e-05
