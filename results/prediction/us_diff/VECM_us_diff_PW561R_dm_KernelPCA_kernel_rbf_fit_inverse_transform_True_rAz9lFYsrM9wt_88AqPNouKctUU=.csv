# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; PW561R
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=5, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.010934192994009495
-0.07137443811832565
-0.024002269053133372
0.014935001422559093
-0.023919798275726267
-0.007314983582720098
-0.009570664676158386
0.0006633470627878391
-0.026017182772157692
-0.00160959713774405
-0.04864162340569803
-0.12028289789040025
0.0030127445158889404
-0.044541767468715056
-0.03361202072842426
-0.03014197453531022
-0.004900954554485641
0.010462346010898259
-0.022657643878860528
0.02583824121592622
-0.006190977638608837
-0.04197626047252587
0.02749220865370708
0.019389234931810134
-0.03194533924112136
0.030674532092442565
0.02624425080038477
-0.052182800112645075
0.022986906202570732
0.07061977156972721
-0.061914305380558074
0.05636475098991002
0.005455898352742461
0.013495446111074026
-0.07724108560179788
0.03656372692189448
-0.027869255413130498
0.003302002357198506
0.009162939105128283
-0.024564890653004403
-0.023218507762170937
-0.0038476376384691856
-0.019811892092893024
0.02173438773066918
-0.012315880232232752
-0.00029222673420977764
0.006081631622970731
0.004088078452862333
-0.012669111692338416
0.016299996139822213
0.00147654316726067
0.015270437301864236
0.006103043251955239
0.03038837818085726
-0.006711174589474912
0.0017846796312259209
0.00022075436486265884
-0.01380132239029138
-0.04079247295904575
-0.009844775662407296
-0.03042408444710577
-0.028014519545437322
-0.0009386125449762899
0.04065676872140141
-0.0002186522043528304
0.04140501107239317
0.048867704421198616
0.011014245037763703
0.043524270320751804
0.003439035108406074
-0.023076932687209832
0.006398918987331284
0.00347686449126628
-0.05477312415656379
0.014930110247589282
-0.0062336716735635084
-0.02091213994854523
0.0009936932281785493
0.03700703612520722
-0.02304698145732862
0.05034577337963743
-0.003982631820961988
0.032595314330670526
0.0024811030608225156
0.026170790510445234
0.027902907063018612
0.025923150480520792
0.039666733517287646
0.05387888680824627
0.0050433824235062525
0.060031397198665654
0.03641123228963411
0.014186064070922344
-0.016508700170556587
0.01515526045006403
-0.02737554883453491
0.017058199342604417
0.05744344480827844
0.07684740218315507
0.1752324503482013
