# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FYGM6
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=5, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.10955112551032159
-0.09047761756245802
0.1311207558933161
0.016303879567190777
-0.045004532126184396
-0.046329579578623714
0.056261119283711275
-0.052683447042501025
-0.06971822574591288
-0.058199577086699116
-0.04230572587600801
-0.008856870062340923
-0.08175610284316437
-0.025751553370370297
-0.02830693728006066
-0.019313483872900234
0.039329734162263486
-0.026545605771706022
0.06210146052468382
0.012388349324291097
-0.0030882456915985288
0.035172945378828535
0.039225527879454734
-0.02067017162709141
0.023833664584965934
0.021513896789015868
-0.018618449180739148
-0.008479447602527524
-0.09350934591070352
-0.03775814866285036
-0.025762126279031513
-0.008461008184747713
-0.0032007312037787526
0.013293798563171012
0.0018696307533689893
-0.04083413306700189
-0.07865643160078971
-0.004548541911859261
-0.003651891645262896
-0.06401703085933369
0.03072155904543706
0.0062481448971177436
0.019796737769430195
0.023979243861527874
0.02220730718680436
0.05647036245814688
0.0173005458401206
0.003755478532855794
0.018198592559161127
0.011857149226575426
-0.02668600614432169
0.0031368986910312677
-0.0018635136495563798
0.025171042653841433
-0.004583676131638596
0.03477269590677969
0.016499798002909138
0.009922820867911481
0.009270963213617478
-0.03339740917515067
-0.027834790530531745
-0.04214781648095562
0.04807174751463962
-0.0440087807821617
-0.008639611350769079
0.05277012508520925
0.011487963020702732
0.0022123858088872687
-0.008717192875633453
-0.0013580160197001548
-0.04733297319902458
-0.01029743912672429
-0.046957739451891835
-0.1145247851894098
0.037529126058232685
-0.07661509248921378
-0.018046605165920097
-0.024002346211239915
-0.051739242820749
0.0012644620726184382
0.017959577574871162
0.04403336455549672
0.03660689303375963
0.0014787006280539937
-0.008423942866177777
0.0011631510837620523
0.010834700870957795
0.0034241607252830024
0.03418695993271216
0.025389868349432215
0.05746477097825498
0.013972450579831893
0.006434154838062783
0.015466406983423279
0.034451197126020246
0.00024691261648086826
0.008315590999885861
-0.03846885471330555
-0.06600668733793431
-0.03568915876590757
