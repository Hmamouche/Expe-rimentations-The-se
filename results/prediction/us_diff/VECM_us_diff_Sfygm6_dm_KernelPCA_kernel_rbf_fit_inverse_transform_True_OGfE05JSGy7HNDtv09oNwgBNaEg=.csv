# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; Sfygm6
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=7, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.004511655240486634
0.15865067116277565
0.03586293569763024
0.08955910149867978
0.0340559433254609
0.08388146507758935
-0.07343080666006045
-0.04756630939528872
-0.13517627507517532
-0.05140863032059349
-0.06715741940409822
-0.06431119128679645
0.0007474814383981974
0.11449644071919224
-0.0017960188411978387
0.07440306554051752
0.08281139793424584
-0.0014826779183725013
-0.008027912999335574
0.10705088864729088
-0.0846446329152612
0.028156228769957888
-0.02439916726653602
0.05003745550195775
-0.06666671522209122
-0.11767307269757449
-0.04532121155066491
0.058949542229765894
0.020949295475838986
-0.053622317323451185
0.11058453275821983
-0.06270252577828617
0.025460770341256586
0.12315425724233672
-0.03418782218092165
-0.015058364607106307
-0.10657358708389413
0.06369051356895036
0.1424280123824705
-0.026954735246220718
0.046740405582751755
-0.04734058841245058
-0.0056609223339213165
0.13510834394861687
0.04147511161163138
0.022757960459061245
0.041141615309134814
-0.09789488706445537
-0.02813717263115347
-0.07244925674715871
-0.05976855862435387
0.06892875590079786
0.09992628675187638
-0.08931946034660732
0.021866260545328704
0.011925342943754622
-0.050779178434260816
-0.014426538893961355
0.02075950446739309
-0.01234877776546495
-0.03201322479616007
-0.02444666341060632
0.04239259123855203
0.045756908723135994
-0.0047987339119003455
0.049071865519232774
-0.03664756252933085
-0.040439778578528665
0.011348106437750138
0.008185632984770158
0.03319793321611037
-0.0024480768797086796
-0.014408131991543022
0.011718253879591169
-0.0031366760996835025
-0.021803768457297597
-0.15580215867452996
0.047763876127590366
-0.0611140818002471
0.044777188642616476
0.10261859892028861
0.0329627055130688
-0.013882852449736072
-0.015323958677038993
0.0007516852344596307
-0.05338353886259169
0.05710195954462202
0.08801688551971802
-0.01017869553136725
0.009086498794340182
0.058967491112975744
-0.005742222961117989
-0.02686647133459329
0.020937579793055563
-0.12308185612703856
0.01822423972367851
0.01987389278085263
-0.04046228253466294
-0.004607849837406613
0.028276911507669183
