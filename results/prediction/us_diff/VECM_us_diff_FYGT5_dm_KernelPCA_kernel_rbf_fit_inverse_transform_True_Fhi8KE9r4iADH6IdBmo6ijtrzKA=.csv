# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FYGT5
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=12, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.11484510416386275
0.12835748678183773
0.037315378372450365
-0.10268406030595395
-0.024830602613352865
-0.028400958865374075
-0.038428073127279416
-0.07644194059054003
-0.01908110939261464
0.03468126869638261
-0.06062712580632873
-0.07839450954158186
-0.07759819880847635
0.009514712253790358
-0.03413544949929299
0.048860324252926496
0.05149207704323803
-0.004754986364508721
0.027899680132361663
-0.05402938093221282
-0.02224787593912589
0.07912832444646052
-0.01328839850279808
-0.0598186067918384
-0.0555246699231778
0.016182662186308483
0.00880092621512514
0.013076896710594768
-0.04505445548705072
0.03609647379056388
-0.06471790898400519
-0.087416355744486
0.07109786654379613
-0.018788006603362148
-0.004099432115694067
-0.009362288223961866
-0.06780592004520497
0.0040982579703761415
0.00817609302355432
-0.00599844118157454
0.09430596966504562
-0.013026574564649531
0.014838579920037348
0.02018095609226083
0.06063631484408215
0.12046413970933939
-0.05744729650677689
-0.03354586822408208
-0.022802266629606358
-0.03968093098186393
-0.02459653920071183
0.03282073864864116
0.061360648567307025
-0.0001833102758977581
-0.01734682468791248
-0.02652961535874939
0.02268801916510067
-0.029688417753223187
-0.029203793192927603
-0.010383928313191394
-0.0073542800670678985
-0.02622297108514655
0.09117801300117229
-0.025177173393561986
0.031924013409200556
0.04498243494331186
-0.029275327070811607
-0.05206910413333252
-0.02181400824838215
-0.030802234522257176
-0.05525113395448336
0.01117099077510506
-0.024340667657682235
-0.015697055498054856
0.06601411091300739
-0.011133169871368686
-0.04413347814963187
-0.017703120804637318
-0.0777804639897523
0.010687589743165062
0.03567139382204815
-0.0008510006931830103
0.01735348330511253
-0.0563721599485599
-0.06302099647393102
0.01554937624289478
-0.017384639062632413
-0.01861515453585353
0.04157146474453206
0.021873200469294904
0.03467957625100395
0.03343965012861797
-0.008962545379747056
-0.04485731727201527
-0.007190960379021668
-0.008322871527437745
-0.005666273226032899
-0.08207941330418517
-0.08298789681137246
0.01060960371317224
