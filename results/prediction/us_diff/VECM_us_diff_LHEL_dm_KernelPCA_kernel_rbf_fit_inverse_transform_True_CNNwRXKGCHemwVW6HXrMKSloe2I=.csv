# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; LHEL
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=8, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.006227572214977356
0.03731553792160995
0.07447231761184268
-0.0024849352385903634
0.026860476545770007
0.058914784163514276
0.05342225688885225
0.03478520655666765
0.012642232501187615
-0.015480307900892262
0.017062920038256546
-0.0070287929891645565
-0.027941886300866456
0.003195506003660348
0.0012555479733755946
0.025071845470856554
0.041713512719065454
0.007441773892422425
0.015159841894041766
-0.005400091350126226
0.023715786848916065
0.03079964378939467
-0.05809620303523076
-0.03277463106465222
-0.02833187883282782
0.023346332823186926
-0.02718137265052794
-0.07682129611884143
-0.07720571032497406
-0.13076802518386726
-0.07159361501320131
-0.009541181158299187
0.013106320638298679
0.0005046181976803843
0.03012509580471574
-0.0006926102101083415
-0.019930523018323505
0.02235435957256036
0.013610181287600038
-0.0031748827957897817
0.04142676749280923
0.06352056360298708
0.08224455286645685
-0.011824692791968103
0.022915007977595412
-0.003288610409727121
0.003309770154238407
0.011936010983868015
0.025922943996541638
-0.006381433757414259
-0.013550074889736825
-0.023813519361170002
-0.01589953048223483
0.013025063361969205
0.05161608419489562
0.014121171805268598
0.022693634490019918
-0.004002177283702835
0.02725550378355427
-0.00214478304570665
-0.02489897306855314
-0.019326572691053874
0.025982828099122522
-0.04615014510807206
0.008150956433293513
-0.004554741381884837
-0.023520465846775786
-0.03553357175382677
-0.033485292413680444
-0.026007827170705624
-0.05814572815126921
-0.08707607253461067
-0.09139834067229348
-0.10515232430059387
-0.011907009310271457
-0.011494349587931843
-0.02633768183563358
-0.006457475390228
-0.07400808698567266
-0.00542571737392171
0.005913726187269161
0.0399090472194297
-0.029580113166265867
-0.03301318603576555
-0.008422934096256432
-0.026981948100361437
-0.01623103548731493
0.0007354712398985365
-0.0018852250169228198
-0.013396800146247315
0.01491746496689084
-0.08086628202084989
-0.03955848501684634
0.0025411604717176777
-0.00413968479271534
-0.0002579120133606883
-0.017244757076174684
-0.08184533387025023
-0.024820746641629976
-0.034783534330383205
