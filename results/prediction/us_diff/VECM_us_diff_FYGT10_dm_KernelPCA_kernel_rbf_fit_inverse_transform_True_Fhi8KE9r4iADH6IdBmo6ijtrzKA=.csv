# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FYGT10
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=12, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.07616220192362226
0.1507483420999855
0.03577702740300897
-0.10837259400190027
-0.009788255566491568
-0.016009051740954697
-0.04800805912874351
-0.0770526191337624
-0.023855761533728585
0.047683140912920696
-0.0613717844511756
-0.09460494962171717
-0.07951073344262936
-0.005742752915508565
-0.02672133282527725
0.06300756452543778
0.05368640188955971
-0.009100902723707567
0.020764488139390913
-0.04306378116505452
-0.05401643715799295
0.04744993991612874
0.0124257168615702
-0.09956956692558183
-0.04269399287387324
0.01515095694620975
-0.0010869279848442492
0.04359352445739518
-0.04746458954037368
0.03088717561451712
-0.0394293624042076
-0.09786021273977763
0.09161973348797917
0.007895712438559077
-0.04169440674567399
-0.0001807010405925763
-0.05606540649531943
0.02493042273709134
0.002546242789126664
-0.03186811138964959
0.009127860584619864
-0.007607417689467393
0.03906881937090005
0.03604718688230034
0.04983383450663814
0.09004674464752833
-0.07177761499534482
-0.03263919986237133
-0.036974763320609116
-0.022640393540609968
-0.019565137269255913
0.03703558993270144
0.0672009167090161
-0.002359719276518674
-0.03653318202518264
-0.02678569802058895
0.004405546735106226
-0.0327692479067789
-0.01892756469688934
-0.02241931004359952
-0.0054833971975487075
-0.029721262367419437
0.0958492327191262
-0.01869394041805076
0.01673265978802144
0.059401992627602915
-0.04872086469373474
-0.06629805127833538
-0.02275045064534177
-0.02841624856969046
-0.01701657251953613
0.0328920454267741
-0.024216625295773318
0.003958574636105449
0.0474614043685376
-0.00221712010762435
-0.05103803041958349
0.004573969046661533
-0.1003278603307662
0.037146806988623944
0.02304853634136006
0.0037444479422543836
-0.009914047284396335
-0.07655450746608324
-0.0608892921556657
0.000552152777772031
-0.03526986434936808
-0.023578703836656025
0.012902673009787554
0.03420860812325724
0.04717314972910227
0.05010876158683786
-0.009167324174512381
-0.0222409814098437
0.005336965691551558
-0.037641861461440856
0.010780943322529246
-0.0734686803261089
-0.062397804450753214
-0.013612450668816615
