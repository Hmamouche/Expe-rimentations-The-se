# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FYGM3
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=3, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.13100753657552705
-0.06668561341859294
0.09249178743620268
-0.012640011109751655
-0.0035513234243525864
-0.06128247850670964
0.07361324442016648
-0.04848991252254384
-0.02984673247951651
-0.07938840126087683
-0.03405743612988106
-0.015339722865116617
-0.03688929653589541
-0.058545957521963586
-0.02466741089732879
-0.0321235458542152
0.023779584672838406
-0.01717153936058967
0.03821174532200755
0.011699200149021396
0.005426554701185679
0.030007510033190826
0.019833718038115518
0.0037459533365578786
0.019122805958448532
0.018159095243071334
0.013007275304223489
-0.029830635549930897
-0.05629726356794359
-0.06141362306593964
-0.06095968016361318
0.005424924191968688
0.009093073574042657
-0.03275134584733528
-0.018945213765951492
-0.05472975067587146
-0.046524779392879356
-0.024373940932029443
-0.04468043322987637
-0.02379490115481423
-0.007471628951782751
0.011577985584240447
0.038583033046664876
0.044155921997493686
-0.0044417493498635505
0.07240905647099843
0.010551214930367732
0.01816582129120355
0.02836248590776248
-0.0006295660006420158
-0.002300052843677583
0.011822296198139678
-0.012359560135578065
0.031305653024260305
0.003930563748456073
0.009549617967917692
0.02284888935833638
0.01102177821830267
0.0012793709277654991
-0.024256509371047983
-0.029257394417382943
-0.01944615686506942
0.02736699533463806
-0.025809947727324353
0.017231359426927467
0.00964541177873508
0.026451197581566006
0.00028066155311927627
0.0006975890525177404
-0.019429129383743367
-0.0340389112486172
-0.02337689909915352
-0.04611135348829094
-0.09070094467349765
-0.00044499644950973805
-0.08433066693624275
0.012787664488437116
-0.07488926334138075
-0.028262204532232008
-0.024143728411963505
0.01814170601606188
0.03855090479999882
0.023509372734335798
0.01601300205402434
-0.014698037344224786
-0.0013222525380435718
0.021276803089415947
0.001443536918437119
0.045809997335466424
0.03669244756895466
0.05263745761548869
0.019837948950140988
0.01013167967835608
-0.00251109203761876
0.034118130927461784
0.005298072112714463
-0.013155424014388822
-0.024877108132447943
-0.06259712895990951
-0.033204520923390816
