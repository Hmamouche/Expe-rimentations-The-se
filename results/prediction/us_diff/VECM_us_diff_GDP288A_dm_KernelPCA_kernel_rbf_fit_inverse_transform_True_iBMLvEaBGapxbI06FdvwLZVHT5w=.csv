# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; GDP288A
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=15, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.00588742701355029
0.005111052473662214
0.005459699667962964
0.0035375995905129595
0.006472404556380791
0.0036467867259831824
0.007654308264376238
0.005123838056951142
0.0032034907609187345
0.003290703999607854
0.0018915574066271272
0.0013599802737475956
0.005793215374949701
0.008894936934182284
0.00510338480426555
0.006698612007689531
0.0037806609211091763
0.003089445051247722
0.005946939384225964
0.006782097481158489
0.0007051343909245486
0.004254618499602902
0.0036755156740138086
0.0034732370090011723
0.0038560347470453157
0.007638322534845546
0.0072304936848219025
0.005317099679128662
0.0065916032017255535
0.00622993347568322
0.0002454211437132606
0.007727707486777491
0.0047195143367200956
0.001723509876124483
0.00034345342438389187
0.004899335968820164
0.0025943352242464536
0.003946680230960975
0.0064335537275322634
0.0018183240365945094
0.0031661735847377855
0.0013704102325256
0.007558805150252907
0.003959793960092745
0.006892338682597556
0.0029074329913203467
0.003811443162760464
0.005118768227464202
0.004824231085067487
0.001791183046612637
0.004341521846699752
0.003405554191051917
0.0053499355548371885
0.004580495170616334
0.0012251386115317083
0.00541422877427945
0.0036824446875461992
0.00218880588840789
0.0002765642165417832
0.0031024945109530103
0.0015607948464759103
0.00483641245479466
0.005735090072071123
0.0042935769300197725
0.005865680592704481
0.00921390434588134
0.008349659847404942
0.006466566829107062
0.009284960322495255
0.007978244514524262
0.005083028219679857
0.007112673368816758
0.004532652254810433
0.002395551597220555
0.0008657925859422181
0.0029778114970738442
0.006189590764685758
0.008478497111596067
0.011457508348331095
0.006389358811269281
0.006467248775619709
0.008912567287737658
0.004933027406664112
0.01155484109445979
0.010868240030058351
0.01237364067269404
0.010489948529103616
0.01364817501053207
0.01588702990834155
0.01861432121147267
0.015619867189609836
0.016801145845541057
0.006435255573179567
0.010317827505909542
0.01299147728336881
0.01077309789788413
0.015961729795943427
0.01626563191456129
0.015782609977014608
0.017042523911066833
