# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; HSNE
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=1, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.07587189323006241
0.011684028436801878
-0.02989595245002724
0.04079663599741256
0.047590545055744594
0.030278847872923755
0.06320962578690316
0.0522113955018895
0.08225959047715634
0.0157764119635687
0.024938785311410312
0.10529924896638118
0.0788604350210161
0.05649182538583272
0.0007892174190521281
-0.02228779889205154
-0.03466024295131145
-0.043799314168428874
-0.056794017912359586
0.0138298691284157
-0.01556116400034673
-0.0389780878317103
-0.07425068039061608
-0.05057540809880502
-0.034837570583244264
-0.04249604975285657
-0.0592016302670115
-0.019461237460866674
-0.051983719099909884
-0.06798145468143708
-0.012326068264818975
-0.038411988553392444
-0.011155907864371106
-0.036693637887590114
0.0030920003344051955
0.0639858781129854
0.01191545738403276
-0.012332045388187088
0.0027540125042284716
-0.017488563388279824
-0.011903058817892507
0.013866784665954368
0.015184295023317338
-0.021210949115366383
-0.00539606428839387
-0.0006005023698481904
0.02254814532785064
0.003016638179857145
-0.0076378103484156
-0.00874530956630303
-0.027389441816035878
-0.004614111413923556
0.02063984963476609
0.019656214083989564
0.012617278535387256
0.020484852528436074
-0.013933994192908899
-0.008547594383173407
-0.011181369309991534
0.0394772561900959
0.02734761386604635
0.0025244015240725344
0.014728285424466572
0.024223339651215023
0.004630836219974218
0.011629142409113866
-0.02504249021802823
-0.005959623444891871
-0.004531211941306703
-0.007433217955681717
0.040284781264176506
0.018435952898551843
0.01335127169868824
0.014379927711552385
0.014724414725294455
0.023910984451238848
-0.010879131657991791
0.013264485213418088
0.008395427683954657
-0.012968740644224602
-0.02021894937331056
0.018007688631226607
0.021498027446653092
-0.015081295397339763
-0.012601935134020172
0.00994577568416485
0.01780356110675712
0.030347429981330207
0.021095787217909154
0.03562280599482143
-0.011323716184407683
-0.006513000688031472
0.005002179322672762
-0.036773171185194124
-0.03049442500417371
-0.04077842600637716
0.005520619462968359
-0.013969135063051561
-0.02288747886825882
-0.030828208538221283
