# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; PMNO
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=10, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.26654849809452263
-0.037583748840205336
-0.05456103443804136
-0.31546601957677545
-0.1394505552069164
-0.02980343172776597
0.021769713449739984
0.06032959225656029
-0.03099147547295772
0.1568969932293483
0.12992434012451656
0.06992690169454703
-0.009717607000485046
-0.05359498058553474
-0.02419544095046996
0.004310039418950015
0.0417489663664575
-0.18493226174825037
0.04665353200752318
0.027437105387428638
0.083009553838927
0.1892377709377258
-0.20453822732257587
-0.034036945527970375
-0.014390156834149435
-0.031061371143838
0.024824182085284245
-0.044283681790990626
-0.001230308966589426
0.010522878937869753
0.06572881423782422
0.02230130897299426
0.0846026920627518
0.03444095204053822
0.08054804060832021
0.07904375667106585
-0.0319372451745799
0.030549827850543193
-0.1681709892447023
-0.09906709314038038
0.01560665277658172
0.046690134097725766
0.09821577950669565
-0.08929461475085017
0.03567945353581327
0.02477878744648223
-0.12393245629914386
0.07955528281273791
-0.057733865090935754
-0.05476451012523417
-0.0658182626228796
-0.004942278826884645
0.025520903288942548
0.11820524836887016
0.05140425617097933
0.021490505228154794
0.07403841258846214
-0.061637554531939864
0.035003404854132836
-0.0435286578199144
-0.11987632300465431
-0.028236521055514358
0.04547011121143553
-0.026337787813889655
0.08261967604147596
0.03490183723284264
-0.042091259176901964
-0.16926302794481243
-0.07101837955102078
-0.04797375122557492
6.353676124365437e-05
0.05816907061117901
0.05905200897253294
0.17225336290890148
0.1994212740582726
0.07829207789575549
-0.009484053848539797
-0.09857249715616648
-0.2078211904799368
0.1299359985959125
0.03974874768886749
0.00815463843105265
-0.03808154002486999
-0.09207589979839283
-0.09576417331812567
-0.06483936784761883
-0.025754208604485847
-0.007181336656657419
-0.011936148402077698
-0.07325549719660024
0.15253325570493004
-0.08915978836742607
-0.05490689595863192
0.0869385567771067
0.016659558992462103
-0.07519094637909997
0.0695912573784114
-0.03473956076664478
-0.13453292730076777
0.026125435811095367
