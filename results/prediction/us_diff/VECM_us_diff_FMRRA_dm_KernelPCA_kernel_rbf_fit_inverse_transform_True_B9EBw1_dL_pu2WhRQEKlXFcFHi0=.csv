# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FMRRA
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=10, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.0011243149379513357
1.5196578316489405e-05
0.0002368709158598275
0.00020976965005733273
0.00015103132961492117
0.0011483032180314725
0.0006781385994370345
0.0016364701314564595
0.0018024031822987305
0.0019307254120200495
0.001597716823300548
0.00213189670596138
0.002765653709256423
0.0034721180021599993
0.002131910283103494
0.0013568910307691295
-0.00028853743081963946
0.002906166165585174
0.000384726093874863
0.0015995807888933798
0.00030276724480441774
0.0004468439888215703
-0.0005843285500200089
-0.0006012559260335855
0.0009808118178453859
0.0012672936466887918
0.00045123310110526576
-0.0008205434197972766
0.0004628737215044339
0.001233833692290938
0.0013283718125021394
0.00010489658311309442
0.0011056025098146332
0.0022138138154195464
0.0036902616453685787
0.002077126062105112
0.002219616652767958
0.004794078749811352
0.003316867259699882
0.0032738982020537304
0.0023117322461748612
0.0029807442348082306
0.0009756654256372032
0.0012073248266009917
0.000449608083273937
-0.00179938262637226
-0.00030396896408528345
-0.0005315939364252666
-0.00011740968336367976
-0.0009611992312408463
-0.002057139380816771
-0.0020886611669198784
-0.0036096308747010602
-0.0028458785197679123
-0.0025635219190790134
-0.0034382865826535496
-0.0014084800270175906
-0.0009351879411610426
-9.724841585770567e-05
-0.0007127982713926524
-0.0010224461498678171
-5.741249572118929e-05
-0.0009152309417327026
-0.001737259670380017
-0.002402503633477924
-0.0009689687303929674
-0.001868707256116342
-0.0007897891634721707
-0.0004695614401390288
-0.0008779674915231501
-0.0005823335669378233
-0.001269628792961384
0.011411823302471803
0.0038564567052730477
0.006098585999654602
0.0020588715308777397
0.0007875465932016725
0.0003849074374278228
-0.0005720580055090568
-0.001861392822104294
-0.0001440634374459016
0.0021524238912690083
0.0009823710261658452
0.0016881324790447472
0.0018548351437177302
0.0009889066049780467
0.0003723946368744414
0.0009179495286173352
0.0001761716749237096
-0.0003181188223713476
-0.0009895160005802958
-0.001675490513261885
-0.00022354445043589518
-0.001124287982861198
-0.0022998136625773783
0.0005312247581499985
0.00021363855990954252
-4.111193854424789e-05
0.0009156116530931692
9.937536306463642e-05
