# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FYGT5
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=15, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.09644504308763929
0.11020590785963104
0.2611672393435969
-0.06324330022225554
-0.042873825359851875
-0.09963956565066495
0.017372305274497915
-0.0749533602855402
-0.02192579856472124
0.08192456237132692
-0.09107983738562266
-0.08720609755868691
-0.05335332562686785
-0.09716355243380721
0.01713596802139139
0.04989338016780439
-0.015885170903962447
0.04372504322450246
-0.011202889721924465
0.05066313978997223
-0.02487256075332648
0.04647388557020131
0.0220081400978819
-0.04709761111377899
-0.06715465826814085
0.02993936190600021
0.0036126132278221715
-0.0024265592118755154
-0.06030235901249946
0.05412545372680688
-0.05133469205931974
-0.04047603264499454
0.07552126559189229
-0.06607038538192093
-0.021505875992093793
-0.0026106556088420635
-0.07517411061811126
-0.05194590246325969
-0.02696703718277857
-0.02974230172886457
0.05573585437218559
-0.007162915268612135
0.010046900915951486
0.03343398819072657
0.06965504693589936
0.12424499502832496
-0.04141514734928943
-0.015820740342606554
-0.05165940016354701
-0.01884547276975581
-0.08663314911593552
0.08229990266505192
0.04580741202353257
0.01574457610168916
0.02171083632022774
-0.031792257346693234
-0.014189222358002025
-0.026667704848857637
-0.02050486492378064
-0.029209486912224705
-0.07066215458384745
-0.05130415409051801
0.07788078561238249
0.07230976410434933
0.010818866697786752
0.10420806312665615
-0.10481259228895867
-0.00426022420384483
-0.01792916529449841
-0.0544381290349771
-0.0748492663603976
0.04571035911929824
-0.00768399248075822
-0.023426493949348506
-0.015416574766862038
-0.013854381993570797
-0.06045144641607087
-0.015695592224386046
-0.0558831975117273
0.04754549411108342
0.004081650237258892
0.06750688962547598
0.018213696210465535
-0.007114874441946025
-0.06357420596154274
0.02849359939845239
-0.008118314686912275
-0.027655513428626134
0.05691093796140703
0.005179360344682308
0.06311099984892835
0.02546085379354699
0.00875539164444089
-0.01620606650331681
-0.04811286487100451
0.05180249682449355
0.008118850547704531
-0.035423885348637885
-0.1083996666474639
0.00626539276087666
