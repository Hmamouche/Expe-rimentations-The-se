# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; GDP288A
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=7, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.004134881924624811
0.004698937277187835
0.0069884805116409935
0.0056363212338538075
0.004374631886714185
0.003530331530971967
0.006668267588926802
0.004262696320115976
0.003939659695563198
0.0038879435544027932
0.0036919807026559504
0.00250074294160899
0.003595676650650901
0.006245549413689785
0.006418620816301992
0.004667654898226571
0.005602020225552058
0.004519921236813655
0.003197313164683131
0.005505886754130222
0.003574384672448733
0.003977421087734346
0.005198409073151768
0.005837453850379551
0.0034555226727186845
0.005588058323361109
0.007815970775994625
0.005340863153011769
0.007262213047372059
0.0068261997257845425
0.004791556482999975
0.003793023784205038
0.005208918759846494
0.0030614859210172985
0.0019423155699727379
0.004039581735367418
0.00322925655594278
0.0029314510417768813
0.004411096468074662
0.0025498974025100953
0.002530347936869939
0.002605869248758939
0.006373706709025458
0.0041793005836429946
0.006685411543501241
0.004770681453123508
0.004347966286767034
0.005358532947885219
0.003951331083294851
0.002634367922625255
0.005290017305855084
0.002130501802023419
0.0048080386007050835
0.0044807738177709295
0.003968586647179585
0.0035635192944622153
0.0034251556771860607
0.003678245307653051
0.0013596422727268115
0.003284373016510532
0.0030444823604974356
0.0035076323055647923
0.003811457307670743
0.006658091726463759
0.006230650221059288
0.008787587793301264
0.010178266985811911
0.007275898351638457
0.00859686886955177
0.007697678112528891
0.005668813988001835
0.005772383890524186
0.004969042395940882
0.002926489978901418
0.003931315479366511
0.0049465100300530255
0.00522067548230302
0.007050135924989272
0.010722577826907173
0.006652721800718192
0.009894892884133626
0.008626404242036772
0.005732377179063244
0.010111614256894757
0.010387653110721352
0.014307463354089422
0.01219424494023916
0.013618342029936621
0.01644231606730584
0.015960663851638336
0.01481293895928887
0.015379090630491407
0.009105135783943098
0.008745935557955683
0.012017627047507128
0.011024045197725088
0.01430973214907108
0.01631196677008659
0.01534327436866368
0.01777161080613763
