# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; GDP276_7
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=3, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.004807599860085573
0.0058441711300199276
0.005730172234620596
0.0072864548368983435
0.0064211545588250575
0.006079288051906556
0.00332407196442909
0.006741584527327093
0.0048450650977779905
0.004175525495142045
0.006667410537279834
0.005043131434376397
0.005114727316079492
0.00528044406828877
0.006029445957115361
0.005773677900605365
0.005519163510063764
0.006433749632313727
0.004311436478444898
0.006729279148317183
0.005116939065890198
0.006977940332514857
0.007931977562229424
0.007234113069372034
0.006192932360585998
0.006753364115359997
0.008371769592929805
0.008753703101154505
0.010833304297495709
0.008282510737237809
0.008739166207593449
0.008994187885144877
0.009257185917918685
0.005701790477767078
0.0036587663782846353
0.007044164044623869
0.007147634750291741
0.005240283539653185
0.005092553236205173
0.006673858277329616
0.005742700161612505
0.005539109548561337
0.004596734610068271
0.0034530407277749134
0.002665786047798478
0.0036133217951005384
0.00464979505870793
0.004658188711266537
0.004731277949140408
0.005604768497052649
0.006190724258168338
0.007390726540048993
0.0073951396175696415
0.0063733683356299925
0.006197195914061007
0.0061677711285709185
0.0067113430649689085
0.005437189251900555
0.005127704221606164
0.006225688004380956
0.0055399327634442904
0.004949400630666972
0.006722312920539188
0.007529099892641242
0.006988620308295265
0.007079577876294244
0.008074133249861464
0.008338295562373496
0.009265905579298142
0.008696125968658838
0.007412753477546977
0.008061279061899471
0.009205970953595793
0.006142893828902376
0.0065788311147136815
0.008453856850749638
0.007702469115205798
0.007505069013007656
0.007920698395204248
0.007870202945422277
0.005520778417630943
0.005197721725754759
0.006249794072612305
0.007053572210819347
0.007275817845100942
0.006990708576569195
0.0073704555973001535
0.005988621399298979
0.007716563254390422
0.010123429494458741
0.00784163710179973
0.00766309905441971
0.01081905873610183
0.005852533683352081
0.0017152809336125308
0.0047822970686875995
0.004172706588761839
0.0070193776967643244
0.007079900866894084
0.010098961434935867
