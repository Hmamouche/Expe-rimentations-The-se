# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; HSMW
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=10, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.1965162110994611
0.04510054169518296
-0.14601276812092728
-0.13139890165389356
-0.1137692662359632
0.18477741278787208
0.07067059956650179
-0.061444394598691845
-0.0154730979881791
-0.07237502670182805
0.08505751733462408
0.049525721308687265
-0.07523905061249153
0.10922811736387548
-0.14079460285891315
-0.029828994373412646
0.13088953900544448
-0.04424669648239293
0.014365477937476487
-0.14365201702493602
0.0505662068227528
0.0044468282429737105
-0.05610556720285437
0.09132945392739299
-0.08237686992299795
0.0981579202363023
0.12792116660700073
-0.1095499053079826
-0.09682910274850957
-0.03028416400127773
-0.1283434607873882
0.17873543211511084
0.03557700962807729
-0.005712051562474799
-0.04346018650299577
0.10509552346668238
-0.01856954695321999
-0.013013376817467594
0.11049260247712658
-0.1391741600789595
0.04066675513898231
0.017267492324697306
0.048604657824028466
-0.18852667097495612
0.13429715270790593
-0.1618775533008397
0.05999692054769433
-0.1350060027663094
-0.0044724985719829385
0.09216382709936011
0.09265677917221736
0.07264375042835033
-0.06793235481653324
-0.08920441587188402
0.12344904443894614
-0.06592144131423555
0.030429838686372716
-0.0011042146969829116
-0.015005501992323714
0.03247637098920106
0.04354705490716597
-0.0006604119457936548
0.07646905565728014
-0.07709822324022597
-0.022840083633327256
-0.007620896437406679
-0.028263240655944433
0.04778903215627171
-0.03353738515233512
0.11967245256735862
-0.049616319246058825
-0.0049890875297510864
-0.013742570732629552
-0.059072340202128704
0.18899776468737695
0.04163269687660479
-0.15510364424908007
0.05894237258102955
0.055487531545978075
-0.10209090694616586
-0.017302904895201095
0.11887394468571724
0.10609428821591162
-0.02650977426563606
-0.048705900082443056
-0.08362925563718408
-0.02737475906285703
0.03403003307645588
-0.033104097174001645
0.017995382373028786
-0.051667299976391204
-0.10658213367271425
-0.05219446537855451
-0.04561967049068697
-0.01032746503611303
-0.01659512500671506
-0.0649845078467804
-0.17588066743231073
0.13032552200191228
-0.020690242941455383
