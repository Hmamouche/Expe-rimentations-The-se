# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; MSONDQ
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=13, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.01486697697157821
0.08317311433916685
-0.01143567776292743
0.01686611204747719
0.02149115833574184
0.022826375351425482
-0.049838076256449586
0.059293778699794485
0.059950191909034924
0.025245250107995505
0.0915143533829077
-0.030757573819009853
-0.008290255732584226
-0.04134856849968396
-0.023451601141975243
0.04149837762328159
0.008165157756056978
-0.016773058370543898
0.01093375049374612
0.05956329675518984
-0.048317330257921634
0.06604336498400216
-0.01668770054492731
-0.03129133700091497
0.050191673090419796
-0.013881976646227484
0.03604069153146759
0.031980376546715275
-0.03367514939605497
-0.007906421324759366
-0.09147509603015873
0.04715584492252092
-0.0015678446937224012
-0.04695034274661898
-0.011619591204749244
0.009549571646611854
0.005978689355688099
-0.0119096516907715
0.0067785316344759
0.03435901857210976
0.013148252387721247
0.015443510785357498
0.047178901116830034
0.01196711406310734
0.014189431124215932
0.002779772920617901
0.008687917676300328
-0.012723205897608655
-0.023586329013647815
-0.011562109778724648
0.039841375290527906
0.08560251960579882
0.038725649040225706
0.022789598839643124
-0.005028035305325994
0.012024706105262007
0.015527110232831133
0.034272181643861595
0.06152991498114341
0.03347268376354347
-0.01625917789570302
0.008215119318980921
-0.015395429612265327
-0.00037729120234770985
0.04530981074882453
0.051691929159746255
-0.028442628515682557
-0.027114639746827873
-0.02922927526176846
0.04778853222741207
0.0047448075352728505
-0.006795778453366198
-0.06599741966588624
-0.07672159159041704
-0.08575911643893235
-0.05526828177378906
-0.03578437957670699
-0.03357263340480851
0.005902948396717844
0.0375855981353882
0.026249736236950146
0.05381102128841696
0.09190088093474759
0.017247059182599056
-0.03680055750228451
-0.0025060291878651123
-0.010387370631701667
-0.026525191649075165
0.05349900722350922
0.05972196639332624
0.09140292027611557
0.06355021896295994
-0.030872665669509743
0.008082354182389579
0.04348528997026417
0.0137753288954795
-0.01984241962864594
0.00560832700899151
0.00415994405973865
-0.03090485101722102
