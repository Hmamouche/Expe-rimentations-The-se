# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FSPXE
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=10, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.02458483277271803
-0.056127452836628754
-0.04577519609816488
-0.05254205944843078
0.021299142192153193
0.019616709472576592
0.043374938381843864
0.0038990624419955582
0.021342892416901717
0.042096553802948515
0.06780494472248216
0.044711549767161136
0.012700158213243978
0.01951524071672597
0.04982309612623337
0.007688672818461861
0.028190753350780813
-0.06937746902476301
-0.055475292741936376
0.017310403331203903
-0.050713232452823914
-0.03430976488981134
-0.0003855904679960914
-0.03412146773883967
-0.011839570825265383
0.027957499112567723
-0.00488860991562063
0.03278787667479914
0.010303421804425988
0.012384985670381278
0.07816930703556728
-0.013615460797224716
0.03495482765053991
0.038807558484101085
0.06308658456705502
0.013393538697653317
0.021012441922997824
0.029214204694476414
-0.025023662996161752
-0.03728418780451036
-0.0007551834427959687
-0.002758823281239844
-0.058133876853645225
-0.024142808559114517
-0.00013719947547860718
-0.05380489316470306
-0.0518715890549972
-0.013423019001682105
0.040340449206809056
0.008106036655875577
0.027066258615763037
-0.014324001243982622
-0.0002877780777064843
0.010141838181141059
0.027607571039079293
-0.015490423852058122
0.04543524983562833
0.024001737031291593
0.04080424489059871
0.06500724160844665
0.008931965255926778
0.07519192254173584
0.0625062592273814
0.015137194281737664
0.001963304602902844
0.021368109151075246
0.0027515634303761673
-0.025887547706423843
-0.02105792533096875
-0.036676329904167256
-0.0038200802480889383
0.016855868589478754
0.009579030111308017
0.09008932940175426
-0.07326492861356805
-0.09808538604292422
0.3556823806534081
0.19522082394693996
-0.03589674997720081
0.0007156396674440763
-0.025535978990595714
-0.03055792123445194
-0.00812349704964311
-0.10283953308600954
-0.13735763740513718
-0.048345444247911616
-0.0046100717747309915
-0.02842498946694978
-0.02422996017216939
-0.01717250604077768
0.009836223337007375
-0.07035865682892012
0.005802474404500359
0.013803507857198324
-0.03476905375799717
0.009793307609556533
0.034316513786104244
-0.030773729798929617
0.011149289030833514
-0.025456918727587576
