# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; Sfygm6
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=3, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.056969334452671605
-0.08209935733043905
-0.1226573576188455
0.023697953361955272
-0.007014912335642894
0.026058068169759946
-0.02063248579600397
0.003184844924815109
-0.027164574625610404
0.016050597202516753
-0.04104831135450592
-0.01331078398565582
0.05738505288461492
0.047942869470185986
-0.030446509097993176
0.10332461684515318
0.01372961476572101
0.030510294535613273
-0.03418537398264011
-0.002316833105498621
0.045119332888339025
-0.08522241940594034
-0.014650622145770682
0.004579249261331974
-0.047960945519932005
-0.06467295890531413
-0.0282368547724235
0.0648301794824441
0.0781325511602387
0.03139157767203671
0.015682607546640782
-0.034398051308714206
-0.020787243569978714
0.002366543769650342
0.00907670160500202
0.04318337235177818
-0.029467720511357466
-0.05565680294273815
0.014600040105384259
0.008478560321549707
-0.0032969081432190107
0.018307621320070803
0.01745610962172217
0.045885128901997176
0.04562915660211746
0.0033209932310572215
0.05790648037656034
-0.018402562806899357
-0.060601858144044674
-0.09396697678829116
-0.04138081435523662
0.03772474765889252
0.04538059832513726
-0.035984552146108226
0.02700834278420309
0.008290235106526926
-0.02778185023819022
-0.035956878378570326
-0.006569011215154127
-0.010864654312875758
0.005265991189711744
-0.023959504715399003
0.05373549253946007
0.06248126441195434
0.018247217331129113
-0.02042913460093123
0.002818965173054036
-0.052414071392514
0.01395887455381484
-0.009896889875142211
0.054316405510409144
0.021254639608424244
-0.01563468355998741
0.0206953188859563
-0.028614390758357686
0.038474280684395894
-0.10245493548770714
0.03005733026700315
-0.027339825413374985
0.031236548341709316
-0.006840680696363205
0.009233955073995364
-0.011357205220930817
0.017344719486517262
-0.04632351863132736
-0.004041427686459473
-0.009194075355010226
0.0589844110520852
0.013082470939442451
0.024091704841211852
0.025812091177261104
0.038000464420552826
0.025479008268588812
-0.0766465654243221
-0.01849520027529821
-0.005340066058861108
-0.0013956941949056674
0.012797465758735868
0.07326949007656117
0.04817458503805734
