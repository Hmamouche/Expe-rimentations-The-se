# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; LBLCPU
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=8, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.007784294822022525
0.0025856969475954606
0.0108034708003161
0.007112147529347865
0.008816254679886183
0.0007159289547319638
-0.0002995347149588141
0.003861373051351446
0.0026226091007112364
0.0037776515298683588
0.006611832217064876
0.002443847611625124
0.0004001719515227735
0.00989776527654773
0.008539944810544537
0.009345023503134183
0.010950160562799792
0.008582233404357256
0.003639192404864809
0.005900300889699434
0.0051671668644842845
0.004449953328416071
0.0065143314473739425
0.005584217148019951
0.0031893044303998156
0.0015688492353771477
0.006929638284224747
0.009473101211277543
0.011333590940705648
0.013849449097927631
0.007294028801071031
0.0068297288180952185
0.00911921253660792
0.002613812136062919
0.0052750852464463855
0.003442617699222815
0.005821694484328515
-0.00036671258536970426
0.002670331128997566
0.008617026700664603
0.002342515106194548
-5.2580206750174016e-05
0.0018657229403774792
0.0058890157338285665
0.0032520994402946398
0.0035218813619343034
-0.00146346281753798
0.0010076336794823555
0.002154588230328866
0.0032720871297960055
0.004474812318290149
0.0017212668549757494
0.00435660207253679
0.0037578060544840772
0.004394024730053619
0.0027793898928687812
0.0012467689760119307
0.00410523260837065
0.004166903958267196
0.011790213210600866
0.008071371910153117
0.006407904662634355
0.003575080829960876
0.009706153890031037
0.004880454856992267
0.003547695276951075
0.014025850276357709
0.01475094504368114
0.0065585566318423815
0.025791947300350896
-0.0021052979616858966
0.010979478798039764
0.0005416269636095777
0.008569836267362574
-0.006380471841624617
-0.006217197295999544
0.0005450727024968753
0.0017966582636940128
0.0026807661268241165
-0.004528247475030043
0.006906672195163267
-0.002799453948956743
0.004952801342318574
-0.0012380531652820556
0.008941119254310115
0.0035888045368518764
0.004516787265054142
0.005874552675447505
0.007106363069092804
0.002765600357445103
0.006538920819071133
0.0172158801681338
0.005814654873319191
0.0065938591395706715
0.0071960900533794055
0.015123633625671616
0.01707152915053768
0.01230691421951977
-0.003774753679421953
-0.002411499217103558
