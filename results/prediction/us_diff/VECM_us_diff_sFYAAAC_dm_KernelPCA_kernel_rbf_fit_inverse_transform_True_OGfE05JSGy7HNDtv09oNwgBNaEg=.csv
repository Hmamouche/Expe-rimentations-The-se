# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; sFYAAAC
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=7, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.28712333949019236
-0.10288082438572499
-0.14169261412914452
-0.0731229460732636
-0.06467349521882036
0.02335222196959995
-0.020808733000119935
0.07096466280542384
0.06628393430682052
0.04815832323585303
0.07646689194535819
0.0780769916814411
0.06330063383521072
0.04162532733501386
0.05845207227302128
-0.018738102770636943
-0.01661303047353397
0.0014990522114995339
-0.07268281994913468
-0.0854143256218599
0.009552898191457337
-0.09745556726903823
-0.12393539174431853
0.037895431603336464
0.06717458014447852
-0.016250616759237634
0.005788238976929504
0.025068670686674024
0.07375279357121207
0.08888222122961482
0.06129518045372338
0.019082306575294507
-0.20485540995245904
-0.06470397212541698
0.03533824376470225
0.0318503778773349
0.17286687745214235
-0.028811263195104644
0.019062271494978137
0.04821226182189599
-0.08139377162997075
0.006933409841454671
0.013934589765643615
-0.06300553229563606
-0.05668280945624895
-0.030762623542416302
-0.023231391538574606
-0.007212873649603161
-0.021865513326088257
0.03696312419302567
0.029568394016425192
-0.03968281443172927
0.0035032475839983437
-0.005229554365070062
-0.031145828875780643
-0.00937384341176852
-0.027508089302852166
-0.03415172286024198
0.017727535177833462
0.0717514519255769
0.047271799129051086
0.07637187774068173
0.004747961286168171
0.04128127347385287
0.036712611180543225
-0.014638884665976334
-0.03604861609868712
0.05216533441084429
0.050259176239450684
-0.002505440407277095
0.05776418681709328
0.03276756669909767
0.06080781037977787
0.057218251743860055
-0.14240419474447225
-0.023294003795889202
0.08657401645591574
0.11082248217405671
0.04915244247881928
-0.12009692298615372
-0.05348664357819144
-0.03463206099496775
-0.07043911291432665
-0.035761427206533566
0.0019497952022486283
-0.0010398008624056144
-0.0839297369516632
-0.028650914377082316
-0.016739908311288444
-0.027072555120308647
-0.05047840835944563
0.017068351904383952
-0.023715103831169558
-0.013189800159381298
-0.0017829845920784085
-0.05989129674752758
0.036480460235119666
0.11458497623875177
0.1025476220452471
0.03762412677667133
