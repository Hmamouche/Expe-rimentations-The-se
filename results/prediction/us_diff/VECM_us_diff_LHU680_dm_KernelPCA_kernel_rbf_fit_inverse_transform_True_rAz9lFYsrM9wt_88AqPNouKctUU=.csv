# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; LHU680
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=5, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.013226411539807387
-0.09957804639988077
-0.06357867906062475
-0.07853350011686257
-0.02510426625141826
0.016957718988006576
-0.013847698818441462
0.003426366231571538
-0.014029670186986122
0.0041043242829963845
-0.04967165386191789
-0.007434987041120933
0.010325297749544417
-0.006664467772781396
-0.007685579835386084
-0.03281842462070758
-0.02566756432025517
-0.045372129464042656
-0.015105311290612763
-0.026055595642365138
-0.03188806190409148
-0.02137836173101594
-0.03965190056592585
-0.018110329132555798
0.006277568976995517
0.007191637937336385
-0.00403916519375815
0.011692484106363392
0.03550659232779753
0.018723421916750065
0.07432614817187082
0.07408576825899073
0.015544753519705566
0.05307434180153165
0.04446999926077855
0.050132634759585314
0.05165438005440613
0.07122860053331721
-0.0022274484860128323
-0.017603462626900053
-0.007046975525096772
-0.009220113294597525
0.019021475808214616
-0.005042253333086135
-0.013297813242305634
-0.018377991222694602
-0.06581107790901496
0.016642641461551783
-0.03577597949635705
0.001238692046614941
0.01356954159183767
0.040690367789815135
-0.0329937778516316
-0.027534603345384632
-0.03249894746346034
-0.04109042439842308
0.01271505064577467
-0.04399664829791483
-0.019088696156795485
-0.032512461186118474
-0.00583129551777836
0.010931861462284748
-0.035311081012193754
0.0015330553571711982
-0.06102471973349137
-0.02737004070752758
-0.06008941629469565
-0.00954108721314302
0.013761120577314188
0.003204639455533509
0.04071175060111533
0.03748407917058801
0.08695980946670585
0.06834072938274363
0.05785828617139535
0.03672585131787769
0.014662410748423825
0.064486128008311
0.011502476672835564
0.07932268393949474
0.04980808456150901
0.0007274739733971105
0.015727885486299064
-0.00961291611575386
-0.05582984290696402
-0.0062766194858792064
0.0025797345246971054
-0.0011344078466100343
-0.03040540093733163
-0.02517568483187995
-0.03991726698884663
-0.027720655168082586
-0.01350570592935276
-0.01448815857596356
-0.008405791611822566
0.006122335324220966
0.028690895102138948
0.015182105325243797
0.017642314845563042
0.028885306653077217
