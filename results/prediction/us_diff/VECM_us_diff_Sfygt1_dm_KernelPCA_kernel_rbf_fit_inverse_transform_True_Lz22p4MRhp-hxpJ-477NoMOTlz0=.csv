# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; Sfygt1
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=11, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.1934683543272368
0.2441464216106083
0.27020321576638323
-0.1662338998373496
0.012959456962631198
0.13831560201337317
-0.0999483612523245
0.02423481897336823
-0.24220276350502318
-0.23909594957175265
0.023264581582936025
-0.03718117190838655
-0.11541736533381426
0.06253726847297106
-0.0017604553519276964
0.08282156533241586
0.11398907137725184
0.1549129037494461
0.1254784775859154
-0.007858705453235512
-0.1274496624341115
0.22427475597700502
0.08103404672949059
-0.07998857491176564
-0.10535518304922921
-0.10394173498133662
-0.11360936240233395
0.1979392040685024
-0.07337212312190794
-0.0735438319385606
0.005120080700954335
-0.05304905426663263
0.12921881249907075
0.12138372000391298
-0.06621085011173172
-0.0460791053981824
-0.03407389912334978
0.044233838871614195
0.05325863229720166
-0.10739803685372194
-0.034009777832067725
0.027145987692421186
0.1328354145727393
0.11183985965720322
0.05385931639275034
0.18764009149786048
-0.0522889921400264
-0.020398592912787616
-0.13408719398961594
-0.12744447912239648
-0.09951085253173415
0.1008875810014284
0.04539306117569708
0.00997229808107536
0.11217300695524939
-0.025875462661099902
-0.06945104197900966
0.02840837700898108
-0.06067328995417598
-0.06476765723709169
-0.01611593933366094
-0.1059303091806163
0.031677678213946445
0.05595064484558826
-0.049311187133914344
0.11317096617550856
0.032366671952181944
-0.027720303098355846
-0.025704257317120224
-0.043612065700339785
-0.12199184337446156
-0.01283936351116817
0.049566522125859575
0.05271771527253613
0.007247411735737105
0.1019144600932051
-0.23336427481669172
0.13099279783633347
-0.11938018893829039
0.07473492841361404
-0.04388946152731117
0.10828564149390937
0.119742045174419
-0.07567244167921178
-0.04264956498624347
-0.04100156494108555
0.012714589810236886
0.04282206195775913
0.04825482277444736
0.01588030862388259
0.005342145653856007
-0.022121838927051572
-0.03254412803502485
-0.020122098958143337
-0.06628315768085077
-0.011153628746224997
-0.04997271766732596
-0.03902080264682431
-0.08780504690448084
0.014444875358916719
