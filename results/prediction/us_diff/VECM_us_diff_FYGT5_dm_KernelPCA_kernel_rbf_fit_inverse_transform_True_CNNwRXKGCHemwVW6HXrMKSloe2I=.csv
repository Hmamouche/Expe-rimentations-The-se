# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FYGT5
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=8, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.14167242997449064
0.09554487503355628
0.08897023770531343
-0.025870396000531794
-0.01880343522228751
-0.053862772825617265
-0.05428678472949748
-0.0685282752127585
-0.023661528191432597
-0.050011255813050784
-0.0691821266277422
-0.04222286304290722
-0.06985187696706316
0.006310006258911108
-0.045346641921782296
-0.005906192146335999
0.0848694914786439
0.02919898999824869
0.05056454871563322
-0.02086261054729265
0.00039750490266254185
0.06487514399507127
-0.01880493302078222
-0.03098694790496253
-0.049167845113881725
0.004412688927596171
0.01138364388811882
-0.0016230383966637767
-0.03025646306452503
0.03221866238463523
-0.04838629969077958
-0.05671421614326967
0.05363378708289218
0.004107837992447974
-0.005743045937408243
-0.038207729250436265
-0.06831425350780033
0.01788135883826971
-0.005500708578045854
-0.017593824980218612
0.052764769734395374
-0.024277584973437162
0.007821962335241506
0.027234713035888036
0.050283971017349674
0.12756993442153292
-0.014853572882482808
-0.029698641913832535
-0.0218667298462783
-0.07842827648299278
-0.03513453052908431
0.05600770903903221
0.03174969065184129
-0.01844876252851217
0.03301507575947948
0.016387442034973738
0.007557631115925063
-0.004895755686273029
-0.04081915302823372
-0.030554683978809935
-0.029024648368261647
-0.0730663808715295
0.0478671187306344
-0.023648137841699533
0.04659507028823214
0.056281690314757535
0.03409912640106384
-0.013022638435443178
-0.026159969053162948
-0.010835563669914831
-0.0536018593386247
-0.02193144588285348
-0.05831396484908016
-0.09426913529527434
0.049270148790435446
1.8418181958084964e-05
-0.042107210652129134
-0.003105031166415941
-0.0636084492140357
-0.009135292911334462
0.023934262181922093
0.03791661046682637
0.05752645841813646
0.010508202709524742
-0.0285866568612457
0.014695678367577876
-0.002494622162770405
-0.011269680492005513
0.009022915465751697
0.029627613641234254
0.03255886693635532
0.001961824583266436
0.008179758505845597
-0.01874951080755
-0.013000330508143353
0.0431584392456743
0.007339840039693805
-0.08614278800673923
-0.05274550856215976
0.028575468599917664
