# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; LHU680
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=9, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.0510822497063614
-0.11706556967647143
-0.06239127022350099
-0.052116913642767694
-0.03484565294212418
-0.006189275285807455
0.009624281652581862
-0.00938147180887642
0.023918400545021742
0.010175950578461336
-0.023702926250625547
-0.021864645025069462
0.014381915231997257
-0.024664337577949072
6.017653419308983e-05
-0.002811909134821074
-0.042476509255963954
-0.0565288623255653
-0.0017928753033440558
-0.03946717105286843
0.0009588360642538152
-0.03425115134708953
-0.026690564340975644
-0.03473464266733726
0.009791594132118026
-0.007752168537176077
0.0006087967053981375
-0.0007336554705307261
0.07087025348225412
0.03717657805653542
0.07888670026794206
0.07358400423001389
0.003377809091071651
0.01142145384682898
0.06289029546451073
0.03078482317046051
0.04631309677605765
0.05399979109108566
-0.01582708245149439
0.00823807168938371
-0.013541013344208947
0.0011531017556890922
0.018234011250669795
-0.005101665783720351
-0.006969153435612566
-0.018136822686119228
-0.08344757209487114
0.03831440871073455
-0.03979986353408554
0.02622816339298798
0.023184650555950433
0.02572161734874191
-0.039733600706408995
-0.04608498861744078
-0.039096079728151864
-0.03905152693392563
0.01702118366289363
-0.03997751772237799
0.0024730276923569635
-0.023666721634751642
0.012541690719419447
0.004700535301761754
-0.028911167043509718
-0.007416109227959164
-0.06955397856660149
-0.024550529528820468
-0.05646893113104079
-0.025792157703086845
0.02618032668101475
-0.011920987022103799
0.08315044358812705
0.043326908614500295
0.10302605335385126
0.08296958016323229
0.02656849313623723
0.032371562730352244
-0.03793925711153924
0.06221837592456208
0.011038029739957923
0.08639202012908034
0.06323917912401289
0.01166284059823469
0.020757020585878937
-0.01001519707386941
-0.047966951083606046
0.010092093550308955
0.006139536883380533
-0.03125428803881259
-0.02918527356857485
-0.024890010346016823
-0.05071083194636954
-0.020486913949631206
-0.015998663346878397
-0.03416869444363735
-0.011687624179210751
0.021611511900322258
0.042784432693454985
0.03264413943499373
0.0029722074792597215
0.03334678641715289
