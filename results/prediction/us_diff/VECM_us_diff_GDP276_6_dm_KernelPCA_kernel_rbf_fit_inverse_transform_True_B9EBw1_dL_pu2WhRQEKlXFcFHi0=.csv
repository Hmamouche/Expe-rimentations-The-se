# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; GDP276_6
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=10, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.007926384151794206
0.007189071791785377
0.00752430941432318
0.009075086509353588
0.005645515989965631
0.005351533847159343
0.005001646188358055
0.004854527260823792
0.003849272505933942
0.005327821973949735
0.0033791448721852435
0.004092378343767267
0.0032173670239963813
0.004436036580207609
0.0025530623090971177
0.005745754294818338
0.007395253706543203
0.006605272299835357
0.007799808220697107
0.0098902625489873
0.01048925056975548
0.011527493323480702
0.011778677457667596
0.008938519964667446
0.010798555062597743
0.009344733935894654
0.010457469513446812
0.010498579457771117
0.00952013037401977
0.009901542075771971
0.007887085495044393
0.009746211216318022
0.008496517486441861
0.007875048883560219
0.007208155895125426
0.008495608067809609
0.00845350048223412
0.007889296255923061
0.008645499537410575
0.007332463435914111
0.007292121656756251
0.006371356652709666
0.008252676048388393
0.006693122644343205
0.006968818630726165
0.007728411549681151
0.0069011691950646315
0.005812777010745161
0.004598520266962489
0.0058082325732733255
0.0031632511157048014
0.006822618127246221
0.004260629427283003
0.005392163861488275
0.004161716393831777
0.005274039577974824
0.004188727777387898
0.003612105950621724
0.004527339372592954
0.005242059570471138
0.005272831430681065
0.004074542716065966
0.004448013974724676
0.004688814832448658
0.004937227720811211
0.006485110474379458
0.005192303051779942
0.006067330643586772
0.006583911610548299
0.006060999404799903
0.00915287693397359
0.007344379136091993
0.007718788531974767
0.0050983003797847645
0.005095408702658314
0.005671715122972692
0.0049491943027745585
0.0063187719168288844
0.00776864939496031
0.00971833086831644
0.009851393163516036
0.011142153911738808
0.01073617465549634
0.008692522815044816
0.00905852114081757
0.0071760201592488415
0.009872680071612114
0.007433940408856067
0.00797005131267484
0.008185278880090262
0.005436406491417729
0.007672714982613795
0.007871883086503526
0.008291709626734384
0.011371047851719884
0.008501039084889236
0.010310210222197844
0.00418323343103147
0.006087419958160244
0.005562530104691883
