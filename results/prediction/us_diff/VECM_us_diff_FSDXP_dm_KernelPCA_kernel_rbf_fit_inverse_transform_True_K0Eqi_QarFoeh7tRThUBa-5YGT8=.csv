# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FSDXP
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=6, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.014607088905936413
0.12634821741386282
0.05877336422331591
0.10980096473584063
-0.0347929721600153
-0.0077329460750008554
-0.0984741337757652
-0.03729935804269171
-0.0775849112947026
-0.04161823221040456
-0.06765980826667489
-0.030448895774082205
-0.0299906817025199
-0.015038931186877897
-0.03432028071021952
-0.019563264640203686
0.04431526189757831
0.045168817483593145
0.03280986992651211
0.009715027036653178
0.02750065401742896
0.028490534959184705
-0.0013392258385468395
-0.015944383877545012
-0.04939441716968898
-0.030909559607421973
-0.020987823710393615
0.010975912713948995
-0.00012865955536769252
-0.015705616363302803
-0.06388634202647231
0.014271590121959781
-0.023548352361215424
-0.005309443838372202
-0.0019387948739404207
0.04382548594052555
-0.023103999058220366
-0.009607832161178297
0.013222168707860719
-0.0866918156802916
-0.0019057304570679898
-0.024960968890806654
0.06112804255620423
0.06896296278631302
0.029783237499542748
0.03535221163595583
0.008754343492819953
-0.06480206523846448
-0.046402695219305665
-0.039408415258907786
-0.04837666342884141
0.0025936642519768533
0.01566520259738437
0.006928769943066257
-0.014588644853638673
0.025817571068251027
-0.0023654080029580234
-0.014221714714973792
-0.008410985852557085
-0.03292396159985097
-0.01816291983197281
-0.051593623825778256
-0.025596402392875438
0.032910040570144974
-0.009742749418782427
0.029395298424148893
-0.0002015447663625547
0.012228315108031032
-0.03519613331131563
-0.027495590396720428
-0.050308533925817016
-0.028982709936805733
0.04091718471099751
-0.04846751928592835
0.020178204855953875
0.020652356063758162
0.014350299935411377
0.004937397404144023
0.029253252475318563
-0.0039335884206913654
0.038278779682946844
0.012638187616571261
0.045745051931221456
0.026524620821202993
0.03306558162964416
-0.029717494170704087
-0.011577491499238781
-0.006574127114294982
-0.015719132460873957
0.010652208313843411
0.010028183795255519
0.020581551009483497
-0.009824336280906554
0.01353592742667092
-0.02496557801846167
-0.003616854992014723
0.014909826822181461
-0.004567341417898125
-0.03181511644133674
-0.00737009619810941
