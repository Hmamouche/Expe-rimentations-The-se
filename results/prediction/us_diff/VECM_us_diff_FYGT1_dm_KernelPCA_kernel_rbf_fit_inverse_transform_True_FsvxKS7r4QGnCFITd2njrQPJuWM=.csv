# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FYGT1
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=2, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.060906104860817684
-0.06928051866045551
0.04223673213388619
0.030809537521868373
-0.021465882415407964
-0.015371756852753798
0.04765976981746381
-0.03850430715152382
-0.042688086837943014
-0.0749361045813739
-0.0470776972538184
-0.03868859937545116
-0.0345450766599462
-0.04304234927280824
-0.017615213207568092
-0.025648076619825745
0.02299017136314497
0.011322831165786513
0.027394549735050535
0.03733138966924917
-0.010688330167415003
0.030089466278106148
0.005806564843658544
0.008302941039879703
0.008257683395139418
0.0006989394758017062
0.0034927214134251663
-0.025561044866781617
-0.04333572539262072
-0.04519023897317355
-0.0592114555727852
-0.00140310275693388
-0.012487236943094303
-0.029136097874923505
-0.0025452238534315746
-0.05770677067288815
-0.05385251460921857
-0.010498468682452227
-0.04855926620891124
-0.019327216268932655
0.0006271978761214422
-0.0021499793761542273
0.03720228341645256
0.05103310630379011
0.013744178093038746
0.07990339561128397
0.016392375489589727
0.015996361705573524
0.01632466424600141
-0.026590212724225588
-0.011835116118134238
0.011133760769103652
-0.015807480847782437
0.03154022836078122
0.008133076349657032
0.018533745570592442
0.009171056772845847
0.008319629367136662
-0.0036768463361811603
-0.02026134701187119
-0.03908873197528531
-0.016130933785907436
0.01586690167437008
-0.008767515879095791
0.013167920501277498
0.014848423321138574
0.024308779846235244
0.007203921066665375
-0.005922685086897693
-0.017978321508984745
-0.04095456859065815
-0.024054875471923903
-0.05210129871524174
-0.08257259436485395
0.0024331937531728576
-0.05988591026746892
-0.003858692764828753
-0.0580899651961343
-0.03665022536067088
-0.029072397233950005
0.023044617418313006
0.02414698896363058
0.02771575955693093
0.026336838075935946
-0.016741150337550907
-0.00832391238415121
0.03013320798530478
0.009069400488986298
0.04483292600329162
0.03632958681300702
0.04409769706172123
0.025597829596437333
-0.0030014835059364066
0.004051643484303615
0.007708753666726501
0.01084984726436522
-0.013245351092826792
-0.025669637510775334
-0.06891084474610822
-0.029730047507190042
