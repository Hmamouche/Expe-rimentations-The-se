# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; HHSNTN
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=3, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.05460465136963807
-0.06836060705990524
0.0477193729177424
-0.019537458599490562
-0.05323863542335897
0.10105832596883521
0.04480938794667584
0.003595179336424092
0.01026976794567247
-0.02864946129035349
0.005222587186624892
0.03906890864239127
-0.0001409486570625242
-0.03930152145853701
-0.04891978160209981
-0.05107253716031591
-0.07649225122943981
0.003867884198849172
-0.052138402399778676
0.008415139602911615
0.009982583786846822
0.04484546723429644
0.03282567456265301
-0.004958418500305243
0.0345831001679646
0.014636390436432995
0.0005622172043551919
-0.018887421851603484
-0.06013320789625248
-0.10389058593110845
-0.09425838655701241
-0.018204696663049615
0.08445344068611868
0.03134593438524822
0.028741877748654585
-0.059469264106382605
-0.04062939438727539
0.05321990081829893
-0.008517367298104005
0.05920911857915024
-0.020443424996204704
0.008128975909141311
0.004282053174823635
-0.07164556861139124
-0.007855008862954444
0.05674241357186663
0.001590748731489032
0.07050677824224927
0.0577093609167061
-0.0090272938780589
0.03073222789107718
-0.028679384356853008
-0.016779125923022487
0.04142475599963629
0.02010498815607532
0.02532458395336778
0.05601268939532892
0.04661628138811932
0.041635556637546936
0.021862208900887376
0.0014214885398214899
0.0068239186544597884
0.004326722957716568
-0.06036319820936
-0.011314506007913123
-0.00956959442597207
0.029920138128298825
-0.007284169701910628
0.036323110784734935
0.014206814434223442
-0.011799931204891787
0.0007097478163038927
-0.0662391306480427
-0.01271408369527173
-0.03753452092688125
-0.023599513722769694
-0.0281851287767565
0.0022284996963501495
-0.12253045903694283
0.0261245878102217
-0.00583546526094586
0.003697886185962354
0.007037315066290083
0.03187516577901915
-0.04091502770073072
0.007975563946963589
0.0037535010349354567
-0.06534185432689729
-0.010250678863358311
-0.07720589833559047
-0.03280082794288906
-0.047812297384058955
-0.0721806985242719
0.056753130698078816
0.07770528517311809
0.01961362573055645
0.03444343963995628
-0.020157706176732182
-0.04754088287083684
-0.09564555127935293
