# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FYGT5
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=11, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.09297868782354637
0.15768855456124686
0.06973760345858365
-0.0925769861761704
0.0020099719320423697
-0.05350212175219853
-0.04345118617734808
-0.04108181129870128
0.0005482680062539047
-0.029568894646064797
-0.09112624376209806
-0.07638843133029244
-0.05169310498202607
0.005942463325613968
-0.056945870089022306
0.03637333848844641
0.03522041691954775
-0.015708490573101107
0.07516115723282216
-0.051516411270059306
0.012266231910045118
0.114670986436473
0.017729834037177282
-0.06826102622205626
-0.04379234174796551
0.012924445040766962
0.0003023760909538452
0.005075137675694858
-0.07691069486116178
0.02936227194595633
-0.06702118406796445
-0.07225368750071946
0.05306500210416337
0.007595800603159099
0.005380484212162535
-0.010522626687002514
-0.06972691312551947
-0.024224380856836862
-0.021050032933027825
-0.028477730045142643
0.06921605005175521
-0.003589487357704875
0.027554349420055264
0.017801753759549885
0.07013077085966504
0.1356051091911844
-0.051735908097585825
-0.02184525309344933
-0.021028577438164577
-0.04803730378871865
-0.0524558388507091
0.04066469887159675
0.07211117384763668
0.008975553889403763
-0.011336730914822917
-0.013261771907574153
0.03662011444172277
-0.02300727562677027
-0.028770404924424085
-0.018673243670726307
-0.020547993648460886
-0.045394329685402995
0.07027081936714699
-0.03802013644710627
0.01715574789913462
0.051146719329930215
-0.012270923918328284
-0.05289314722282092
-0.017184759482668767
-0.028083763855359427
-0.05992553783580332
-0.003833574606540305
-0.02783346257952567
-0.029140241077281585
0.061329272199152914
-0.0028796338978192334
-0.06128344416057034
-0.012719236594279044
-0.06316968457360042
-0.004070910395964113
0.011101045860955429
0.03148691960260347
0.04678149187919038
-0.02313301214205192
-0.04969608800736201
0.021590519741883155
-0.009968114237779484
-0.00834523636326652
0.03967454075396447
0.031519810838774816
0.033661035789257505
0.030865472437030868
-0.0016554753476175042
-0.03433835560285658
-0.005175532232442548
0.009677161607816046
0.003332263570495841
-0.07428608855574832
-0.09024477759114699
0.003334112379426428
