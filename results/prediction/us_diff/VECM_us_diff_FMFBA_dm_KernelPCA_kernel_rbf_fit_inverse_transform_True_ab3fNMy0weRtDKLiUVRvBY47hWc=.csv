# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FMFBA
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=9, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.003099672766132085
0.0019434145617453938
0.0026042509240435862
0.0017428014234576923
0.0022222146243195363
0.0027618736456040014
0.002197770566957302
0.002788419603235382
0.0033380755607028125
0.002994862697889843
0.002656364346720964
0.0031929684767870734
0.0035710637993070556
0.003660759256009532
0.0036584597326081193
0.0026968663707887934
0.0027944487423977
0.004150698459948742
0.0033037521329085245
0.0034380302400888437
0.002901426114484352
0.0032618862702771534
0.0022833333388441197
0.002430904152287835
0.00246072299939349
0.0023189363035424626
0.0032776139784222924
0.0036557972597841787
0.005378181110617429
0.004806262695822968
0.006117107327694101
0.004061322890482027
0.0027110973116917453
0.0060302734138199185
0.005487744305173818
0.004185941787753649
0.005608251946060529
0.0064102528963337
0.006026802496161779
0.006703236471186433
0.007257448869775796
0.006472393854079501
0.006943530096110433
0.006846766095017626
0.006350514243537485
0.005153449836392761
0.004807503964630307
0.005388411644374
0.0019425289710957765
0.0018752388997996076
0.0012986826930419973
0.0022826930050884784
0.004211412341620268
0.003390387526894035
0.004098815305866431
0.0031306610879581324
0.004553496793049011
0.00634035510631053
0.005167131036202758
0.0045241153726126955
0.006685421544291704
0.00840318643528528
0.007146645830655366
0.01008144716922786
0.009352164474776792
0.014685709226630943
0.01830362410303754
0.010857140793770597
0.0200386412566963
0.0019939439026908684
0.0035993071566535685
0.002941082259133873
0.012537906355147866
0.007897133058978429
0.01144383402683172
0.010039176430344021
0.011895794278096483
0.005674845303042998
0.008338238339099078
0.008343349405678378
0.004955629252881967
0.007977799224532222
0.005702409196992967
0.0085053219034321
0.006312620589914867
0.0039134390796943565
0.004571051832070764
0.005866724020634902
0.006599460147809998
0.006423672320645749
0.006674504306706026
0.005930673945477886
0.004458952233492585
0.0031952730361778084
0.003324553146418185
0.00476736546578156
0.004524418869016537
0.0030539010026764167
0.002156332650625818
0.0031874930247943922
