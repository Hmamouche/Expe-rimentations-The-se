# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; HSSOU
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=8, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.08706926124893193
-0.009377659078414684
0.011430852106196258
-0.17878476108274519
-0.10458838087437235
0.027700968979782088
-0.03402577086076543
0.06274875727317691
-0.002868040262809129
0.014819534284149324
0.016031882072166714
0.03741233374100794
-0.09796359210610672
0.012754980582471301
-0.1060153589812874
-0.13309404741569422
0.11230411409062675
-0.046597288576636156
0.024467115806532932
-0.12999529152973746
-0.05711827687811738
0.022214431456558464
-0.07252461625365614
0.0079553663264956
-0.01277396173681453
0.06579307561945205
0.026019773023284162
-0.03706615097942148
0.04265094940464233
-0.023508260237400082
-0.027275947264170625
0.06633523873581672
0.041169215490189974
-0.009117480433852132
-0.0373678407194813
0.02499450223387001
-0.003446794930985028
-0.026722081635912748
0.07647008946434615
-0.09479299890069681
0.02177783565154885
0.08182508376984213
0.019533369702004783
-0.03158057333338869
0.08505469217470407
-0.1152127197997087
-0.022366049867362583
0.024741844140301866
0.014786288388655488
0.046325590268345165
0.045185779399520704
0.03488934659691312
0.025359431416454768
-0.0147391632326866
0.03711923036958084
-0.026465246931851375
0.03622932294042649
-0.0027220329193296047
-0.0164850452498354
0.008811361549549734
0.05774366771191054
0.03817320760779229
0.06513653893591616
-0.007335953704381103
-0.016614280447836735
-0.04221000735017556
-0.0671123958653239
-0.005428112562505018
-0.028412263213627113
0.03668252487615663
0.02923038953775848
-0.0020169998443156055
0.08733317081993035
-0.009847543770694286
0.08206334633959422
0.06801098916293978
-0.098945460265401
0.014176385549842626
-0.05242974383342829
-0.01668483965951092
0.06994599178266545
0.06328518631541144
0.005026094758087813
-0.013160137800196953
0.029113717366128458
-0.04678321042115481
-0.015080246036793771
0.023806498979564786
0.0482613068981963
-0.008024909885442397
0.036358205186509715
-0.010351706720448772
-0.043785344501979594
-0.051128579463079304
0.017633910611824163
-0.0420415563923385
-0.1035449810964727
-0.1401684919883815
-0.03092197065260115
-0.06934282057730905
