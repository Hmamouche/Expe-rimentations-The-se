# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; GDP276_1
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=8, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.005901862945807978
0.004310529133284034
0.006853353487032164
0.007188894056570853
0.0065342580102641205
0.008477087648444183
0.0067097336929001225
0.008243869143710392
0.007965660706199282
0.008016668507527393
0.008814225974190779
0.008367662880246325
0.005709564118689343
0.0073052117851449645
0.006989231383683736
0.0050603382742933824
0.0052658320670773955
0.008482471896003943
0.007911952601966475
0.006183498653260648
0.0069832427785644574
0.008522974547348884
0.005000426619999416
0.006493619199052648
0.007594097089578931
0.007577015264880091
0.005987045308646435
0.0076205277557681505
0.010581192962770122
0.005493880861415792
0.008681530001196525
0.00519751524210162
0.0027550901008305947
0.006354765973469628
0.004382377019391664
0.003826251210307703
0.004971305789038125
0.005218871869751879
0.004962165630236451
0.0062799316046507955
0.005176956694870378
0.004246682127634548
0.0049593120753522145
0.005944425167927193
0.004634254183710562
0.005801903768210736
0.00567468354833871
0.006015155806975886
0.007490548656475337
0.007337592043049817
0.005440930697504729
0.005360801980408658
0.005903455231902442
0.004304625373917254
0.005717331263364004
0.006961605040703314
0.005339187456368424
0.006835762364317042
0.006550683926454081
0.006725153112104553
0.0070461219543864854
0.007809848594052519
0.004515895483719834
0.0056626213085814526
0.005775652033541595
0.006133021095816276
0.006888470679521196
0.0075113602911968235
0.007113476026402979
0.00873669128350735
0.008388814120596898
0.008433300348981689
0.008826869267856701
0.009056046322431267
0.010283743826651592
0.008661692644881145
0.007797826880347362
0.008454740106742561
0.00627449589562487
0.005962580352122315
0.0059859095814855205
0.006017801630288335
0.004962502181709122
0.006773275719196
0.007012405307226298
0.00648834032520989
0.007784567051403374
0.006927937017788833
0.00526518532060901
0.007120914882451299
0.007693327826459876
0.010989988351937282
0.011056040732440585
0.013049775486969934
0.010336318506139358
0.009529107847313313
0.009144991232495805
0.007816833166487751
0.008334495483203367
0.0065734090252892435
