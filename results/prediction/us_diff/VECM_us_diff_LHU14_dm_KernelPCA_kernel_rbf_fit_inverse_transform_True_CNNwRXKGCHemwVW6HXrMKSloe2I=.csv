# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; LHU14
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=8, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.02251011113263455
-0.03332997745084114
-0.057149229032674755
0.01663620002145266
0.0819458601371357
-0.02948772080420095
-0.03912935408187444
-0.041418077759699995
0.02636803062601331
-0.011448535944101359
-0.04059881822594594
0.01910122206931719
0.055965952092298835
0.02376442174188819
-0.027209023766578556
-0.013917490081511885
-0.1027046514576519
0.01962040594690573
-0.019204466891104752
-0.08226893154956424
-0.025187184990106387
-0.05813759955581173
0.05716484875606272
0.016768892086913897
0.024847594582450026
-0.009636172008690982
0.016293390463883742
0.02132063379072789
0.02950504056526468
0.0899138329942612
0.046933362706171655
0.02439205399818038
-0.0317454547155485
-0.00023044291476884832
0.032019465566782826
0.021705246916413214
0.0546905283598943
-0.05610407414708081
-0.03270932609476576
0.04021855958331742
-0.04196118403527161
-0.05169927252802184
-0.047141742874476616
-0.014088894849551557
-0.006355967053177374
-0.01683824002545852
0.011807016660064479
0.003963286530154694
0.01123860614222551
0.03266246566007973
0.023832468158422847
-0.04861847374218322
-0.030430176497641945
-0.04311121508160887
-0.04774342435824388
0.0017601653074990342
-0.032920935611117905
0.007646333001763116
-0.010400018428488964
-0.02065977074815375
0.03889340842941626
-0.004708695171706089
-0.048551634057118306
0.001566833171614332
-0.04976517515720413
-0.0607713240589236
0.05053841336164973
0.03031299273964779
0.02457781606726761
0.006784312709217534
0.07409813920810548
0.01926735202492024
0.060063688858659335
0.07709771051544159
-0.09211075266204521
0.028869622727098202
0.028857310917677733
0.029417666510844508
0.05720611154761762
0.01986874156294045
-0.04384299410596544
-0.044986182714642574
-0.024523352405827377
0.004981476927956172
-0.00644673322326521
0.031990297320433174
0.011003872617673098
-0.0033320431240827855
-0.0048747766169150684
-0.019933153232265848
-0.04336089429860293
0.018461714019120667
0.02946118459919441
-0.029087826890086287
-0.03857446842659404
1.6252704190030526e-06
0.0302224088064082
0.057706054202902805
0.0672283895211139
0.04296872331478984
