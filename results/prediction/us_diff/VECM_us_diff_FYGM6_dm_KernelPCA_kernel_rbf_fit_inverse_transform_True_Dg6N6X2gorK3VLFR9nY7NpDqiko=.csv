# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FYGM6
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=3, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.1489651278207977
-0.06159786057569673
0.07176342383821271
0.010893788147158778
-0.009299815062043977
-0.05066986630629945
0.07565552268321384
-0.04767428459516132
-0.0425491492520004
-0.0764135309440076
-0.037907513580334375
-0.015377599203497981
-0.033661488096976534
-0.06295815025505991
-0.02879408201622239
-0.024786607724487666
0.02309795014784027
-0.0038972545507594695
0.03919254711748254
0.01728953263532576
0.002108926427526584
0.028971115519931975
0.018061025338000414
0.006540817731864816
0.01257089491164895
0.010123485447981789
0.004000135775109902
-0.029515868842235622
-0.04900438193396407
-0.05260018803018639
-0.060233055451504945
-0.0009058655264162652
0.007589318623373753
-0.03217189310871035
-0.015561113553680044
-0.05356277715165132
-0.05153502261171827
-0.024248036071184524
-0.039646794042963056
-0.029935919296834146
-0.0011568933728131735
0.007307138249206117
0.03825219332764622
0.05014144014481433
0.003204167428917177
0.07932561548553363
0.02064010892335922
0.016572440067563126
0.02429790015540642
-0.012606910324717849
-0.014810222486266527
0.01248730179439514
-0.010051864223421395
0.03303023048528329
0.007766474495654971
0.012300349530324222
0.018203002324486173
0.012499573796391102
-0.0007523620176509056
-0.0238030781475178
-0.03316099473855525
-0.017888196348249195
0.023361081281452756
-0.017502483607120402
0.013220750529667176
0.016251969423595976
0.026561618426379226
0.004865218822911667
-0.005094693259749067
-0.0160452246644272
-0.03823260579761389
-0.024459239648055506
-0.05121570789360618
-0.09074155769473834
0.0006769323766115318
-0.07946250448883876
0.011622130138991155
-0.0725095784720525
-0.028006769274548392
-0.03272954028180731
0.01971076242124878
0.039032391333723326
0.020512586438385196
0.020902770354738972
-0.014094347766034129
-0.006387205505918472
0.028421735754618586
0.0032926010377205884
0.04725144201067477
0.039901451267734776
0.049162816174234375
0.026044301613069875
0.010238051558620037
-0.0060489379569799995
0.02961627206145476
0.0053078479243813875
-0.014921847349072266
-0.018087411799381004
-0.06362579956728658
-0.030263203776555253
