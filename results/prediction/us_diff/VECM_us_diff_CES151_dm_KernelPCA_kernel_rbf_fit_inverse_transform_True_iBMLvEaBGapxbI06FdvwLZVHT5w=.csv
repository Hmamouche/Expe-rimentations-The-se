# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; CES151
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=15, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.2489481319280214
0.13320959297648408
0.5328876625837654
-0.1707331726698037
-0.02439297780908195
-0.11830177571791267
0.08735375049487201
-0.0588915949963844
0.200785330097528
0.09980811832817174
0.21381684347405291
-0.08872642024140573
-0.06785419873478452
0.01919928523177292
-0.06876932760186551
0.14566127634772302
0.01599696463562407
-0.03864923428480914
-0.011968935701062985
-0.15622028615059605
0.07311852777187673
0.0027642516984164225
-0.0005658828690231626
-0.02812748145737197
-0.040415389203869334
-0.04154905414499404
0.09427069363010525
-0.01994767839565086
-0.07149174158004012
-0.22072206803878597
0.16146872624447076
0.07595759912474999
0.08205961064645921
-0.12873893865024022
0.02394552244876847
0.09915158936285477
-0.0826734741607
0.17700285754975803
-0.07954745243206843
0.027449344580520944
0.055828699613958935
0.0612130918614624
0.08356023116776615
-0.0025536350617262824
0.0670634090300836
0.03906524080031546
0.05747721099219711
-0.0013256559540047869
-0.13676173477958048
-0.08315013088662684
0.08108682792117738
-0.025323743488574968
-0.026506422256915622
0.08794751692011783
0.07078928660895553
0.006999940752480414
0.02502661349263254
0.06718167512827168
-0.0840048915127348
-0.02172096178148879
-0.058082368074501324
-0.0155253487129497
0.05363835998868344
0.009238072564539235
0.1355836153658835
-0.1934265456338809
0.03822056229638757
-0.02983307004926422
-0.040019635772229875
-0.033667807726513344
-0.12133719408520277
0.016153264392492362
-0.04651334918684152
-0.0026366660523396285
0.0615870770080752
0.1304780668719378
-0.1817757361820292
-0.17178502139326127
-0.0935213945961873
0.15146435051033363
0.020092541985913842
-0.05290721653439846
0.11114314710799106
0.0030981710316772375
-0.01352283903386279
-0.090699936722509
-0.0009008288338098816
-0.08590223238352573
0.05572952820036624
0.04633500513308365
-0.010728449655078793
0.05503908558536051
0.012821665905915259
0.011144074116590817
0.013070246455582569
0.11630318730613977
-0.044431349476207894
-0.06711930829611513
-0.006779113816747423
-0.027950416522459134
