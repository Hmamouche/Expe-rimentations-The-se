# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; PMCP
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=13, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.006650239018020893
0.15359278775031204
0.08660726882235777
-0.0954567345697471
-0.09912735839204365
-0.16794915577720101
0.03669822911319154
0.11468451051940134
-0.040549753075014794
0.1128999758291912
0.17239395857913367
-0.13641072044881125
0.21166773531173128
0.08512045109882886
-0.10334455583670589
0.05116957239377609
0.11851488370457855
-0.09553992805579473
-0.05234934322013093
0.183090212398393
0.03960203778324026
0.04533856558022968
-0.06684477833028786
-0.19187587207640955
-0.07801289844045581
-0.12548587442866763
0.07566527399117082
-0.10156674336251607
0.03919693990571504
0.1716957687989405
-0.1528562914400273
0.11852195990452671
0.09542741028388506
0.036847533471013966
-0.10264271008251924
0.09522144288731131
-0.1397280921039363
0.011451007703896593
-0.002090994052377934
0.0005917637477861545
0.03419189916305152
0.04350078933447797
0.06893818069300295
0.13546034375455607
0.09133355863498244
-0.031423347761659344
0.034356626514965474
0.04367856257705835
-0.0738573701401343
-0.18009479741510756
-0.17194291532773384
0.17266512294149566
-0.10963335961557935
0.0031618152877490903
0.09530727267338204
-0.04080114112849499
0.05184872747452589
-0.04343558356208508
-0.08880353758682608
-0.041927010838153145
-0.11232014505079169
-0.08356076379448146
0.08756936914807273
0.0648965859682984
0.15737413487067026
0.13809305802980032
-0.011989353972933499
-0.07424152696695588
-0.01742004137135146
-0.10616698299210611
-0.0028298603776787667
0.06278770313733832
-0.08120533313214864
-0.01913106392722131
0.18429545325984498
0.10410883794856622
-0.14958903844891433
-0.006618296658355172
0.01865334606463321
0.1602764420811118
0.03341644656813784
0.17221344816944872
0.07323927625002855
-0.04434461400539566
-0.06827198796095793
-0.03473306583107044
-0.11140676092905542
-0.017934212271761577
-0.015824255759150353
0.04840190105745583
-0.02659612385264727
0.022440420576752367
-0.061899031267325044
-0.025048065906909483
0.05540922275916645
0.0036328080979150072
0.07736601646437914
-0.10825404987168208
0.040233585821285785
0.0420761816100255
