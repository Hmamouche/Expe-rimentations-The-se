# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; IPS10
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=4, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.01755293079007172
0.0017488115527033842
0.013756549713965975
0.011242974586835318
-0.000616345739975054
0.005174361387639509
0.010171432131975834
0.00341884099325682
0.0015353131472866902
0.0050330036026999345
0.006491667333638824
0.0003085438434507769
-0.005071197589508924
-0.004889973713216579
0.009200493191995772
0.010193382401422499
0.013050642825576969
0.010125396751211555
0.011494984147095506
0.01612719633155181
0.011725083137830859
0.011354166909471428
-0.0013216409632143661
-0.003965911928189652
-0.005556630491817146
3.1989696484922316e-05
0.004602136818478854
0.00224310912491739
-0.0016322341984387164
-0.007398742278143376
-0.005193587208895048
0.0035690223396517133
0.007576283691989249
0.002378529508403499
0.007699438048979514
0.0051626535554767875
-0.001903417194559991
0.007532531521768523
0.011648069702286419
0.0023766180676355977
0.007100944823855472
0.010272909824892193
0.013354644388519483
0.012256159123974214
0.011949982991496067
0.016154525770191303
0.0031926355138475527
0.00415650649177797
0.011212998270844831
0.008353096117391821
0.009388382167387044
0.015358659493994897
0.012282763232413467
0.019160162030702338
0.0143086158644238
0.017525692345236665
0.02357732330430358
0.019109574403014386
0.017977892568060358
0.017123654582514518
0.00984033880655016
0.008988015705207547
0.0140013659282346
0.013260463582404416
0.01660022033844413
0.017609046529393445
0.011239487547226978
0.010069952625489261
0.007800951597406955
0.0034209122567752212
-0.0035433636114478645
-0.008196052932316405
-0.012636774217842964
-0.020772964133929893
0.002002063812950366
0.0011825235787948428
0.0010617929620989177
-0.0014808928239193882
0.0032535086514340786
6.322674946829688e-05
0.014015037044246056
0.016549321467155435
0.009708879333870189
0.0040958392164419585
0.0017497048630027525
0.005355771888896584
0.006694250724773342
0.0065142059354773955
0.011534918956379963
0.010231936613500432
0.009906843404190156
0.0013810265070214106
0.004066228359781542
0.004911505542175117
0.00849019342992411
0.008806090562762825
0.0038641714328216865
-0.0007737572204446083
-0.002560118285948422
-0.0029138032308541244
