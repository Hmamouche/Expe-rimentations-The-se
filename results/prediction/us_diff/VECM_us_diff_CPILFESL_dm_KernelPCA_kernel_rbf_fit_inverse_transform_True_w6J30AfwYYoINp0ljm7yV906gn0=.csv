# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; CPILFESL
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=14, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.008731834580209182
0.007559298366361643
0.013408075747471293
0.008090423381731871
0.0058757241773857456
0.006236502133966904
0.008394485155471148
0.0065460882774225175
0.003740593378311707
0.003963094741431901
0.00534138525814864
0.002836061654805015
0.006069944210943378
0.007634810545514617
0.007903060625989748
0.006826301504124356
0.008333472805843357
0.007420155950701891
0.006579936652736669
0.007395114464693662
0.0036431143981611683
0.010839443059038369
0.007440939530516353
0.006242765393949916
0.005397085401518595
0.008176413022190318
0.008758079568483863
0.00910146571250937
0.01039001873918175
0.009207922322182528
0.008894970140441787
0.006105023491172912
0.009770932174311233
0.008556131597104479
0.007275551289337494
0.005929167004904937
0.0051725846872625655
0.009631734152656637
0.007740570668226969
0.006265081435817982
0.005792418435758505
0.005794487635396521
0.006715372776803211
0.007057111034970589
0.0055787322908609745
0.006375026079023264
0.006412533555422579
0.005891591080767071
0.00585181914073492
0.004983430594516432
0.006080717210459133
0.007927832509534168
0.004726393782030073
0.007322230837519915
0.005395229369309328
0.004792504327259118
0.004510939732767736
0.004010224363187168
0.004874811860908289
0.004733786313683964
0.005330533366319931
0.005783082447139275
0.006673244771324782
0.005409408525740952
0.005343616701647484
0.006377630086771372
0.004226951129392505
0.005891050045238663
0.006089336644533342
0.005732805951119278
0.0055001470238945135
0.005919004707330558
0.006638587391098599
0.00383396667481128
0.008446255813008258
0.004898734676644784
0.006246643654145652
0.005978387580684592
0.003468201457747172
0.0016247129758720676
0.002410527629194817
0.0037786160786290237
0.005623494552612492
0.006576750937490111
0.004378278503594008
0.005814455084972226
0.006246461058034546
0.005969648952936314
0.006260397054311104
0.006451601142315596
0.006423622751536023
0.008525298725354874
0.008905285973303196
0.005411462443736431
0.0054069987328542795
0.005637123725136639
0.00583354797153616
0.006255562024465505
0.007760314871548859
0.007120204942703005
