# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FYGM3
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=10, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.01435471150877126
-0.06540513303779752
0.14350220075720094
-0.04700366649489931
-0.0038030306020309587
-0.14723043363104715
-0.029170527353923836
-0.037059492007999816
0.05563432750217735
0.024662408604598157
-0.045749774007411856
-0.039491213404901714
-0.051172246973815616
0.007695597963359932
-0.03876331491438492
0.00898076199259768
-0.02689974544970469
-0.11021322624856432
0.08863818138888048
0.016183126385037154
0.04302609766973289
0.07665073606769934
0.0427460681380081
-0.04578294904603154
-0.053360805648010386
0.0789084173299906
0.016727084629911192
-0.031731894661738064
-0.04956708059691664
0.04962004154669987
-0.146915712446645
-0.0366104777567782
0.05299372999793464
-0.0026400131237119123
-0.0017292075298710463
-0.02237428739033629
-0.09124264811892197
-0.05164477015521126
0.018075590934756688
-0.039987769413433266
0.08989250813671613
0.020121356450318498
-0.015231255736277961
-0.011356441717067768
0.006627860818876466
0.07372409009376546
-0.008693396432524758
0.037071559585371346
0.045821512900155036
-0.004260890448333363
-0.033052093420136826
-0.04008017922179503
-0.009431241236548383
0.011280622602566317
0.00027248853302974836
0.02641576880256414
0.034129609017821326
0.011949465244054682
-0.02781673371262619
-0.026465966079797572
-0.013985082984416052
-0.055771753999276086
0.01253917401198635
-0.053970772158424496
0.01695545297320731
0.07470213094215405
0.025263161641492043
-0.015011285165252023
-0.021943260449804745
0.02085430866356312
-0.02090011416478535
-0.006560946946864026
-0.06988530140530223
-0.11843265653696683
0.060784450299261966
-0.024199320788091547
-0.018340335090470714
-0.02456585619185024
-0.03719756400174077
0.03370273065889204
0.008429730912461565
0.016116904576809393
-0.006036742827559725
-0.004167755267895439
0.0026063121428449992
0.022344227641820653
0.006104917593850727
-0.015789229132160396
0.02230102727102054
0.03645275747229463
0.03413351899962224
0.03607979088398043
0.01980897615144018
-0.023124480767784804
0.041244791789421406
0.03291939918708714
-0.005905905155229853
-0.06533307541257494
-0.07888867793613913
-0.015258624978079172
