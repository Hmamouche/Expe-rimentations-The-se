# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; LHU14
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=9, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.024954859210214106
-0.037869381436087646
-0.06441375554263282
0.015862668612470508
0.08434330920666311
-0.02674447022045931
-0.03948990550760656
-0.0473119908163908
0.02625116470644332
-0.011935201101242021
-0.04624927141436671
0.027627097661960406
0.057056931458397575
0.015803079866151433
-0.019433802453081107
-0.010329736704118752
-0.11117330264615682
0.019799115350694195
-0.015644731005711128
-0.08300685506117336
-0.02271590077344856
-0.06356907970675552
0.06097688370392064
0.012961732587395618
0.029542506355454495
-0.010636106176698702
0.01567982429014326
0.022165983395704858
0.029714402346826197
0.09312268282918404
0.04808569510644043
0.022723570257599232
-0.02769060051237684
5.200626002847472e-05
0.03480380780524286
0.01939561378850373
0.0537446593157964
-0.05364471191984877
-0.0398764000444161
0.04396580281176306
-0.042157351193097964
-0.052255002145145536
-0.04757470336070649
-0.017160518985881362
-0.008364885586266408
-0.015693505246984558
0.00904978064545979
0.004894863440994044
0.01459312323187457
0.03668929225597095
0.023917760872850302
-0.05099875234481761
-0.03798911580890757
-0.044880596492934174
-0.04273222268603051
0.002072247028581546
-0.03458299274064676
0.006711800877148649
-0.013306378903972144
-0.02200510070884093
0.036552636506309494
-0.0066442131430864675
-0.05154641586988469
0.0025348544382586893
-0.05095579535378004
-0.06282881166616358
0.05268644386959581
0.03312573420869767
0.021135965729947182
0.008451391810756638
0.0683255858326027
0.022284315239805775
0.05747401379762969
0.07830082698246127
-0.09903639467923891
0.031138609052159393
0.03351316170636786
0.024409340089751697
0.06521823517698866
0.014418896859907496
-0.040597351740594714
-0.04696753244742849
-0.022522582833041344
0.00955071892281988
-0.007097578231214603
0.030456216926944665
0.0072287209696470685
-0.003793443925605898
-0.00584442541296426
-0.017409842354261677
-0.04636754822588615
0.014740243420378399
0.028576734746310777
-0.02909566005435131
-0.038902626964158485
0.00033819844378618624
0.03172016864586426
0.05644188495308842
0.06743112337220694
0.03600615757005089
