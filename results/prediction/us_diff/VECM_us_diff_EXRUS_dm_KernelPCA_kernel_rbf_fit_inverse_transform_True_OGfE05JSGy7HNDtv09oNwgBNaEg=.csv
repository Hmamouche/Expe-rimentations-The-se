# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; EXRUS
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=7, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.0035915901869676
-0.0370212950522676
0.010717309037847603
0.06225212396107656
0.051798331595498126
0.05121652756196404
0.07768902641101438
0.009271249283459682
-0.03202021028789772
-0.10339815093902115
-0.07652135751319811
-0.019270821380253298
-0.07034382636777098
-0.032518810911606126
-0.08328222608378996
-0.06308493273983873
0.0026186434620232898
-0.03883284331833664
-0.04583509270790055
0.004285449954970767
0.03600370472837609
-0.03874000177230377
0.00017229960496109673
0.04867205043933162
0.021688221780425176
-0.009481770413433887
-0.0297876437055158
0.007108046085171092
-0.10312335765243677
-0.07851407418789372
0.007974187895637287
0.03584614004339688
-0.04222419204121369
-0.029412347505737423
0.06629317561757542
-0.001096441251952674
-0.06576408762118251
0.08385450759679233
-0.007860307703282235
-0.021262599972679645
0.03772636574092096
-0.005930221878976636
0.030693460687063898
-0.013544557658860819
-0.019186017549698904
0.025813562395772992
-0.013683110252997479
-0.02464054824819134
0.03247286979829009
-0.0350230611366219
0.024699963459065374
-0.007728069015024673
-0.022687031468329868
0.04890539915802284
0.024245542896674244
0.03365773017092248
0.04537271482805824
0.029989821428796026
0.03720153142758704
0.023495897779611714
-0.0005807119632262776
-0.02986241218885017
0.01406122457500739
-0.017247533424712455
-0.03880390638918545
0.010823490507206868
0.02763525144007303
0.020780070148773852
0.03131104521457929
0.039909295450057336
-0.02158053643247325
0.046899175557934755
-0.03064563856765742
0.012051665424757732
0.027803381947925157
-0.04195598914455185
-0.02231970807998176
-0.0014039286342342958
-0.07280061284423167
-0.0400560034365921
-0.028763483294100122
-0.0671094934367193
0.000684373529414771
-0.008529090511720475
-0.019138232681284156
-0.0012194282689333182
-0.008976238752841498
-0.024441170751667705
-0.006853240565658605
0.018341059150848164
0.013712593648145425
0.0006599584457620631
-0.019202115971128808
0.0303246789579351
-0.034568417220584687
-0.016925207262288886
-0.010236840375881934
-0.025016598759543186
-0.044466104595436436
-0.05982113639945371
