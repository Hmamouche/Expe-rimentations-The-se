# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; Sfygt1
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=2, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.1938360529597113
-0.09235164335863606
0.01005838060356553
0.10818720550267752
-0.023544560069449216
0.019161022905105074
0.056528791872511294
-0.08331670634384988
-0.11501150434934934
0.0011291560447321963
-0.020231690204781924
-0.037857488880445114
-0.054393046389668985
-0.03441410748115198
-0.04224302852383675
0.11410948940735713
-0.023276518476020545
0.09115601545745122
0.08358447112414294
0.010290587908042895
0.007793314761216589
-0.06518553050924188
-0.0016450854519025962
-0.03200221534593948
-0.060826449096664764
-0.05599708982250716
-0.07765221415314341
-0.022337549808350395
0.03300842447014855
0.07833763124117564
0.05119774989230309
-0.04275735295312493
-0.042147025433205476
0.022252278645703345
0.018011710816627655
-0.01195322822761341
-0.018372362104108236
0.016385316428938056
0.039332005547798164
0.009893120244340995
0.027580694950199897
0.0013477664854765446
-0.022729362358725434
0.029613565121236186
0.019728429825233297
0.09518718986609755
0.08017525608651666
-0.03358524346744986
-0.04322877224361123
-0.1206212787622462
-0.11660030007417788
-0.013198257591993258
0.03200747983410822
0.014605028343579136
0.0685067254291013
0.02310420682326062
-0.06624491568593001
0.008431612140371074
-0.019311130428276528
-0.03691775186731232
-0.04403579477920189
-0.03271039315982618
0.012975252089629026
-0.0102445627113684
0.004794248324360687
0.036264630021100794
0.048000327938074
0.0065196472784522855
-0.023657045131114177
-0.03183349738746469
-0.05290345237789311
-0.0448283036182933
-0.05676722773152016
0.055382268441880905
0.041897577677093276
0.0756827770835402
0.03532927317060344
0.0793774160816827
-0.05728431949315829
-0.02521950567981295
-0.037702155465106094
-0.008766806496463442
-0.031996529934457084
0.0721789937321491
-0.013303454462597227
0.018282929423144096
0.024972209891617178
0.0001855758573504774
-0.02082298260262481
-0.003496449252263301
-0.013335867285703698
-0.01351705470060339
-0.0034724773779485334
-0.04506908669735873
0.012607861890749715
-0.017966087087872284
-0.003919392435522213
0.025214547308452143
0.005042524155504942
0.06701492216012632
