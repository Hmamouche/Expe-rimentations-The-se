# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; MOCMQ
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=15, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.07229897129736106
-0.015094644837649041
0.07490911739953571
0.005225103826683515
-0.054351598612967233
0.05516749750581486
0.0816795731019626
-0.03575956540875669
-0.0038218272629892136
0.039485642088980144
0.050902695639855396
-0.023286485837465034
-0.025606330621495074
-0.0404773205606355
-0.01982670281896108
0.035536492735924255
0.05853927601969632
-0.019840121441025316
0.029830016814291333
0.006245932725987877
0.004623393786020865
0.043478915453509175
-0.031420339188630776
-0.007856609052821843
-0.007149443193088063
0.0001484091495462976
0.008971332059985723
-0.006563964873473986
-0.051515644356324955
0.014616267840335312
-0.019929399086305307
0.0073200560325762075
0.010629227108333928
-0.006426098090368827
0.008429109757561587
0.012821129956151426
-0.02428801191089476
0.056552732745497644
0.0019720414300513378
0.019131086127199725
0.06446703640830743
-0.0031058036430793665
0.02259452302134007
-0.014353703468629445
0.007633004766963349
0.014217246130908318
0.009252132996866084
0.008857681836897268
0.006492842159833029
-0.002503785980995643
-0.00704606173734239
0.005142718667268818
-0.0013081687283303415
0.011154239248139809
0.02501679817774142
0.029621284283264
0.015305141260945693
0.007594288116856248
0.030730793015104185
0.003575637790914823
-0.0012732591098276909
-0.040197956301130905
0.024682708812163005
0.0054797378377857905
0.030493362510657297
0.014348519846802531
-0.018313381859938842
0.009142334747091793
-0.022342498617831823
0.00974558111070092
-0.01236726618485541
0.024907856681468103
-0.02002133259490835
-0.05133875148758877
-0.019150088706032395
0.02079989146390375
-0.011595286086846261
-0.015170093834761204
-0.0181708968880845
0.0227990439223978
0.013898528013297752
0.01594397557392167
0.049982685507772746
0.014669381035783281
-0.021422483128439144
-0.016285593382718712
-0.032486355912796215
0.0034476744385517266
0.008783983430159684
0.017288007732977707
0.015784269685257597
-0.005968567245427091
-0.0021286057802101556
0.012713032620019094
-0.0016126746635695488
-0.023081105357297762
0.003303831733790452
-0.05365813207270066
-0.024510978846658045
0.015019609722080498
