# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; PMNV
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=4, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.2767172573435208
0.1126624063405306
-0.09134916204159127
-0.0484697933122311
-0.18161775313542938
0.05555638805321899
-0.054769295530928706
0.021838525057855024
0.00553536818716318
-0.042118351419720854
0.01868930254691959
0.029291076253130106
0.041125001145646156
-0.09170302343491325
0.015464645897537245
0.026220246145499157
0.08733616697828878
0.03316064404732625
-0.08365795948555516
0.0880150095006056
-0.0743420092455735
0.1115294782622033
-0.06952433061647319
-0.032904071539663994
-0.05003968960187659
-0.04128020022756357
-0.028314779915701617
-0.0008724583014563039
-0.00865753954552018
-0.15977243943311714
-0.062154322495070205
0.05174555196313737
0.181565258367615
0.08438818050824709
0.08541824026997405
-0.11724546985487487
-0.008755405272224459
0.005685875562928672
0.014505952853521978
-0.06859986320349157
0.01108487280700967
0.02775516647049097
0.020537150248827518
0.15834740138703562
-0.04666756523158883
-0.014305613012453054
-0.11521341231099487
-0.024489126098855438
-0.03050259963101008
-0.060851516881888015
0.0489973121469156
0.041849466452756334
0.0017521329080926623
0.07059338035268704
0.03462267651910255
-0.04501926429426359
0.043025155110707265
-0.004506243690169144
-0.021696473715195813
-0.02495804420493634
-0.05206682046894316
-0.010455336166551369
0.060094841880453144
0.04207955436851589
0.028150467167473953
0.08471455049976016
-0.057380067018062184
-0.04416499117461527
-0.019713245279500233
-0.05161402721756138
-0.0824580937073059
-0.027932531757581352
0.016868143061566046
-0.019833629365395095
0.1495875585630905
0.0037216736385904878
-0.02874849897269777
0.03257319302799079
-0.13381793444957485
0.04637611055025062
0.10450731115931491
0.11050749573953784
0.054345673664149455
-0.011259098653223816
-0.061372275012792216
-0.021889738425911385
-0.0019817357589110296
-0.04148074435096906
0.0734065158891139
-0.005920980721650242
0.05284581805873887
-0.05253887499761718
-0.04300855734520041
-0.029464681325856307
-0.017987534456966204
0.04391848342437406
-0.03897245649040304
0.02276816813428481
-0.10207946373126614
0.005762879630110079
