# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FYFF
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=12, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.02544910292838916
0.011895774791001887
0.013731083807108267
-0.07937673421813987
0.016147575522325514
-0.09020582491881887
-0.00392500568056349
-0.032760590063985795
0.033890088801744
-0.023494806526741997
-0.03224304113863454
-0.06830221738607245
-0.005254145297991335
-0.02159103960670907
0.04642016711121559
0.06601868125789016
-0.04282528117867926
-0.10292681463943475
0.06747230445955228
0.03622678388953283
0.005239360915841322
0.10121603575546974
-0.007548708857940185
-0.04893816012039852
-0.01825616373701632
0.028148323001087806
0.048115855772985716
0.013197508531011955
-0.0696862508643519
-0.014238247310987995
-0.12999809897348172
-0.08721163055501194
0.00617894333899013
-0.014290855194174973
-0.0007220455200685899
-0.03466095751394721
-0.03929886759164956
-0.011145024181955301
-0.008016677262371586
0.03152458190467793
0.10581749227109996
-0.004771946586322003
0.04182651399306282
-0.011932252439101773
-0.02723847135547766
0.07370333765436539
0.038599335614112024
-0.023969016081227784
0.017641800642834087
-0.04142556470975926
-0.007579374003541497
0.01269997786365896
0.021232576246424127
-0.003337124152691792
0.006796636300893386
-0.006124604706720001
0.026043098608938494
0.0009333795913174531
-0.0025566171315277025
0.006530275638398045
-0.07402404384021107
-0.030608259485665115
0.09239418622839081
-0.0689360902750317
0.03274209910982617
0.02894942041758822
0.0016185883027652111
-0.035061784265752585
-0.015664431894775222
-0.022639057686228883
-0.043557541332631365
0.005846503787785926
-0.10067079784550031
-0.15337922313629593
0.09266396452708457
-0.04743966941580113
0.0465868645420321
-0.07307607547685617
0.02580531486317602
0.021950368271499106
0.01948496521420435
0.015806087048941646
0.009659173490021426
-0.0007509930427689176
0.008646838448813539
0.018582920223254293
0.0141795610590706
-0.03826078367063948
0.03550511416645399
0.029545541667057754
0.041381584666865666
0.020761509045580263
0.022382596148634873
0.002653514786550313
0.026559691854764318
0.009357202177919882
-0.007833969214381026
-0.06349614678355398
-0.08332185983304488
0.03689677884750829
