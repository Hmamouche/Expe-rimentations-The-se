# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; HSFR
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=6, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.06303482326393386
-0.057305678287730324
0.056944837587710094
-0.10295006320446293
0.02133511060019489
0.10160598661742577
0.030794498585281684
0.02735414232190989
0.00940599964308356
-0.07266356168038729
0.07327928077700617
0.07808621834048891
-0.053595871041805646
-0.0036503920729524623
-0.05451757422282885
-0.1251170968378214
-0.05044685899694176
-0.005314791019523122
0.0032564539702437806
-0.11305625333684394
0.03348446628691003
0.013014790139335704
-0.02337470719629679
0.0008034432206981378
0.027995231754308576
0.024666853299864185
-0.016020057851369347
-0.06710404076949256
-0.09618860899984638
-0.060695809467278955
0.06422140667237741
0.06274641806657605
0.10985426126697172
-0.029582153151197686
-0.06359435779070713
-0.028323358162472893
-0.01910410026911152
0.046490685475840156
0.018703503451397326
-0.03505145230086551
0.043660034315792226
0.04090354832208746
-0.03687836503371849
-0.037522627193032974
0.001552785120929259
-0.09371857605786096
0.0017488992056785106
0.008240929874237071
0.05155988359248723
0.07370427740111447
0.021856507075732043
0.020243964616485154
0.034026996148151245
-0.02984379492225627
-0.02134373254405509
-0.026175867997642658
0.018622920256311886
-0.019495028303331868
0.03498168142590846
-0.025579331685529633
0.029977333972282055
0.09463538083949435
0.006779943612850019
0.010062857730708294
-0.03876078007310685
-0.04223570638601175
-0.05219509751711164
-0.024072774628932692
0.045508037560845024
-0.001295162058140422
0.034677416780208394
0.04619655863459782
0.058621109507774785
0.06187127920050789
0.008619641470723062
0.008808658557673057
-0.07031130016870686
-0.0019170252045532608
-0.048485912320092014
0.06135521295047256
0.03086787940595543
0.028109492399379953
0.019450166692022822
-0.09110407584885094
0.002391697433538468
-0.004539882549510755
-0.00210104709568076
0.048160228946401984
0.024097694274094653
-0.026831550831980477
0.013790443799534036
-0.017623321969423653
-0.04814687610819835
0.01153018632019521
-0.03433673409312106
-0.04389706568496635
-0.07208306820983415
-0.13434357060400276
-0.0049662670747328205
-0.05327977874891478
