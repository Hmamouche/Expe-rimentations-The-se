# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FYBAAC
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=14, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.024319028268095545
0.09309700423619109
0.05051859766336017
-0.0562931635269238
0.006408380923995656
-0.03498595995704581
0.00853932099535485
-0.014099647728125934
-0.0701421859796387
-0.02069213673610954
-0.13805482080684436
-0.08833432616000307
-0.047947736866416846
-0.013897891675614237
0.008134775785877242
0.08907237995402394
0.04587556769062968
0.01694119257058237
-0.030683013623256396
-0.011380112784376915
-0.037438638143997775
0.05084879481499073
0.023656566235067986
-0.05674346670574031
-0.03493228861292798
0.0014701442244891743
-0.027132513548347118
0.02836038855854721
0.012563380080584271
0.004400858231790971
-0.016709410296086784
-0.0673662287624272
0.03135612931029205
-0.0015398995390766765
-0.03508038431045027
-0.008176372512297775
-0.08800421579622525
-0.00043822685209050916
-0.01093804917361281
-0.034834801717634874
-0.021301637928894838
0.008273118353639787
0.010210171288379755
0.04125554372033451
-0.01230760238388143
0.06466632585106655
-0.006366988314879633
-0.04217259107474539
-0.03643662377425328
-0.032737463436719
-0.02576370194703521
0.04819094906533016
0.028164888015150075
0.0386247692813914
-0.02121407343581143
-0.01713936468634085
-0.002925116897733445
-0.039045884480791854
-0.02984149536989175
-0.018050967393854665
-0.03898297613561428
-0.00488525222164419
0.04004305599685988
0.026036862494563263
0.05244041390528765
0.04917166959702303
-0.02224918769494013
-0.021148114866812878
-0.030690107886822342
-0.032497263002862764
-0.02387088320856848
0.04494743629260832
0.00864853197018412
-0.015085424700558583
0.018277479468213623
0.002964977550918452
-0.08969030250280949
0.02084135163915382
-0.07480304863778928
0.01948521277375154
0.03283060669427554
-0.006122574629038596
0.031882518781280594
-0.027330839066933524
-0.05562124263526953
0.01708440869707093
-0.03969821206461792
-0.00478882690431468
0.007974034916215575
0.03401957897619401
-0.0037964822298979656
0.03888005178057782
-0.008521336206853025
-0.03527848336638264
0.011054406039808434
-0.006771473141812301
0.02116225071537196
0.018841435804826105
-0.01699669320792015
0.002205723874646485
