# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; HSWST
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=14, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.04822576941994752
0.16632604150077662
-0.42864922357452345
-0.041945440233293295
0.07080856091871834
0.08733572060704162
-0.027742228868443082
0.0665520233596012
0.11028156373694697
-0.06170085680150581
0.07546722176768839
0.05943081246117526
0.04152893982551025
0.11413488782567056
-0.1396979288085233
-0.059775145977475616
-0.03957469703522859
-0.12516741202046688
-0.09171041159181909
-0.09464173505820497
0.0105640511373647
0.06848300829810826
-0.042559077702978496
-0.0008841643034424897
-0.038999870395400676
0.07577712447947105
0.06684030315064564
-0.06549433901340788
-0.03883837584706595
-0.10159275840676085
-0.1961773251468214
0.027012867929934348
0.0125495947795735
0.06214949861567508
0.042222209886273294
0.017413951132560612
-0.12944772512097608
0.09619982650621813
0.018524686719430954
-0.1485067879285624
0.08899459722294445
0.0027311637710758993
-0.009864343370060313
0.01599570225938398
0.03556700739213828
-0.11051844346178283
0.07287272513937013
-0.05539322461219649
0.12919515978892204
0.028314969227993425
-0.054243327706610965
0.03932104275368495
0.04533648539234851
-0.09675191445405673
0.011291929596091162
0.010160109872966842
0.046427664852674466
-0.006324907433513355
0.060873400330821066
-0.018477933498608743
0.05641211992099718
0.024993239276659338
-0.026163987703527555
-0.06325039313810232
-0.003917627687699212
-0.03979448590163623
0.003622736006332443
-0.00975924548757269
-0.07213280152341264
0.07802030188353476
0.0927313838337002
0.010611345806991085
-0.00811604618781959
-0.04720850921546312
0.10645340398302848
-0.0030605244536687937
-0.07324017578492294
-0.049447368362979624
-0.04170566591600414
0.05355544487866746
0.19334203295734936
-0.06252968796323762
0.03751429499578356
-0.0011900002709696392
0.012996133154349023
-0.022782924108877625
-0.019588720010623298
0.11414234592760894
-0.07974144229866052
-0.011127549030563249
-0.033284489899554716
-0.0233353338708898
-0.07219345191938117
-2.1503039031976843e-05
-0.020651851987684506
-0.11877477493653564
0.05210861239496062
-0.12342513903535006
-0.010271019884805152
-0.189106449078141
