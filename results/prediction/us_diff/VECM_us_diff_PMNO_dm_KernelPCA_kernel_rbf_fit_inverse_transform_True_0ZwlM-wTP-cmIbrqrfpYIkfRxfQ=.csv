# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; PMNO
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=13, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.4994528131701664
0.008343077594334447
0.02325267377395082
-0.364773726542539
-0.00336283688844935
-0.018770674954232874
-0.011624512951280558
0.2077559860430963
0.015863902939274754
0.477699258994243
0.47307359404811317
-0.011084478013624464
-0.0901143432673678
-0.16926480379676526
-0.007242941688941171
0.041275764881521806
-0.03875600586793098
-0.13833137762253048
-0.04023461480398484
-0.10628266294655075
0.08011627250225875
0.15709732788370234
-0.22617882910704762
-0.0861291465106633
0.027841324340459658
-0.024423691045574347
0.024969696872048357
-0.018406219730487713
-0.08289810682286303
0.22797117649793586
0.08756085047632384
0.07457113272376967
-0.04654846423411703
-0.05811311853353031
0.0735925196922296
0.08811859159455752
0.03887813588215646
0.11640745512944246
-0.12665849771037596
-0.014005971843905469
0.15859031088687323
-0.03627039397917084
0.06839985515402047
-0.22214587578846462
0.011354693959459941
0.0415186571411186
-0.11949568807087568
0.11114357985489969
0.02982922010268624
-0.02224100691657936
-0.07095814587783074
-0.07481872440732572
0.02096640490650968
0.03742082906510387
0.017318592402579394
-0.006368683765494501
0.07217592709772645
-0.08119120807957331
0.06842793883085399
-0.03628647692658629
0.002684860900558146
0.011636398299435687
0.029640724832414308
-0.008088766531315583
0.0971353261149133
0.0766876540827803
-0.006288503055356522
-0.022718728505045027
-0.0912344406328579
0.049222027678585495
0.014476566356953498
0.12128460982211724
0.027631901595351872
0.19437245734824735
0.1699454577332547
0.046663351476458954
0.03361931237172563
-0.11465301410495293
-0.14876260620797627
0.15498211527801714
0.017578282489863154
-0.04830975502608556
-0.08659518426255533
-0.1691054253012274
-0.09263198155464664
-0.0464274948823694
-0.050843946680409924
0.019723946084193034
-0.06212166327922003
-0.0917583408786612
0.05377789439868608
-0.0426727468679856
-0.157591626188537
0.03910092522440668
-0.018140553193411547
-0.11425884107495904
0.056988166369100875
-0.14911409857784466
0.007998773689406522
0.018457475585767862
