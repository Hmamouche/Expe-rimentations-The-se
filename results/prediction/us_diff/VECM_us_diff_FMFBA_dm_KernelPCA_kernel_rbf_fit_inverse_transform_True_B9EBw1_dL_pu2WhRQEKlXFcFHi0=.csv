# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FMFBA
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=10, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.002945397337815523
0.001982907154198313
0.002525711728970261
0.001573166820240575
0.002271775919840675
0.002732411563657319
0.002106175542386142
0.002904116601134678
0.0034003643907206853
0.002894900582645585
0.0026064821509063046
0.003247951960501311
0.0035492728442075355
0.0035735829154311936
0.0038598739424716244
0.002564186550198041
0.0029255436957303293
0.004104622421556642
0.003100515241724144
0.0036070352135044786
0.0028007697901909524
0.003264907008214099
0.0021874531783578803
0.00251892146553385
0.0025181808963258253
0.0022775681440880134
0.0034569899669183494
0.0034719133599353086
0.005276525169065354
0.005054186917601283
0.0061593124700978445
0.0037496342247796835
0.0030344712747209512
0.005890476092208034
0.005503524155117123
0.004214040741669581
0.005641329606118697
0.006381095604480835
0.005775811573621264
0.006849089817390842
0.007355711559673177
0.0064351215110260585
0.006873853286971378
0.0067730880139718445
0.006472058740430144
0.004912815099223005
0.0048756063912730775
0.005493249094923675
0.0018748125576003216
0.002120964713303478
0.0010132481012188408
0.0022460954986528955
0.004259623803211496
0.0037433408388097503
0.0037906645259321
0.0032829401681948108
0.00457253514178019
0.006555725540097202
0.005011517238896544
0.004518579557490056
0.006779780373281494
0.00839510320638239
0.007301679067316149
0.010072464386441356
0.00930733685124209
0.01506052149514153
0.018081057275974567
0.011177472031678674
0.01994663063961493
0.0019309528376592615
0.0031479846753977936
0.0030711634661118453
0.012966491434259403
0.007767510075581394
0.011488186039656617
0.009746500616138905
0.011633311244026986
0.005407850752704004
0.007965223164291359
0.009258848419366073
0.005102723452501517
0.007849841658988709
0.005547981927718554
0.009071260090206626
0.006266750729968625
0.004204154732375379
0.005112101797987387
0.0055467023489543885
0.006499085327049873
0.006966702986921893
0.0066587801245998925
0.0061234231754212325
0.0038548903456124403
0.0029534814531799922
0.002714976959006006
0.004819893354950805
0.00466045857714219
0.0029087946203750557
0.0019785391599605236
0.003231230077186518
