# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; EXRCAN
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=13, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.05942071545303727
0.06875626282974355
0.03103891321261036
0.006120857445114267
0.029810437329644675
0.03704439610025575
-0.0016795737414603655
0.030708459595296767
0.06656733211991119
0.00899487428613473
0.01411337129174663
-0.032332543727419984
-0.07364980688917248
0.018889932067744454
-0.004668857214434399
0.0038592909037330134
-0.03876457017513782
-0.004640964243450568
-0.059339457015328695
-0.06458148371132472
-0.06753968516278377
0.0018962065834010507
-0.02707567431774109
-0.020280602889068286
-0.033228482573857215
0.003012359769503339
-0.01083369301572899
0.0015807641759671386
-0.02688495679241222
-0.02849095439114238
-0.03402146989962814
-0.001476236989290833
-0.005709578223462117
0.005843925876172641
0.027606940839451734
-0.001677971155674589
0.024347777242295688
0.05603096301887214
0.022930960474308388
0.07168964400574183
0.0686030632554956
0.005762927808097267
0.033439378806422906
0.052217677542190956
0.011863804060011817
0.017527647375450116
0.020437014696490046
-0.037571816908549396
0.043881971317984936
-0.072496375810863
0.005430599889693244
0.0005070441245338482
0.023287324703575186
-0.02853738011293732
-0.03420245330158706
0.007592509063752188
0.013848358674944845
0.04223406334521535
0.00800791997685604
0.027428655906164973
0.07815147742928708
0.0580151872487519
0.0593227522573543
-0.03609864384290728
-0.011575064218364427
-0.01556271166494615
-0.024230621325841882
0.0030832120133795823
-0.028510321307645418
0.05521642986845337
0.009803326112143698
0.024893325646117105
0.025837524847076944
0.03741527129884646
0.02856761329457721
0.005945949925803465
-0.006885626154634491
-0.04070828231019413
-0.008334619560035814
-0.02058536298017575
-0.029948666207376816
-0.1440304938965177
-0.023380019382703256
-0.05995853283182626
-0.03173218027179829
-0.03918408153762625
-0.02409352965953065
-0.04312817821721142
-0.048472622827620604
-0.03002936286482666
-0.043225583305939244
-0.04878499819118094
-0.005132274288250778
-0.014359960273055238
0.022441858837204297
-0.034105237609589396
-0.006147210506108912
-0.10363111598784538
-0.03964310531011645
-0.05788695438934061
