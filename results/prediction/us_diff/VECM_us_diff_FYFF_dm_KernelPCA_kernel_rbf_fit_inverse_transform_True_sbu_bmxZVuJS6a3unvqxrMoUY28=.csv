# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FYFF
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=1, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.001851171400133425
-0.020175429589272195
0.013903184510240606
-0.0006464758442231693
-0.06129811473525456
-0.03868413877346955
0.0070795521471449675
-0.03257010400587551
-0.036758415134088854
-0.08259442787606924
-0.025442204194379127
-0.007687862898611663
-0.022022811097787795
-0.03423584219233598
-0.014556169513087039
-0.01171705617000745
0.03224474528330233
0.013055232965955255
-0.016713323516977588
0.03073970102424084
-0.02338638243881924
0.04075936374068212
0.008591433119096256
-0.006889128718157689
-0.0033255676381922367
0.009491092017391636
0.033823576069960344
-0.002989797287649528
-0.06435631133632269
-0.10114715857788324
-0.09702143276279965
0.008867890524056132
0.00847865458055552
-0.026125241516421455
-0.017982922197033
-0.06119083730299032
-0.04667639922150496
-0.031313662125477995
-0.047197655559467325
-0.027594064441491796
-0.0014997858218794447
0.010536720804731814
0.04718914030566412
0.0696836291612543
0.01606501900997587
0.05932732448272587
-0.004674058145538935
-0.011737186012144026
0.0010631901451637615
-0.014185270674308692
0.03289853580550055
0.04075535290167096
0.00015020709661829872
0.03725641033706144
-0.011172584217802475
0.013735500510612805
0.019580425303465834
0.01491405409135474
-0.005908970461474232
-0.018623966915718562
-0.03333986877221232
-0.026410604833823403
0.0276802709441113
0.005763742691213268
0.026903290315443393
0.006733015285382924
0.013510034956504751
-0.006416426110352248
-0.0221153690111668
-0.03689014770158205
-0.04229885578342061
-0.014722027093158691
-0.04662581268622114
-0.08589342353702578
-0.0005960400463775814
-0.07336878627119903
-0.02317328817834205
-0.07051512331811016
-0.055307450531712876
-0.00437204835475641
0.02565971026683866
0.05731490598072153
0.042976816251387835
0.02734851779775145
-0.02283596413469323
-0.017960384742404426
0.0052404679749824615
-0.0013870098539231859
0.05508132621118371
0.04166155031772784
0.06059138400772125
0.025180048669730296
0.006209629409846292
-0.008978440653201138
0.01294913385153705
0.026440949632160416
-0.017710531889212427
-0.019910795012990465
-0.07067108949467853
-0.04331164864287629
