# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; LHU27
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=6, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.006178657471822363
-0.13274798340067703
-0.12627650382863806
-0.033936962106661854
-0.0305527711973895
-0.026396056363614723
-0.024196504725358323
-0.005074816429741107
0.007933156188491105
-0.019915287052366344
-0.02496183482840708
-0.0023629237787148333
0.012780187580699424
0.002941916982036507
-0.025202020270836956
-0.021474252793537135
-0.04944111576176313
-0.027362264470243217
-0.04107218623642542
-0.006763382310389739
-0.04740525311592659
-0.03232810985966165
-0.040007419358813114
0.004533340980113843
0.00781376869138869
0.020773109349589154
0.001885754127850242
0.011734297383714537
0.056862931822899025
0.07582581331504698
0.061384990692814945
0.03826827179275823
-0.004073283594815505
0.02343655186429294
0.09444972534871059
0.08652979020003167
0.04948470091979399
0.010612583274376568
-0.038481338127322344
-0.019918786355930015
0.02473934353681189
0.012866115007477696
-0.04864730647158898
-0.053371571991045655
-0.04536422507255439
-0.023037694786377745
-0.06960474752500083
0.02090243586271306
-0.02247743082664027
-0.004923890065831756
0.0051743649457023505
0.043042769178590914
-0.03235394189792943
-0.04666668334560899
-0.02805520365805288
-0.016574456294435984
-0.016763707462982266
-0.04601045495578193
-0.02088471318521609
-0.018708093813183134
0.012599481585486814
0.01947323511407164
-0.03092749141296897
-0.015346789174343562
-0.0038084876691523656
-0.031312331821482256
-0.040307836340207445
-0.008499746702798305
0.025353677895376764
-0.00010157808080162208
0.04598895330170097
0.03792290538169418
0.06188677430783021
0.09102330414033265
0.05112689247038853
0.04824106031657749
0.008855019046222224
0.04609661466826511
0.03177373014536434
0.06279854969481186
0.02324103218337181
-0.015670348363984843
-0.02547144619509526
-0.016422935825243255
-0.049688387427577654
-0.016387954084364845
-0.012440608214387727
-0.030486157193141242
-0.037055743737876595
-0.010683906058798907
-0.03576397704423708
-0.0133045063437466
0.006377578060227658
-0.039580197682483484
-0.02244025123995634
0.005970690982204792
0.04481916803462679
0.03345560846065366
0.02155421054430172
0.04131831657534102
