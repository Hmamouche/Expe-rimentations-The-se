# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; GDP288A
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=5, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.004353787603087437
0.005063199142617175
0.007389735841113452
0.005740454871069926
0.004727557717136838
0.0039956662553372185
0.006147983727627342
0.004299675062771922
0.0038248179698041488
0.0038722039236687347
0.0034758091166315675
0.0026244685615239385
0.004137905526593393
0.0060330158964487
0.006742356253440668
0.004702784224439307
0.0054933248208135525
0.00432363551063686
0.0030645233815337096
0.005221489505500997
0.0039768720068744275
0.00414193757241239
0.00502818352916632
0.005744729728542037
0.0036121614166421305
0.005906562719021851
0.007619162088081361
0.0057549448343712315
0.007461324642124242
0.006670108983261856
0.004541828428998841
0.003506376196061825
0.0048614374310259984
0.0030042695538516464
0.0019623932122732786
0.003935360962948538
0.0031789918047964326
0.003437549328625281
0.004579187339829735
0.002508471995362112
0.0027464480416945804
0.002439554846956513
0.006189315009964265
0.004135729412733061
0.0066068664880362905
0.0043683592287333504
0.004450711849711081
0.005442132446891304
0.003975098119784518
0.002799229696387525
0.005269686995156047
0.0019852097754674593
0.004341948523781318
0.004375852586594825
0.004073732380476685
0.0037022869196149215
0.0032642239377696836
0.0037037608350292033
0.0012198035078534565
0.0033996563014936114
0.003063678608939045
0.0032874476174017293
0.0037589853634217726
0.006666859050815812
0.006144740016758132
0.008824122455266316
0.010256462573717333
0.0074788938293440035
0.008773383967420632
0.007465698615173471
0.005270590995227692
0.00565299653678768
0.004722856949070139
0.003179786173041187
0.0037056135126191637
0.005338460696659308
0.005143454898478145
0.006808940506656929
0.011018726369330548
0.006497597324236171
0.010452154855981718
0.008191026025682929
0.005656295557533247
0.010105225438216698
0.010312128981129367
0.014320863024822604
0.012835661856837504
0.013217595113970032
0.016597652081080574
0.015632791016317482
0.013985015544472451
0.015861881619747956
0.009307693066645913
0.008094635051544265
0.012817100813694193
0.012412196689889594
0.012524640612970026
0.01624439622353027
0.015710935664389313
0.01797796875407868
