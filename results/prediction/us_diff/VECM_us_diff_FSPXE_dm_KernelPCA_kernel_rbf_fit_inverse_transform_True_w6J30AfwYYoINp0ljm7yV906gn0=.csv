# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FSPXE
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=14, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.03535481000594322
-0.12329947736667458
-0.0370925169922628
-0.04581893631000536
-0.006803427262731657
0.004641204949337632
0.045677172223438825
-0.01370172462374655
-0.003792507161557593
0.09564605515260524
0.1251539803207317
0.06577662536734702
-0.020989861196428976
-0.005626698849982334
0.060496798519242265
0.003981934705052101
0.026494614570487313
-0.0694931129104095
-0.07109403602146483
0.05285314671000321
-0.06931721290897072
-0.03729282348289768
0.0015284372184258717
-0.029021822134164644
-0.01615119588302404
0.015024437193429007
0.0025076820468873866
-0.0002161139712540767
0.010898583261935865
0.0027839106627837434
0.11473617798132014
0.013349965544285228
0.014590475170225152
0.0401107012545197
0.05264629658508235
-0.007476820708538404
0.011765137194079258
0.059001303403588064
-0.004799591368677308
-0.022463407466629608
0.002648912623692483
-0.015847563540966787
-0.0445757920517787
-0.03504145541411437
-0.0109617440701795
-0.06232818723611921
-0.041256868892346546
-0.034291337908948054
0.041197409529661194
0.029819716239722936
0.01514807795298049
0.008225503343296203
0.0043319669166023474
-0.01752809003024426
0.0435984734053928
-0.017790429563903205
0.03753806886588494
0.025339132393166146
0.0306225845106482
0.06531659211164763
0.025104081942268677
0.062320301858912346
0.07839606090056929
-0.021059156526646014
0.005464561115495534
0.020550139277365483
0.0004716323114996347
-0.004470711644530688
-0.0020500528068148373
-0.019844006978213945
-0.013901572148917508
0.007385356502421355
0.027232826342248665
0.12034371100789903
-0.09334268779340449
-0.11771419769660765
0.33805253601334223
0.2791307319670522
-0.015675236846088195
0.014377833658000633
-0.011808616640889865
0.016976278693369085
-0.05701268855928595
-0.11769302407045391
-0.10613947258463484
-0.035292600720922426
0.018757699393595104
-0.03061258684573561
-0.02850103824589684
-0.022696359626518562
0.022894707692501525
-0.06624910821526656
-0.011050429203411483
-0.021458989727472334
-0.039015867686977936
0.001880844208133725
-0.012392493313111118
-0.06207906106530799
0.0030025878867157435
0.012221866397589267
