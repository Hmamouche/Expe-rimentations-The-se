# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FYGT5
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=6, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.14196484543558424
0.011879791150676333
0.12277137953317592
0.014381730311044044
0.023847954919932426
-0.048104177238851374
-0.007514221282228718
-0.07149570673210737
-0.041593633704056304
-0.08154588775610948
-0.07365207737840693
-0.008256339990109423
-0.08526157575627458
-0.010023610936981255
-0.07287942278031935
0.00014409771294223404
0.07956525398850667
0.04729256528093925
0.042724779009696984
-0.027394818212768166
0.023432276531278207
0.06798685072656555
0.02019876589312953
-0.016791329071421564
-0.026148607639665963
-0.02572681341487323
0.008975340934716663
0.0018424657688260078
-0.06020179508434774
-0.031033454190713718
-0.014653360684650778
-0.053384074243438155
0.03710509899148242
0.01893426013483052
0.008353055536380681
-0.03766091055353875
-0.06052196661286369
-0.008422442395531466
-0.04874446768104447
-0.02253054828393607
0.005882583535071719
0.005408719782843546
0.021970537043555057
0.050761770731354544
0.05503739186383253
0.0875533924344051
-0.0188037888688523
-0.02075780714486452
0.000523947709026468
-0.03291665883390911
-0.018263870062266117
0.017233091779296134
0.0012688011583006295
-0.0011544205982416036
-0.009088789811658242
0.038222709093199976
0.029219953006310556
-0.012180936831648986
-0.006619306796387627
-0.03948737116075829
-0.018821945634846446
-0.044656870172607825
0.030517257367444928
-0.034239369828976185
0.017736590061967546
0.048561638159149205
0.023741224791245193
-0.008377164758038236
-0.0033421210541012356
-0.033958481887255365
-0.03054329077827067
-0.022975467360746127
-0.02769162932756416
-0.06832415055738818
0.01923585985462981
-0.01764090619507934
-0.050239833521602786
-0.017653302947752945
-0.07113701315531584
-0.0071794825442501355
0.04084368270586459
0.02661266648006535
0.05429061822320677
-0.0013648284320467163
-0.031264066727965834
0.024648074154280948
0.006642401372557215
-0.00032455648057964047
0.01740868011813923
0.021212236409336824
0.036599089881678995
0.0006412888368439729
0.005304253553323897
-0.011526384854987973
-0.016598100871158768
0.04561668072143285
0.019716069411802106
-0.07465836821485745
-0.06963478072826838
0.018933868281162412
