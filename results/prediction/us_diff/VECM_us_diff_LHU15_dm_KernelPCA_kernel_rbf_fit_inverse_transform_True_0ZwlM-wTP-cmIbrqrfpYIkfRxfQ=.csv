# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; LHU15
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=13, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.01902337574480644
-0.13423285455219805
-0.031098508475917424
0.022132648851027364
-0.011641446503824399
-0.020247632113638447
-0.06424977161009494
-0.022974113035652367
0.04199126641257128
-0.06906477338319508
-0.02184670025602438
0.01841952195006237
0.00046129941617217146
-0.019969042128851658
-5.139452589435038e-05
-0.04443551588570628
-0.09114428801742339
-0.017757375637434864
0.0007288090161367347
-0.04801465434373445
-0.0021212963247574067
-0.06886866803232816
0.01801934218492909
-0.009989578823484578
0.03436880962958462
0.01168639104733203
-0.007462573255701294
0.01525780997867098
0.02789378622863682
0.076257339194644
0.11107714252400709
0.021332900023484768
-0.0314615807293406
0.04462942543663888
0.06924737572040648
0.05479002187699858
0.04856909141013055
-0.022580984132852637
-0.039725131661373404
0.0025714571724883596
-0.04341371781538912
0.014828094439499015
-0.012552842996659952
-0.053561957914837297
-0.04723124471452587
-0.028855788019729912
-0.0406085853384532
-0.029820405040620536
0.002115483224040331
0.06338648253549556
-0.006059945164054271
0.036285858336338114
-0.05276153756125826
-0.05287446607269769
-0.034586245134756055
0.005847685556795625
-0.02929903421534931
-0.029067463887195624
-0.0071825705832213056
-0.03641668869119283
0.027445819770834826
-0.004422774485570633
0.002832805175746595
-0.04555854156690278
-0.023819126305267377
-0.03789725835981909
-0.005143963054165285
-0.013906238596563545
0.025688673476962826
0.007363953148577112
0.06012358888190797
0.041943620576305525
0.11649573794331394
0.10824906412245204
0.0437598047880189
0.02314874206986181
-0.04414977312030667
0.02466636496261691
0.050859980564627955
0.048929458228917436
0.024537369500311112
-0.033758419699243154
-0.005790388085468003
-0.021950090334082795
-0.03510118395857956
-0.0005640370941562695
0.004011177277259833
-0.04684451066457936
-0.03235319848860889
-0.003063523804877623
-0.019297914506969625
-0.04022216590848476
0.043751071440230146
-0.047719670065414804
-0.012832227200887367
0.001777702246022232
0.039169114842497
0.04706125899366086
0.021936887455906166
0.06524068475387827
