# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FYFF
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=14, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.060005145497807014
0.016848483139711976
0.15063014245894096
-0.12139840754092285
-0.029764639826253696
-0.052130757973261874
0.017860900561049387
-0.05119419273059477
0.04034501500036877
0.007954483606444372
-0.08969677788795682
-0.06935261368616523
-0.04750569671666974
-0.022420849085108996
0.09114247625689237
0.07010567728675161
-0.0423512373461163
-0.0784148926514256
0.13322103143277744
0.04797178694628705
-0.014790798718242855
0.16410902853612
0.004300043046276174
-0.07269884242736989
-0.0389069478255742
0.010466926669506917
0.040698388710861705
-0.0009398003327049341
-0.05296640031020565
-0.019253706556674167
-0.1453480835324775
-0.15125276930369413
0.010651145288409993
-0.01066004846345005
0.005391810996512439
-0.026386753951748228
-0.030545210301544494
-0.07070327003311422
-0.03806926148972953
0.021900635033231496
0.0602221128572241
-0.003975338739294246
0.030031507939272683
-0.004662475788377774
-0.01137674428998685
0.0761962548776128
0.05183489490131571
0.001190309880107072
-0.021383402671337415
-0.02729564624916892
-0.019768254597364296
0.013112799659276513
0.014465602026497893
0.016094743490485057
-0.002364380582603638
-0.005003797061825833
0.0231197322294141
0.024087516062785176
-0.019213178700365298
0.005945890008819458
-0.08877773749596038
-0.05241209994211089
0.08294591374759358
-0.05845342544402904
0.027581742863072904
0.028764594186424568
0.010743464376099455
-0.04568613377055737
-0.032613838701319384
-0.016986597653795107
-0.05053485039529118
0.012647068137288917
-0.0758647445770748
-0.18666547649783496
0.09602064768615928
-0.04496523088438816
0.06064923192076761
-0.10302505133479357
0.008133754610742911
0.031044685518037053
0.029390218947031876
0.011516953821756045
0.009261084711862978
-0.00037468878565099714
0.014344437291995957
0.01804074647035515
0.014297619085227645
-0.04479724403087726
0.03764696172658863
0.02085067756173831
0.04756709300209836
0.01910490255987775
0.025816659808711378
0.003845610086011567
0.022941513345291455
-0.011565307663747271
-0.023850611048571682
-0.04852147250649176
-0.07929850725322955
0.03091765290869417
