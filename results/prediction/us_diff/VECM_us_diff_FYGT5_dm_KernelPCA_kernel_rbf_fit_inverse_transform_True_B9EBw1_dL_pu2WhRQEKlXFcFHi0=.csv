# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FYGT5
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=10, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.07982512958879007
0.12993670283074124
0.11342549050218065
-0.022890944808887426
0.029481836141950542
-0.06004141013111165
-0.043988141641632075
-0.05230048497393418
0.0019162991583687866
-0.0406998854850653
-0.09146032411019002
-0.05989312622013006
-0.05555109589380351
0.005946367081926536
-0.053903703907944314
-0.000944904857862628
0.04651357070468842
0.00544329363129345
0.05764517085770349
-0.028196826996180507
0.024097056200031976
0.103898968293059
-0.006037665796041883
-0.04220599798081837
-0.04803793725103532
0.007518598479810276
0.008916939711184173
0.0038648978261271415
-0.06050428545961232
0.008975253513972284
-0.0662317216828468
-0.053397685098576515
0.07578572154301615
-0.0019020998232327016
-0.019037008068082763
-0.02325390690351734
-0.07205310135542936
-0.013076513041298324
-0.030213268363231516
-0.022677553844862565
0.05317032298810757
-0.018668288316385905
0.027550345287287083
0.02858657419983158
0.05952012885876472
0.12012655128942727
-0.05639179003380096
-0.016663650324487588
-0.014407784935482773
-0.048135875199776135
-0.03877235412884568
0.05068149852644789
0.05214637558896778
0.0031259756122786744
0.003080336958621289
0.00555743183374607
0.02295385764759834
-0.019251106448794922
-0.025459652554218566
-0.02444594923632753
-0.011065745988303954
-0.05353241814561229
0.05900814996539906
-0.028482334217318557
0.01917381046118919
0.04886725203404131
-0.012830761984370092
-0.0655523712993978
-0.016603852097176887
-0.029037454480923052
-0.05642259179504219
0.0009000242252409901
-0.03143317624067622
-0.039841976498113815
0.06598904705071948
0.0006781458590595507
-0.06836643681780488
-0.0023755282944498968
-0.06474607116078858
-0.021701081144781694
0.01164777548887156
0.04471509429082232
0.0522777078306428
-0.035321470395265425
-0.05398715966345623
0.018445806974574618
-0.010042527776033015
-0.0038075382534296653
0.06193377453720959
0.03054114008822037
0.044091649596683054
0.01166090901859298
0.009490290623812552
-0.022286591723242155
-0.00573822777190533
0.008169458647692071
0.008983665589175932
-0.07610485210989967
-0.09405004968479438
0.011373998524444548
