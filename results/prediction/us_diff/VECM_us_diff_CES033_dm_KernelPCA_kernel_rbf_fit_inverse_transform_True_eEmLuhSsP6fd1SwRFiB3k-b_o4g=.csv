# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; CES033
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=4, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.012588483552944046
0.01398560758906153
0.0056291276290588435
0.01091324594862371
-0.036916266325700914
0.0039690689396754434
0.004196108470649051
-0.017672635262049166
-0.005426872302195385
0.003262432643022858
0.00448947308834348
-0.006693587175803581
-0.013736757153935318
-0.0011463967016581548
0.012103612674701104
0.01898074713964292
0.012712467836444056
0.01812689938367791
-0.0014832017820681856
0.013022604811248789
-0.0023396292990314057
0.015681673400062907
0.010874394284455215
-0.010506613125286168
-0.008700433119621202
0.005786343874521221
0.0113876734604998
-0.0076346567401815914
-0.01710622190381796
-0.015134111063893303
-0.01807335451256934
-0.003079857206252174
0.01719730250666449
-0.0039853015008121965
-0.005827447224393769
0.00023541862046828942
-0.008818635060215447
0.007064215689234236
0.0051250393218927005
-0.0056236686873294165
0.004952471515452469
0.007612307859984469
0.010565176033318314
0.0066245075553509275
-0.000945432879964045
0.006234048387966961
-0.006665578290148708
-0.020788180330537805
-0.0022493994898614604
-0.019378403892430826
-0.010473901532392735
-0.0034355610621047347
-0.005679947685350398
-0.0011328051288390368
-0.003041202401915812
-0.007381908549873078
-0.005731248948204755
0.004019407774274333
-0.009798164276508895
-0.017158257364916853
-0.020530307235684112
-0.013896443625576717
-0.004709172120463087
-0.020212177718226025
-0.008229018532748326
-0.003783938739481972
-0.01341133569884009
-0.022975703260838167
-0.020560607772340055
-0.02345223396249278
-0.029357631898274377
-0.03732258895420861
-0.04438337922126635
-0.04157186723071388
-0.015778132655061994
-0.02649084131302058
-0.03186303978771379
-0.030386588096849788
-0.03435743889618734
-0.03080391063322705
-0.01400815310697937
-0.005257930766407768
-0.01588311373219109
-0.011450752730121305
-0.026557784352782526
-0.0193455270621996
-0.018696278434285733
-0.014466232212776061
-0.010291658114087502
-0.007656680260309866
-0.005844667421483905
-0.015319106839980622
-0.020020999445591364
-0.018290749263423087
0.00010023364502251145
-0.012774518806040669
-0.014646355730374032
-0.01425029176766623
-0.018247007079484165
-0.016189646057335173
