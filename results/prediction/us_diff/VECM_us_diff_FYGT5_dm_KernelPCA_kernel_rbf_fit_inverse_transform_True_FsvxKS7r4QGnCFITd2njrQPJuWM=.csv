# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FYGT5
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=2, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.0406120317191764
-0.03634312369476856
0.03690490880601918
0.06476729140797036
0.0009173573037483673
-0.0031406388641879454
0.023665734258959212
-0.06501938559385524
-0.04797684402973973
-0.04865405728673352
-0.060269352053592136
-0.049884253953054056
-0.06264576785425245
-0.05945523691014408
-0.02426852868940431
0.013973340040824175
0.03321502164797924
0.031022200262597374
0.022668262311396008
0.03155457871324542
-0.010114612449658392
0.018453931853406
0.016649055509709576
-0.011368682170377578
-0.022387618590507707
-0.015997920239960967
-0.005695206530235408
-0.010850147323445749
-0.013066929971163533
-0.019772780839403852
-0.03158525730211165
0.005808204550270586
-0.007031799831735973
-0.03063771395162606
0.003412157282376368
-0.039798717524191875
-0.046281842163270685
0.01343242588504252
-0.04117960400436383
-0.023391791868399632
-0.0072245906852665505
-0.01972062081779526
0.020383971108879136
0.045568399450471336
0.016933868873196412
0.07766294705731507
0.012766535193894497
-0.016011093899250962
-0.010767599925271886
-0.0465051851532962
-0.01947502846721974
0.017394152217382993
-0.00463339859917039
0.0233755733201043
0.01277010314975777
0.010902364688142142
-0.00595541800574346
-0.0017605141471919987
-0.01785171222162688
-0.019892688770421066
-0.04016391835191818
-0.02877378723321903
0.012023098771791377
-0.004522992135485357
0.020091719858946255
0.029864508397801547
0.0354435821053807
0.0059445718991092454
-0.010236482540261025
-0.027599683763481864
-0.04276074791140899
-0.01674027316932963
-0.03955709413661761
-0.029856803158077892
0.02471104846668912
-0.02770938527332278
-0.011244935370867532
-0.04183102356004219
-0.04260285597942876
-0.03726651741510023
0.025228050854291546
0.009816971341404856
0.008888686668425103
0.034273246592710846
-0.024546354111772885
0.003015369723184563
0.018301205798683613
-0.005011493741822316
0.021441830601601184
0.015777047519593416
0.021706020285492945
0.018948078823723766
-0.008877937416784176
7.418909463628373e-05
0.008047746265021544
0.00355799840658388
-0.011629040985592833
-0.020643817446720412
-0.06217910077457882
-0.005450727833223434
