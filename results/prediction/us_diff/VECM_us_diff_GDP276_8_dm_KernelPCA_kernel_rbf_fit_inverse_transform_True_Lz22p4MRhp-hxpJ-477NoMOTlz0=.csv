# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; GDP276_8
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=11, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.008858770187587548
0.002697287580955822
0.0053378355744538985
0.003235705317678664
0.005079490257807798
0.003464258167805547
0.010831744997258117
0.005626477022999905
0.0017685985778061407
0.010777833532017963
0.011327469895831065
0.007519424884980575
0.013510140519999552
0.010008794566875218
0.0035844286387842874
0.0027345565666465326
0.002942961976721197
-0.0005213679719875638
0.009200030433560364
0.007497523383439244
0.006719419724335458
0.005033284724641418
0.009110965839649566
0.0027889056368747504
0.002092851591185313
0.004445410301553698
0.003133223335944771
0.012760239176278021
0.0069199711443797725
0.007840884986262982
0.008274356951428679
0.0029239312648371802
0.00989426810216091
0.004808488753381596
0.015264511863136553
0.007859007146281108
0.00821172345449618
0.004673675336995384
0.003984605279636047
0.005710478634434654
0.005776845870532251
0.001952264505887385
0.001622795575784617
0.005184337076949389
0.006056722141451014
0.006082737381136553
0.0021741028289085098
0.004647513786367356
0.010161413874641666
0.005942080559558179
0.004451093195299029
0.007860846457363074
0.005930435373398665
0.0062033834887742206
0.00898786274023964
0.005912653868432306
0.005732580708547568
0.00831443527748268
0.002792703403614389
0.0021206163100544856
0.0018850288644361281
0.005859165411056448
0.003442912882414769
0.006405007693701808
0.004038499642295294
0.0054177492001562366
0.006309689568734567
0.004146337464057197
-0.00029483920553668086
0.002832308544111791
0.005867675402478148
0.0005071961582509223
0.002739122949187785
0.012528586003637899
0.007681098093925169
0.009482813792984844
0.004866750217943919
0.008115857968012128
0.0037213485576112247
0.005525846384293704
0.010443387241628436
0.009284946688149567
0.008439278246525074
0.009548303230944757
0.00469538655665323
0.008126049625349319
0.009270212191118994
0.009847599301013976
0.00671741366462519
0.011743316005604295
0.007779553515929761
0.012534930928460978
0.007599634884959384
0.008123594263211171
0.008231209273793666
0.012813467973179726
0.011308531144051474
0.011118673567432562
0.008424643461134374
0.01090679938640765
