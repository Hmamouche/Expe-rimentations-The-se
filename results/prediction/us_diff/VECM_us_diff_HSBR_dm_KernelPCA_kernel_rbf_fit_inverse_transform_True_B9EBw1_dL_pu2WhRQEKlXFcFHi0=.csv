# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; HSBR
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=10, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.08577911453791343
-0.04023935714526573
-0.022772192661793693
-0.12234454079983643
0.007792896175905352
0.04099898589910186
0.07553098401821973
0.07583416940058627
-0.015713217497668176
-0.03664080072827942
0.09503794551241553
0.07214614091409273
-0.01905728139122853
-0.0008236216905842253
-0.09298479802451895
-0.10055161831305366
-0.010930675139441359
-0.05907550658709393
-0.04966717978390506
-0.09208624879072311
0.05857761670639814
0.032944988107049114
-0.03505132505647514
-0.007167568424472024
0.03468739289273111
0.056755243393789505
0.01715055931532836
-0.055971455105631215
-0.09579594482466791
-0.12750861884625297
-0.00856077217236156
0.04702594737009989
0.0940339290992604
-0.010061146411325137
-0.03745485516448452
0.023515343999393923
0.002650137536709729
0.008871027782695115
-0.005069191616034273
-0.06209749881352899
-0.028288333842865648
0.06992643857011367
0.03409008506747971
-0.034258957753236854
0.04917661554007788
-0.07639900670885455
-0.006674763502751882
0.013567821558343298
0.05393735100483823
0.0526471433277761
0.021765789378025993
0.027126515680448524
0.03612549106435145
0.0007277805123790601
0.02498020095113782
-0.02892414107348225
0.02294666288704044
-0.008834863418511679
0.03517831748487406
0.011104816912662437
0.038294989008974115
0.03603568453621502
0.0310953491702149
-0.0223500346386181
0.019042148345011913
-0.04939984991142306
-0.026256916461499025
-0.004569722157042686
0.002533835058043707
0.03352100852047869
0.01850470192185396
0.03138138025764042
0.004318885996231746
0.04380590535656126
0.05968165357721934
0.014852360157569876
-0.037815725075433154
0.010565076198641712
-0.05714243731647054
-0.007104879407683113
0.0711669593853669
0.05012027606702438
0.01013631431875124
-0.02856713856728712
0.02697415188225559
-0.004126545704846364
-0.01407486181058991
0.03192004968418109
-0.019850150375797432
0.017119065874000118
0.021712431905427702
-0.04299667764109906
-0.02941646960146213
-0.045442914578352284
-0.05963134999139201
-0.024032236177334264
-0.06201046987930795
-0.14289363596482604
0.008658661827335053
-0.11922819636861776
