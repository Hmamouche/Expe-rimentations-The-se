# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; CCINRV
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=3, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.005558676672900533
0.006197381170199917
0.009286415002898233
0.009852172100478732
0.009722267999171223
0.007821857517806204
0.009339719060228902
0.009743503672436985
0.008964948636001193
0.009829313052496068
0.008146042885551492
0.00746712730881734
0.007024089596422287
0.006375577673159875
0.0020295307252733356
0.0028812593216587714
0.0031231938300877284
0.0027246648120785983
0.004201914992441856
0.004671339966561593
0.0046134051631802064
0.002753793490973128
0.007186599467651638
0.004931029417935642
0.00437334052909508
0.004336683558745927
0.0019242697893263557
-0.0009624067634602543
-0.0021164016412970835
-0.0040326558873617375
-0.005123487211838821
-0.0037857042837408823
-0.004561164472831789
-0.005737167456173831
-0.004474032584270042
-0.00452163588888183
-0.0016635631785736275
-0.0015626943791848486
9.158330154316409e-05
0.0013132901314628753
0.0027524960268433146
0.006426428344790823
0.010164802417799844
0.012310229743408455
0.010784156613460657
0.014492817022701596
0.013147396630647342
0.011743804753289021
0.009840392467535515
0.00987166770912962
0.009832522799650991
0.009931886178139322
0.01095803182747533
0.007710746669375585
0.005304414527568699
0.00753391635756715
0.005080056123551166
0.007278158800660335
0.0048196317523241956
0.007263942280918309
0.00976001279101668
0.009780614937486455
0.012747995262878235
0.012614066404699252
0.013114245865267105
0.01336715004731058
0.012278893861625506
0.0133563874684703
0.018672527046757453
0.01948067494584708
0.0191312708086012
0.016373589693654517
0.013828990856041458
0.02130658039616875
0.018556001737376007
0.018692439426029755
0.015820697770438232
0.010821800548798017
0.012103628457984312
0.013059195479683866
0.013388127054485473
0.014733720613267772
0.015121146170126592
0.014275557819524607
0.011206196986863202
0.015115608045224466
0.013282759361043983
0.014489382037081859
0.012891333034406509
0.010172649854995044
0.00892433991468331
0.008066571637981523
0.008633746102947433
0.0071451523362398356
0.008443458354914715
0.010176202252703883
0.013123484913533963
0.009382284321392062
0.007137414537368708
0.010000879297100595
