# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FYGM6
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=11, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.21024546336199373
0.011308052806857746
0.06907256273696226
0.01131273084241849
-0.009451512431058323
-0.1300337424379423
-0.007281793460258534
-0.045337594740958206
0.0042159128480033226
0.04453358992329443
-0.07646990631094572
-0.08231398001127752
-0.052324898455273154
0.00458982779733088
-0.019050704166054228
0.05417187556303427
0.0032335292005388275
-0.11696846509447872
0.05031514606508899
0.053960115745913036
0.04181785513193286
0.09877405153438576
0.0432940046784717
-0.04407747941793193
-0.04146487175226514
0.04521432364501176
0.01514437996595229
-0.012702204359995188
-0.06111281750554597
0.06739585702944226
-0.14847041488421103
-0.04709557813950112
0.026257177137387004
0.022440176732587178
0.0167853044765851
-0.01898817229890165
-0.12490619328123966
-0.05618344713308706
0.024392687302209153
-0.050000184930856915
0.1102820813836013
0.006319682464005168
0.0003722794135755485
-0.015126837672797286
0.011408999502894457
0.06651269714520386
-0.0002958948358736217
0.03803917953790687
0.04305960999356763
0.0116060498949834
-0.04726009079864177
-0.004342724324220074
-0.008077703491989007
-0.016727785158373462
0.01294844330806633
0.02658062784004575
0.011633987441746912
0.004262021828176189
-0.021250911006571248
-0.02772699534398114
-0.01194574727122012
-0.0693886453404595
0.027783595228712484
-0.0476755341725342
0.016479672479970793
0.08508878051427325
0.017910445493190576
-0.019416348238205042
-0.028728282616904483
0.035191758470932546
-0.0239002122692087
0.013151619541246202
-0.06779576256037802
-0.1117659653496371
0.10404861247758597
-0.04097571956581479
-0.04510731042097699
-0.04250332052674873
-0.03999718916257981
0.05001866658128728
0.023551649567887117
0.05338192282543193
-0.0094006115992184
-0.01933146913675777
0.018407333801660937
0.007540039538129831
0.012868519949997041
-0.010730143237177536
0.03448869704997834
0.03255521552388054
0.03294775018741619
0.04058802853046806
0.030066578237632362
-0.02487695016723466
0.045848438817130764
0.03149221849628947
-0.008543189335958917
-0.06849132869195639
-0.07852752388219873
0.0035356413033694155
