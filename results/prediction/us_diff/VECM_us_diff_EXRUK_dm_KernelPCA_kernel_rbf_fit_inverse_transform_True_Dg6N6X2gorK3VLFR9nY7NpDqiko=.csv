# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; EXRUK
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=3, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.038722274724635006
0.0124404686433114
0.024857622221923507
-0.029220667899434623
-0.03623657558545674
-0.009180890020230724
-0.05660354548370945
-0.007022749923697162
-0.0035478457922048234
0.03670126727935373
0.035955947112615995
0.046183769228561614
-0.007346737288907879
-0.014121226228513114
0.05345774021762881
0.04436669918111644
-0.006099706559219843
0.06079193027529179
0.055138897457594094
0.017324293607981968
-0.008709167521417236
0.012316671750388237
-0.008602033837716606
-0.048416160156068366
-0.010471819952683294
-0.014981866694694434
-0.009861420662450614
-0.010722808510380526
0.07578980603146616
0.05873296589815804
0.0383350785520611
-0.03675587138378274
-0.0059674141882746085
-0.003815926628917378
-0.04707727724941778
0.014076439134507448
0.05284732352856687
-0.09616932919273023
-0.024856187362502693
-0.02915746196726672
-0.04476965101388211
-0.009051364581977459
-0.010819216104019523
-0.004693848449225317
0.015545605416015717
0.012318164534903508
0.01072409318276205
-0.005716117308466918
-0.007830530803687032
-0.0010173306562683837
-0.02207485514334324
0.005603610599520279
0.031404363980101255
0.011074952693891433
0.005962140994923475
0.006538626440233387
0.0012421573084737082
0.008256014678358441
-0.012341915403080458
-0.00910598743388408
0.011261694281290747
0.005344154654472442
-0.0045101599543471965
-0.006174384205099357
0.004894600777306766
0.009974205069524865
-0.008190622699672235
-0.03622375292414851
-0.025369027544927977
-0.025840936663389068
-0.011064771158053276
-0.017769210064089925
0.015506417073779071
-0.00339145265585444
-0.010127813172181373
0.02568541587330883
0.013240897727054682
0.005621331980557649
0.0228540896349315
0.030272553198756946
0.0033707121354246677
0.04228163782082866
0.01946440056811233
-0.0016293989760385735
0.022579780785691683
0.025271947095041866
0.004749359191851606
-0.008874289168173143
-0.007968080179018238
-0.0019679308717500416
0.0005384938858501503
-0.009797877312455523
0.027986232719473533
0.013729974472886233
0.0428141746586982
0.013635796561433045
0.024375162113967598
0.013923755815535989
-0.001783607042024508
0.00387326031481986
