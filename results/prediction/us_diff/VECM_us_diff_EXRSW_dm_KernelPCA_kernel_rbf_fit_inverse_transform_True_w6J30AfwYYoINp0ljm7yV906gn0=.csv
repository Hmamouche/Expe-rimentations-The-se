# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; EXRSW
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=14, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.01739835244427429
-0.026661526150400683
0.10660079594119382
-0.018929169716178414
0.054940815088151164
-0.005585963403333453
0.029848710815843636
-0.042222090095321255
0.029072878007727843
-0.0363172114063485
-0.05721452148136197
-0.007008199444884039
-0.09573017239419773
0.013713552725260654
-0.039223234463485426
0.017538157465414844
-0.024917159506452766
-0.0433263876336972
0.026214504603210415
0.05400786630293626
0.02193699895624483
0.025081750146269163
0.020620167819004602
0.00019505233915673766
0.030661316918591915
-0.012310479948574343
-0.047735531980295134
0.016707519688874746
-0.0500191451193692
-0.012274294797539312
-0.027388640395036197
-0.04602958453222869
-0.011313323676311927
-0.006687101579325099
0.013248643241457855
0.015360134977052848
-0.001919022292980497
0.010934791495599616
-0.03371433888426257
-0.004236478967491171
-0.004308299357373735
-0.06791227826035258
0.03324804820006761
0.004911573730204847
-0.023547024456543122
0.025579910654574798
-0.002214975798464096
-0.032910333371533
0.0032786190477433943
-0.04076121052047786
0.01704711669478641
-0.04205662989615311
0.027811736436026004
0.061663727460368314
0.014275580358753589
-0.016085290978234676
0.015545491718885907
0.01527690899883842
0.012659511116275331
0.0066205414636718194
-0.03576568493589098
-0.034537421510068875
0.04609910460178274
-0.022049221931918946
-0.004955792981717497
0.039404558305879744
0.001487723050555577
0.020844496156365397
-0.007588893749450232
0.00451548295382971
-0.028658518445135676
0.010030861895566004
0.026321751551261114
0.002746020683493554
-0.008328828226557124
-0.023909662467106192
0.02340242321892226
-0.03814598118827177
-0.08061837914876178
0.03708163324409752
-0.011240913509429226
-0.0471291439324803
0.009950015843027711
-0.010904763072407113
-0.005000051710064491
0.004053498019404175
-0.006869049812518381
-0.011251695994433857
-0.007084250463736858
0.009353079860935148
0.023354765079403768
0.006063785517100927
-0.01405413978065057
0.06981300491542838
-0.033038706885528776
-0.04454543388254689
0.04482850398876138
-0.011480958094085015
-0.020276772926594317
-0.01760396813143429
