# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; LHUR
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=6, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.0022543697619026113
-0.03967926040761146
-0.11128881934096455
-0.020596379602874755
0.007785988991207314
-0.02770074131146502
-0.045877060487366325
0.0010093479939604147
0.034454801123676146
-0.0070854544937647345
-0.005236598883105657
-0.031876468482917766
0.022959974797144952
0.033428989801881
-0.013238601428325798
-0.01416470743847405
-0.08516668311281599
-0.018401972174929424
-0.04740051288825984
-0.027208410228072962
-0.022829896780123142
-0.05851202045442667
-0.014621454523031145
0.010899993032207528
0.008131084307989693
0.01871443766618688
0.006305901848819176
0.0043592819860336605
0.07593913657330055
0.10963313699173773
0.04573110966259971
0.003703227579651765
-0.04849488545242515
-0.011255371549349923
0.03176110651952624
0.046512158206225604
0.03302021722500898
-0.021471142421188964
-0.013195709116891167
0.029979863124213406
-0.022188219980867147
-0.030091926409539733
-0.056434665602410394
-0.05394294848006113
-0.019219664527718498
-0.04077598292072208
-0.024759274102815223
0.009925020216940458
-0.01209731762321563
-0.005568503642366159
0.008167815673491915
0.0009858833173046419
-0.037547154741109805
-0.028200597750564316
-0.02020444491081973
-0.03285372796452321
-0.03991831294004989
-0.025427138687678867
-0.019667442639619205
-0.00036088292532494804
0.03046721596234877
0.0003020671153992913
-0.02651843594001733
0.00024440764841174127
-0.014541994211451392
-0.04203625714986461
-0.0056653800481831565
-0.00698924833401705
0.01394269194175611
0.008914545585730672
0.05565575190647877
0.022156276565731818
0.04739351465534597
0.0671485177741228
-0.008279421979731624
0.03575125024262877
-0.002157345600721977
0.031897671153073875
0.042108529460687175
0.01417441249746772
-0.02471647119641534
-0.055861892852031435
-0.03925587919331245
0.011322578825888756
-0.017290903294912775
0.008529033274090523
-0.015291918573306832
-0.008938112272904348
-0.013074321136930921
-0.004399468138950197
-0.0507775324755354
0.009596370602941665
0.006772130151097431
-0.025710575060291112
-0.009376439371522976
-0.009867704876770928
0.014282856450214394
0.03508463780117244
0.04237478523523181
0.06221913342897742
