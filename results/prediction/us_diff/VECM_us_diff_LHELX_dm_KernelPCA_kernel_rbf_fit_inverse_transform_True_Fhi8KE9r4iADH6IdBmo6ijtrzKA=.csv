# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; LHELX
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=12, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.14638339124337352
0.027958236125060303
-0.010692688818247132
-0.0076196275902754365
-0.02237595388563054
0.04183092219560123
0.005324031621516418
0.028732444512229748
0.006915410910566914
0.02622872978111561
0.047793655319430556
0.0036551370270778874
-0.03718421222558726
-0.034624720402996904
0.03206994043846592
0.024377491121464714
0.03855358979088801
0.008584051993844143
0.020177820276562623
0.003238516910333885
0.02186635565952095
0.0860215753382527
-0.05756451287841859
-0.04368207040846947
0.009151155101049059
-0.011782315173222554
-0.019020309103638534
-0.048347149324419206
-0.05568027939106794
-0.07553604332943845
-0.08267398033665299
0.015598955469538071
-0.0008929522888062799
0.0058348256345461605
0.02695134938871916
-0.021142450924754953
-0.045172521878964776
0.01277884579430284
0.02632192368572394
-0.023060069360326587
0.0629711652531888
0.031378060153737515
0.037221178660348506
-0.018078960990731974
0.05708165577314309
0.02757454832980143
0.004512691826967289
-0.002873599308848857
0.019206487227627113
0.007581864490033483
-0.022819411925329203
-0.0015355418636355759
0.009393072889321185
-0.017682958167838683
0.059909550367383095
0.0041719276371196615
0.030164980865397158
0.009519061759466351
0.03109191588187757
0.011345180305061264
-0.017674546824844393
-0.013849807489816426
0.02783094965278186
-0.03193042574226882
0.018655119109633102
0.02505955773200021
-0.0075533755031017835
-0.011174253764782253
-0.030930057492002838
-0.01276536048307838
-0.06401789631031575
-0.05295413146030555
-0.10013075361148167
-0.05672824672368572
0.041748357902781794
-0.02880154820298201
-0.02372838197436789
0.01528114209336379
-0.06135269453669687
-0.024508346462093662
0.03135302867885833
0.03849565666719263
-0.012253910594555965
-0.048819681541880114
0.017117748854114233
-0.01215341837238151
-0.0044777611636665025
-0.0033157326786402846
0.009187516034948353
-0.009318760841044979
0.014863194391955504
-0.0242699510231595
-0.020233550289205097
0.011687575663102967
0.005542554160315851
0.005583447075919521
-0.013632438932364335
-0.05372591147497191
-0.012454638961479993
-0.015381177413187303
