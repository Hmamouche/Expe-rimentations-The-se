# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; PMI
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=15, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.16018361215101373
0.2738800284525863
0.049105656173356965
-0.33948151076631894
-0.25122083583181537
-0.025392678577573047
0.03670765006144978
0.155075918721172
0.03484449469870379
0.1932539881289984
0.2791150162105454
0.030231663930236102
-0.021264478776404805
-0.10773351110243977
0.03024671794100528
0.09877953633831796
-0.052535972716949327
-0.23895144432086432
-0.05393239938084594
0.04920614428738991
0.056741605634595134
0.13131035550929637
-0.0464334914050852
-0.09853331568878987
-0.0269822136097849
-0.13061519847590075
0.07625871343901845
-0.061005183446284354
-0.23533876253826364
0.11917691693922486
0.238588735455184
0.40280025501494404
-0.07094189178225778
-0.02078121655878612
-0.005373017033197577
0.048732218602003774
-0.017330805966654782
0.10005585851831707
-0.08559180591547932
-0.14017834134012105
0.09614941129708732
-0.018549253899765056
0.06881294903298449
-0.11679214704854776
0.03583185281898184
-0.0863325512627978
-0.10223201110690827
0.10749520274803559
0.046074732643117886
0.07672005127267875
-0.12934413595125627
0.006976741617973507
0.0762459695270678
0.012054789690391629
0.06827582329738821
0.024012037408533048
0.016181860942608258
-0.10135658404979869
0.10526419897992234
-0.016363792443595995
-0.052006907096594224
-0.1656571987181062
0.14971054182028126
-0.02417168627645018
0.12635761161520329
0.09265980979478981
-0.11643279640672187
-0.030656648619615715
-0.06402240066121692
0.02932436023890692
-0.021185894215631995
0.10281534869355133
0.03859595355586441
0.13413338267571062
0.14871076238151082
0.042919682646777806
-0.14818682532038613
-0.017590311410721537
-0.05076632376307355
0.09767379337874904
0.057990282722158906
-0.08008480263517115
0.026041987095325854
0.029283113067536906
-0.0917039444239519
-0.08040591561240243
-0.06138006125618619
0.06428370558148207
-0.05268415063866174
-0.044426900135508714
0.06055906550482759
-0.08239333368628267
-0.01635001813023934
0.07485307219812358
-0.003993706637507279
-0.11831369140878713
0.10877007278729323
-0.01949832532474554
-0.10998945434894031
0.050623949513270594
