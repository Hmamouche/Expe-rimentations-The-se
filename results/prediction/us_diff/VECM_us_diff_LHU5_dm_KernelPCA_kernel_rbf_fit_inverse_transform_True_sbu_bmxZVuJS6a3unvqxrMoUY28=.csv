# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; LHU5
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=1, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.04071915881679168
0.01732276941142231
0.048989984880274146
0.0012351903926826862
0.027090703913604363
-0.024103479967305552
0.05541661578638859
-0.019279239062370213
0.0022366425442203706
0.0002131461429272128
-0.00513403232523284
0.006613996928219786
-0.0004728262295091983
0.016089164096688348
-0.005141326143836134
-0.03260072625438658
0.0032074434148195814
-0.002020216004015129
-0.011049422292125056
0.010249572960900387
-0.013559232975852611
-0.017437748436223754
0.01428454305405269
-0.0020578810775122337
0.008060591061079798
0.009128082676637235
-0.01941905571444541
-0.0031886504264706734
0.0024875623558897816
0.013978096903705533
0.01902311142336143
-0.0003917490237816479
0.004183934758423227
-0.009440920021141861
-0.006570897949393928
0.033632532422444496
-0.014451103055823422
-0.006568887666680492
0.02255941857481043
-0.029569952280223646
-0.005590048872623831
0.003039553844870283
-0.060375552982019506
-0.011435678814765546
-0.062340843656307675
-0.04629629234523807
-0.0034196152040639576
-0.029330991573813265
0.016341998943655702
-0.009506521352584698
-0.01631298467542889
-0.021791400678903852
-0.01617209368774
-0.005231003902809957
-0.002688999503267445
0.017499966784663812
-0.007606519331490274
-0.005033299672755962
0.0076501320011957506
0.0027553564477728904
0.02568198527005598
-0.0038248335416526356
-0.02070607618030489
0.005708382599109007
-0.031111730835731072
0.026869654759717868
0.011814874483934656
0.010246497243981367
0.019817770300482696
-0.021752422646378052
0.010663404703846732
-0.030602140271851763
0.021315777520533395
0.028766395783936617
-0.009408005828519268
0.044460832241030535
-0.006448334039992998
-0.007530967252443926
0.01718312306460971
-0.02020365039180414
-0.03291114099081454
-0.007470501567824682
-0.033893616862325865
0.01333646475522397
0.0035424897124876117
0.03270590917041602
-0.005716536340843931
0.015967363463924092
-0.033918739684854884
0.013061434716060147
-0.03131311059587625
0.02565829650598789
-0.012060364717854898
0.0071153834294172625
-0.02519213993170733
-0.03006510056164444
-0.01246707266005806
-0.01449002891134083
0.01991858038848816
0.013880605145609421
