# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; CES151
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=9, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.06738159873562516
0.1302397553988162
0.0059953432557992865
-0.08818701133381579
-0.06002700121400173
-0.14079700171732612
0.21266099367831648
-0.05298618420359601
0.08514755380897898
0.019622887070449707
0.10573729856735126
0.004895038119145004
-0.0276481258659386
0.013449548560970675
-0.06502449654473617
0.12612308083125012
0.0589483222844526
-0.029695123649312084
-0.0022740559134063476
-0.03678164316070279
0.005378424033099641
0.015609314570565264
-0.03273077511612141
-0.060020793594800226
-0.05853800282040915
0.005956652898822924
0.07949471594372808
0.010910994138607601
-0.01817207988211049
-0.19276053176253646
0.09366120064143979
-0.04912574307926118
0.04922106843326145
0.03683239846587411
0.006118997506012063
0.07577201771417172
-0.03545752975681532
0.17650047698680912
-0.07416962113548067
-0.0746868788631399
0.026643137011568875
0.13230632216504334
0.07235680562468881
0.08340458732793317
0.002852395530850897
-0.03971459535236889
0.05065086666455029
-0.03673850723887294
-0.0825455654849526
-0.06737025931479437
-0.010929052847579505
0.08529273170565166
-0.021804658733132837
0.09354653878500627
0.08840462671113557
0.0028209823039341293
0.04197310083398281
0.02073107558161231
-0.049685757104395205
0.02943749906746374
-0.11772770496769339
-0.024358184198883817
0.10975197946088364
0.014639827835008345
0.0391865256943172
-0.013877418863459993
-0.04653232791531518
-0.12186729985240953
-0.028618903591302307
-0.06193409205189778
-0.09754162045127442
-0.01156372263765048
-0.0726353563254325
-0.037003020731420706
0.1514562995529173
0.06458509510590378
-0.09436341395700998
-0.033811789624776975
-0.1438919191938156
0.05068501397773391
0.023643174463327844
0.002220025548053825
0.04531491476803827
0.07170359572051141
-0.09424117526770538
-0.10760342417146289
-0.010676297095172873
-0.07656795742256409
0.04034790813738765
-0.0034139757982815985
0.06363079931142232
-0.019120357460357854
0.005567283229892757
0.048091606316237955
0.09123626132061419
0.06597188625247771
-0.03543998471574485
-0.05693472568966329
-0.03824551761041919
0.02661178505677734
