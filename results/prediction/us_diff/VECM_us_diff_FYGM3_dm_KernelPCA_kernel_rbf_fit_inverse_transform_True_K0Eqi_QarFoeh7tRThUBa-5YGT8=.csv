# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FYGM3
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=6, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.12993679108262593
-0.10537098083059497
0.1249050923147225
-0.0250471231170753
-0.018960898072463383
-0.07201263472447805
0.081245417062185
-0.04914627000023251
-0.007560491885761515
-0.0646962395510978
-0.014697350003556289
0.0014620479884425268
-0.07154126983642523
-0.017705923768517984
-0.02718560442586049
-0.04382654869287704
0.020062566026505222
-0.017789872477174007
0.06528791603702196
-0.020089901560951584
-0.0005781650804008599
0.028756062502402115
0.045832380607261775
-0.011724833472073738
0.006127584499574203
0.03481544187998427
0.024873190679562767
-0.009549661248652246
-0.08337019058085432
-0.0034614233911872652
-0.05618931789271549
-0.06649180944986204
0.01604684999886279
0.002286469508757303
0.024644407089903515
-0.0313650624908485
-0.06930855546492101
-0.039505119885728676
-0.019054394879402876
-0.02675545269316923
0.03424150746929133
0.032703875348464986
-0.002177532438187639
0.01461054968105618
0.02790221220269985
0.07553425648690432
0.00515231809882518
0.006927767170178
0.022299102544653605
0.02328288344841272
-0.012785931894831641
-0.011920987266068914
-0.00549846208375674
0.0058322584248866535
-0.014372931914775046
0.02549827264902913
0.02168225223270001
0.0058503638591247435
-0.00252527954973908
-0.03862099024921488
-0.015750753743862932
-0.03952391197898249
0.03168788494352536
-0.0636898533728479
-0.006208919228471862
0.03724052062115239
0.025590259467199818
-0.008304936316412648
-0.00947584312812613
-0.004480014972085736
-0.011072956269984191
-0.02285372774164025
-0.044988740360149146
-0.1177771472283398
0.02424901995245541
-0.06011206400930105
-0.00984873802184301
-0.026251795994088726
-0.023148537833015514
0.0008114701957241833
0.0026337323592384816
0.05765969094575258
0.042119066399658046
0.00801915703788745
-0.010014527962414296
0.014006822450952592
0.014318690828862028
0.001299137482612313
0.04642037046524329
0.034490792869392564
0.05777103235436683
0.017405130113630804
0.011785982124220773
-0.014166016149940355
0.035221545150573186
0.025089425596523496
0.005116640568908476
-0.04403819650975437
-0.053744552034539476
-0.029438566480489484
