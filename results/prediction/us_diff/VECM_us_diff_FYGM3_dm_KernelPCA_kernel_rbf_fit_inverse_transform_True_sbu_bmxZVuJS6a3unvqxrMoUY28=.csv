# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FYGM3
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=1, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.024059237549002715
-0.05042796072768636
0.05773346406873388
-0.01715193417979602
-0.01675991457474355
-0.049151518553058195
0.034395381711467665
-0.045280579988323216
-0.0048662693079530375
-0.07355931318688008
-0.04209396658527574
-0.033487769457887846
-0.03424665050983139
-0.031753379099298734
-0.012371246585136463
-0.026144382724754743
0.020331543489821777
-0.01333908399002045
0.004101706769834649
0.015832061052571803
-0.004639362892026388
0.030807633035166003
0.01857171204627498
0.005917937643062868
0.027322584150774768
0.018576929015725902
0.025822053070276886
-0.011363920809267514
-0.040543767454235616
-0.05945941059236107
-0.06133184956254331
0.0066908922839249655
-0.008534403639434914
-0.03883843549104064
-0.02250149891919108
-0.05924467423404603
-0.04795189305664095
-0.026350286252084586
-0.05573890410425981
-0.01321713444739903
-0.0161350708013758
0.013741439848733617
0.029666687984734236
0.05415828045304379
0.0067448489891142015
0.06167554971915284
-0.0042392830576183
0.01120001456885701
0.0172490006594058
0.000596612722602162
0.01642737276441553
0.025574431414613793
-0.011277462266275046
0.02411593851391006
-0.0069953123533899614
0.006810680870317359
0.01817377612744661
0.0015573613371610276
-0.008678021078182739
-0.017350634967080517
-0.024175134953895328
-0.019834233931071657
0.02414501365833216
-0.013524952457920562
0.02438388431614992
0.0037283810557079446
0.01720611599447624
-0.004648802040095056
0.0009619690688548142
-0.02072139720178049
-0.03165282343256663
-0.005343658009191668
-0.041987882196083
-0.09031600989474667
-0.003940876061351191
-0.08253879819384818
-0.014640253165552817
-0.07220790936565236
-0.03607162673240817
-0.008413679162789553
0.016915363377227405
0.03395241149261296
0.023543738374138642
0.013413584424283202
-0.01823977379651824
-0.006774468719508011
0.014410256005441515
0.010442530451240554
0.05685639708742481
0.040497934235205674
0.05892148556678167
0.0184828036736596
0.019435049833776002
0.001561383058975619
0.02168344231126708
0.01491540912603212
-0.009988486635717952
-0.03138176240057237
-0.0643166391484615
-0.045325866141935164
