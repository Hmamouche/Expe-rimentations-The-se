# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; PMP
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=1, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.03731790924796615
-0.11489199390548957
0.10924089884975162
-0.03116886708586053
-0.1143149426066141
-0.0036062199290547124
0.1348977161554501
0.05421680077845845
0.05926493321021154
-0.051520235662282254
0.009505830597956226
-0.013056249691130964
-0.01663624874794757
-0.014127138714262012
0.049112005029091514
-0.01956430947314532
-0.010465973139392785
-0.03690483851014328
-0.0057800219247694275
0.053101936977141995
-0.04729047215995549
0.05696846850611821
-0.024070140377236177
-0.023596122975863665
0.026219575373290213
0.009350519664637642
0.05712131587284505
-0.041555672112190564
0.01166919542364999
-0.04705558127808837
-0.01114382403574157
0.019215613019115327
-0.023996922655319212
0.011059967540763656
0.009461145254546423
-0.0886361793541777
-0.028724945372975888
0.026035138734327733
-0.03835956078865022
0.06794609813039462
0.02220984790810771
-0.009312168022241177
-0.02147011662969715
0.07560431812652736
-0.06406625778638816
0.03193738686525477
-0.07953163736453507
0.018356645216881493
0.04260395440263369
0.04156763858822814
0.07811530415502521
0.008959173622055089
-0.038165385274428025
0.0017640886790472998
-0.050094724617697345
0.024397120062050873
-0.036001769087850785
-0.007603958722870754
0.004314549124207213
0.018032783611116036
-0.028592673175074496
0.03318352675303814
0.02672288812155673
-0.019617271896313248
0.036157919455526544
-0.003245005195546727
-0.03660788187041285
-0.05918228149427612
0.015489132741298085
0.009282767639802226
0.04593612228577009
-0.009170132041036127
-0.05034817769539674
0.027985639846145726
0.10490058086654694
-0.10407633796179819
-0.003881315405574952
-0.04621270312759863
-0.03334480770175538
0.03542325043387077
0.01907152164981024
0.00486587775382275
-0.013685237874786933
-0.01860065306234233
-0.051502083627128464
-0.0014881158909693763
0.022134030225054457
0.001014727192558948
0.07199033169620181
-0.016263409891145122
0.04689948064045296
-0.0383689912567129
-0.02731313217992539
-0.010143883696245207
0.030490337246504143
0.021917603268345635
-0.0338889872616983
0.03150451459479331
-0.0981302403748314
0.054799738533288456
