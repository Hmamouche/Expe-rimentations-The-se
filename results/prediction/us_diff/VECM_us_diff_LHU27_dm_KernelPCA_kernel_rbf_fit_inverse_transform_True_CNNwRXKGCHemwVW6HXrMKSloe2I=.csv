# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; LHU27
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=8, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.00841906758431291
-0.1695374018319829
-0.132460542658836
-0.00023596551721257603
0.03412568350769953
-0.037216464274054464
-0.01717575882191029
-0.014876666246569132
-0.00571348477303215
-0.019894872887087522
-0.03866150255378227
0.013661119367713262
0.016261928281233552
-0.020073223086871136
-0.02780985228195662
-0.026950877826830838
-0.05212879769789921
-0.023959155645292822
-0.041368983766548406
-0.0056032073862101114
-0.03163891617176714
-0.029640358135064913
-0.029414446283843036
0.008146536733679564
0.009138094903255997
0.013257613122845912
0.00595746653697788
0.0034228269968736274
0.047492041177527956
0.07007638506584568
0.07395678479996175
0.03215517288887727
-0.013164874208363287
0.03353123863092405
0.11209899976157009
0.07752749874382324
0.04643851250187378
0.002505516619428095
-0.07380886862515386
-0.03476193742822769
0.008347103636129205
0.00963652365435544
-0.0412712336845641
-0.04228933217766233
-0.03556818938923582
-0.026128977433910166
-0.07771677359299681
0.02308035880102746
-0.010006795228405172
0.013783985479935455
0.022639264983054203
0.029188565156734958
-0.05198749519782683
-0.04395676223163063
-0.04111792707996749
-0.004423712177103287
-0.0035702993376357135
-0.0418013866791939
-0.002031205148920223
-0.0240277006662144
0.019844025292873997
0.025339745233918895
-0.043364909163484996
-0.016820679144026834
-0.020185353576227937
-0.027599383548549995
-0.03032198198210491
-0.008842800878207022
0.034015022335283814
-0.006011168616852516
0.0605247604653759
0.03888060209490112
0.061015400991766444
0.10156603666670645
0.04026190089427461
0.041405496590927585
-0.009977458305083772
0.0413363196630019
0.03868041167933147
0.05367212863570746
0.026507874551823463
-0.00855235080070734
-0.014086726223697745
-0.02880060853737363
-0.05049424210241853
0.003069553709829461
0.002662588533843921
-0.03731386459229401
-0.03874468967546631
-0.014452944353147812
-0.04265418653747251
-0.014095753142655144
0.012275547144489904
-0.040717681698529615
-0.028701841188271
0.01148969754763247
0.05223383570378179
0.039388380176027776
0.022105184846969123
0.03952155938075964
