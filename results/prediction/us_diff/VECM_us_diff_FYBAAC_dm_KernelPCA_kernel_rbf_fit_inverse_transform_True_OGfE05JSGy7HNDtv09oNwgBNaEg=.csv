# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FYBAAC
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=7, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.07225968482350674
0.06787404534045562
0.053277912386632126
0.03842587750861122
-0.027215195456948355
-0.08103894082775029
-0.045139776372353696
-0.0835127318070099
-0.00020382848495108935
-0.03355683256858185
-0.06774653751717115
-0.022475803259864743
-0.05135148777817928
-0.014003734988335364
-0.05306666043008584
0.027671386029056336
0.03963106967754565
0.04297835472646542
0.023296324733311627
-0.012514364620595124
0.012007981656296103
0.012495742444502423
0.014747511215129095
-0.04148167104026104
-0.033780042749384814
-0.01532308784852345
-0.01320611731894759
0.022010869721756643
-0.019576629968263433
-0.01395972495153763
-0.021868177935511518
-0.004786837410226039
-0.02371093477311575
0.01703299699755553
-0.010105490080078279
-0.015092656390542674
-0.039187004254152585
0.00493077298106555
-0.030129211932891392
-0.04371232888350359
-0.013929051110507525
-0.010921139206760395
0.020732538418052414
0.050160594005050095
0.034506143005393154
0.037627566586848966
-0.0015026605820938647
-0.034970005835676964
-0.030206774356943033
-0.023722986921029778
0.001025529851294421
0.004393557550963946
0.009186159654417363
0.002855164477535455
-0.005398133308450275
0.007321325938885741
0.016067067558409167
-0.012131423684589897
-0.011534211425933548
-0.026765673881211503
-0.007610925869309181
-0.03209302797621877
0.01963628795701191
0.01674363090937268
0.014129963877600203
0.043706096325046184
0.01842443275631169
0.009087876014923117
-0.026333872549494673
0.0042407590319215455
-0.03128105713145676
0.008735275357547315
-0.0007100488756083276
-0.04304885151178048
0.014752571118830774
-0.008025406321311965
-0.028129700501423084
-0.005087160026203907
-0.041207828304634764
-0.008178172341101478
0.023213714305786677
-0.007685213692995172
0.039045749222665835
0.009422023836137475
-0.045256882724456576
0.003599146658087485
-0.02721391506777825
-0.02040798857735748
-0.007050366366515366
0.017954550121024464
0.012125393447537986
0.02215844785077347
-0.001993773384204264
-0.0070860417923150724
0.011959265246968557
0.003312599514527087
0.037543373362738316
-0.027048528019962416
-0.027469115946342992
-0.002550278012795026
