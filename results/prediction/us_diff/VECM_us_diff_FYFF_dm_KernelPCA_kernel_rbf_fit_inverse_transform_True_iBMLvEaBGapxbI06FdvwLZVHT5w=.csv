# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FYFF
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=15, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.07237990005118633
0.016508961783939617
0.13853372340807374
-0.11525496008351561
-0.011146404649706737
-0.0613374115726739
0.03341750727409515
-0.05679518089070381
0.039830516491283025
0.010831081359153685
-0.0854643726478736
-0.0751252981505743
-0.05097116894859629
-0.02230174743597886
0.08566792370236032
0.06451317668139805
-0.06589304983247271
-0.09400970675882714
0.11939642869673651
0.05322937978723603
-0.015924120792498123
0.14391498245787748
-0.00523124349115317
-0.09199210842841028
-0.052890973413463924
0.04153187758591542
0.04168967917000009
-0.010450019387675005
-0.05833049532686928
-0.007229562610197245
-0.14239237646959915
-0.17087689870018102
-0.010765653867069661
-0.005415104352841524
-0.0030032526662535017
-0.02384015609654457
-0.030078863256349498
-0.07130877572981514
-0.060414976890187516
0.04928920972231035
0.07288685291639918
-0.010624965034085106
0.033340221152138055
-0.009180221485416199
-0.0035864430721766656
0.07257968800683745
0.049714116442459814
0.0006136780105982023
-0.027325602943483636
-0.01183433099832501
-0.0142164733492437
0.013014823341785785
0.005975571187002298
0.02565703026770899
-0.012530370061807659
-0.01529802346760504
0.023934686454527856
0.033039343039492367
-0.020224935260980015
0.0019808004015064787
-0.10188495004065544
-0.04325045247912902
0.08356120375572243
-0.04060634479591915
-0.002464337893739118
0.0417136068149254
-0.007148694422712081
-0.041495968193513776
-0.03173837782603119
-0.02905687692297393
-0.05991533902632894
0.03563614118516416
-0.08150789081893751
-0.1850015675114756
0.0781792827229128
-0.05297844658649434
0.054962114968136994
-0.0924325218983374
0.017436202965234904
0.04479954309547652
0.004986800155665737
0.03933355644740658
-0.012027121525207251
0.018609832437715915
0.0182894066229033
0.009425352133002949
0.019559183806114933
-0.061605850535564916
0.04707095134751876
0.0007992701443355794
0.07200361798067498
0.008951889602070167
0.03256605824844505
0.015851417701058683
-0.0034442634890706446
0.011872841560825265
-0.0373689339909281
-0.045390593850258974
-0.08279798280535652
0.041841187556749704
