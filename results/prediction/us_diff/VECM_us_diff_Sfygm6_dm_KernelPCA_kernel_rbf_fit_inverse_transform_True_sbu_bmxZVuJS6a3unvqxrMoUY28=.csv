# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; Sfygm6
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=1, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.1088914313957944
0.06719060576047417
0.04384618951712367
0.07004101895641589
0.006336357955909478
0.02324113996724313
0.010778483085114327
-0.05234919652486961
-0.024765498681000454
-0.006509216273258382
0.001403158852440706
0.00206743985369173
0.007714240695268548
-0.008660561146320191
-0.016590838452685937
0.0030883837795295872
-0.03905875632623356
0.06119299192180317
0.029616513299431754
0.03314789542184886
0.04015929369211782
-0.015643679942812815
0.021071616886778964
0.01038817791585371
-0.012509451482879972
-0.04284319711619998
-0.06230330412201572
-0.04311423252219425
-0.0031221127484568467
0.01762467027817991
-0.016454845911382684
-0.05622282125755602
-0.03829370385227609
0.002537761043813992
0.021146422281920936
0.024176213061100398
0.003730648348118053
0.006117888735383959
0.006100311977625934
0.005804688788763848
0.005744946001275598
0.0015859094909690431
-0.007126999456801151
-0.006487882914091415
0.023797626322060433
0.03804593987803679
0.05936861031015754
0.04334267123420521
0.01608648448755473
-0.039433567714927883
-0.06729588976920942
-0.025175530022487437
-0.014799684657311043
0.0021985254812274717
0.03504104600333089
0.009249438218053938
0.0006704091402824042
0.025195576791907672
0.023314404417216078
-0.002900792427800683
-0.010021142901677608
-0.009242928164578532
-0.006278192373271678
-0.01672985222020299
-0.009883653922520243
0.012271904987693004
0.024005797488087004
0.009781184630978618
0.01448832622863229
0.019114628770039187
-0.011944629179701658
-0.050022893777816574
-0.041535881961916085
-0.03464524732881825
-0.026694270356489663
-0.010148556083733492
0.010251025980014447
0.03240429818159776
0.009774810549798382
-0.017970437206736543
-0.006230496175779157
-0.005595781263779782
0.014916630969671444
0.022934660893878644
0.012254984136192446
0.011294600592382773
0.011812435285005926
-0.011312525082917562
-0.00781726691424204
0.00033824705948585967
0.004138913034272062
0.021129864310761484
0.010149767653789441
-0.010086702949688078
-0.00282498512102219
-0.02214149302212335
-0.02362645451196741
-0.022684450397147976
-0.0012566866833166918
-0.019784210810790473
