# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; GDP273A
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=11, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.004150462943506677
0.0050534180267110766
0.004775704983964701
0.008157830502299226
0.004930559615512013
0.004739563100646382
0.008432088610322085
0.005019350784780599
0.003658383987038554
0.00649650498035409
0.005507977187721695
-0.0007916876624881235
0.00463902899807958
0.0026619468106421603
0.005562180484037719
0.007518475358218185
0.009079301918397719
0.006774426444584798
0.0056303220875364626
0.006761636321651108
0.00743384812912325
0.007930429608625177
0.008465051296910176
0.009829096950677395
0.006321182644760688
0.007390830635903246
0.008499087144274584
0.00783324757976785
0.01062651263785266
0.011607394156632814
0.0070803273054707416
0.005927256957493679
0.006041037576698727
0.005864229930843129
0.003774677037932203
0.00669587220617695
0.00592377089575992
0.004215437485343205
0.007331115485743194
0.003851956507943994
0.004567813620502521
0.0035257185008376914
0.0037382504671543207
0.00504113037747018
0.00631904188336321
0.0044824716084099926
0.006596584269573409
0.0025174834369219995
0.004581067032757016
0.005035466244000049
0.0033920774284421666
0.006197807587987068
0.0052185324203119395
0.004929804306074143
0.004575362953874322
0.003886168818207
0.0023279801320084904
0.002123213179665069
0.0006151501805803804
0.0013619800718206606
0.0019572611032952175
0.0032079445808794434
0.003729940270513851
0.005460256842396263
0.004456517354349274
0.006940835951650748
0.006251383079389406
0.006208581251390087
0.004753080004669734
0.004676518731999191
0.005675872008714559
0.005545062377012225
0.0057698225133277145
0.0013720226020268672
0.00344161066104033
0.005533087633299736
0.002532592582971879
0.004485371465809125
0.005947093868581387
0.0038247684577813025
0.0064882133314101646
0.004594516553007904
0.007489735747628697
0.007657519120226302
0.006405590062674069
0.007943815264029982
0.0062816021814487855
0.00787881615361325
0.009076913239418337
0.00995779006025233
0.008477690714400648
0.008114531693536653
0.007159642181666363
0.004513677100044169
0.008026544447424538
0.003512909530467274
0.00930757099476517
0.011259894898980154
0.008495699438850467
0.01198310004762391
