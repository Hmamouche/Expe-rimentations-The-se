# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FYFF
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=5, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.06525182300839863
-0.0826326632296801
0.0383397754843247
-0.006991528864644686
-0.03381336234479207
-0.05972031274476126
0.045232316401550385
-0.051836856355865976
-0.049064712273142776
-0.08494358788127007
-0.0160509879905158
-0.02906059291637071
-0.03079810809609971
-0.015654931999477754
0.0032703840751258217
-0.02935076559775676
0.004496650864561379
-0.0182570357451032
0.053404793181648785
0.02640217517559018
0.004182810995011045
0.04186232744522654
-0.005722378846924569
-0.005379328299103703
0.010987597674690444
0.0011936320718474273
0.023261327455358156
-0.015760765581207765
-0.0942830557198874
-0.04064104105720302
-0.052531198679796755
-0.035592621439434445
0.01301020681103854
-0.023351303171206213
0.02449028545056634
-0.045985736764040164
-0.06476451837385863
0.04373034451775182
-0.05123888237794226
0.02418453688171122
0.04543106079249136
-0.004903286913830381
0.010944629871867898
0.0045418241899134616
0.0022524651396176485
0.06687744566788383
0.0028923769112501867
-0.0024875463640747705
0.0306021912387264
-0.017229340488114565
0.006240579365319803
0.025793943162355077
-0.02046178590981581
0.027888942344163076
-0.0017133492853498213
0.013273791968038278
0.01988097378931678
0.02006695940524913
0.009430742991615396
-0.015824530525325856
-0.056876587974471496
-0.02090624260696524
0.047050076269344446
-0.05134789441883551
0.02618483705345448
0.00991632244447354
0.019984428809021473
-0.008749537624521943
-0.0029784076580537597
-0.021577168341331766
-0.04515572016810974
-0.0159068396456331
-0.0890018716982574
-0.10528539606138441
0.039306022756747216
-0.08142781219081568
0.05004117238757536
-0.04556968655098411
-0.004140279337205344
-0.005549962444445784
0.006576368027465292
0.035011317371271944
0.03896660584942765
0.023084859885234572
0.0012340266785614397
-0.011811579188689735
0.010129784011083071
-0.011374324058880524
0.03196301774568538
0.03138545755567312
0.04631132125500339
0.012955009172129783
0.011450132501965934
0.01849937792163769
0.015516056439094763
0.01161397455227551
-0.01461300366598254
-0.043065979750211016
-0.040288672668852554
0.0038007500590434284
