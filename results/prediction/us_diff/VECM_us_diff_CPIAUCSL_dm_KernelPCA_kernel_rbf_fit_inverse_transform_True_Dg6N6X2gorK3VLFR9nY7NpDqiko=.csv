# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; CPIAUCSL
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=3, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.0014759984425611519
0.005358532803634864
0.007925537871774642
0.0069759856260367134
0.006754292418812264
0.003907849873171082
0.004136226433282073
0.004922880653572208
0.004632109467600247
0.005922641446703161
0.0021310572542319
-0.0014085588515864288
0.0027939174439909863
-0.0002254298762661003
0.008728146951731408
0.005222064729971973
0.0067518571213935775
0.006066969559214365
0.00529745422610087
0.008064063230053481
0.006779194371059845
0.007438561143627833
0.0077401061964375295
0.009225456663563471
0.005929952397077138
0.0076006553844865645
0.008315906681875675
0.007356030276520623
0.011697918050421663
0.00965509006695164
0.009255069330918714
0.00481165405733758
0.003873766509101621
0.006139563564852052
0.006004761507878366
0.0061371580736588
0.005278275154471976
0.0072627257017516544
0.0060516651288650474
0.005797962742748882
0.00412486887499905
0.005494036464487574
0.003290242103373332
0.006709501225276939
0.005578655797080685
0.005971064828033558
0.006722671652685894
0.004691405739914198
0.005331541553412849
0.004416497274287829
0.005553915690541685
0.007062334043850987
0.0052184080753969915
0.007209577979066667
0.005182226239385958
0.004331825373584313
0.0036787075929085884
0.003663065317522183
0.0032300357764971986
0.003582491520463211
0.002895267305057113
0.003775440674810997
0.00433584807288888
0.005452607839726737
0.005142054384993062
0.007453277755639395
0.007890106040828425
0.0069188131131072686
0.009231274269857303
0.006152474672797749
0.007919968175887396
0.005757580685496802
0.004869603401797799
0.0005075762158919803
0.0036257018480427105
0.003780009980290596
0.006100552218622986
0.006621927557216385
0.008230325108739478
0.0010408847016798316
0.0070188287498938184
0.0009010118926367762
0.008033736566311763
0.007137703421882208
0.0074904801294580735
0.008742028892418069
0.007442136364928473
0.007303645768922783
0.009615074134342079
0.011088423466672674
0.009653239364650344
0.009371072429815344
0.009055983433283784
0.0032442262963018453
0.006656130017550862
0.003554222121395168
0.008939559656747118
0.012240306353116546
0.009865017017082273
0.014654071285746785
