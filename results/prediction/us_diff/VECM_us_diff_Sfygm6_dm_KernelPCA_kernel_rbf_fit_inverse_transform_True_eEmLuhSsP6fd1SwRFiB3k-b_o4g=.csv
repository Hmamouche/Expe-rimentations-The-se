# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; Sfygm6
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=4, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.0063288017224167326
-0.06945350171328815
-0.07297213998809232
0.06609586835478523
-0.03491356256225475
-0.017087434983595946
-0.008735935429968773
0.003593594504629285
-0.01235424898042927
0.0012363205469216876
-0.0761320471052427
-0.047809637228117784
0.07502344331487136
0.067431316429593
-0.012561171735334019
0.09519968528353098
0.013012704216588112
0.008353155024432665
-0.036468663718646335
0.027442955281144668
0.0012676790619796652
-0.08161242981454378
-0.01644600712650289
-0.013873770004468801
-0.04377280078382153
-0.06852472427766003
0.006558029352916855
0.0328997294635369
0.13353748232041535
0.06102853782719831
-0.024252095682155334
-0.0585866649748814
-0.05327304437022775
0.025362412408686762
0.037909506316115976
0.03269894335193117
-0.04566677154500823
-0.04609380062881688
0.025076126129032643
0.04071035146290142
0.002150588090218038
-0.002720583758789332
-0.02201680759005315
0.07666473894792743
0.027601528681541986
0.015563971709066884
0.046474732061202784
-0.028005942347106
-0.052196203016685386
-0.09052778963213393
-0.03579909790168542
0.04872847713550317
0.03352080951321311
-0.06438119543313177
0.04620267771369737
-0.005923728425857747
-0.024339224152006136
-0.031720385655564606
0.0004441532990006672
-0.006269033996282293
0.003993402583401071
-0.014120950199368563
0.0366575149844349
0.05689820619893442
0.02315672016225029
-0.013286348699064348
0.003004473075871726
-0.07992161259717812
0.02254272434403949
-0.004653869661994136
0.09426907325121096
0.012507760562632983
-0.04461283285487635
0.024195046455952968
-0.02865083548496452
0.03331094393443686
-0.09595652340366886
0.02419726999069014
-0.007385225824217522
-0.016267411926146953
0.025921169348368966
-0.0010719012315508654
-0.0071577518718122434
0.04152066878084177
-0.054564906676364
-0.01178095238833591
-0.0041777502993038985
0.050885957079932494
0.008285153470710894
0.012869504769075155
0.022490622824617723
0.041855074778333584
0.0022549096546989594
-0.08432329657726881
-0.02641006504375845
0.01394093671097142
-0.008645926086085096
0.026405253710706365
0.08276538445171706
0.061955630454277544
