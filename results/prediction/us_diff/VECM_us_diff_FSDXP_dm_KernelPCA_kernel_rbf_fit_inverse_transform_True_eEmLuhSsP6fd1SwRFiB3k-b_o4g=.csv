# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FSDXP
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=4, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.042582612030971446
0.0822509131067621
0.04728579531397955
0.09096316305080847
-0.0035105332346963367
-0.03409412952568024
-0.05481861645131522
-0.052147250252734305
-0.04159831761198509
-0.057069040779882195
-0.06175362883038825
-0.04273173708296666
-0.0007846326104108634
-0.013325664828609137
-0.055664804825459185
-0.00039297388973362664
0.022573410384065802
0.05974284874070876
0.048652567459818524
0.00038413567684643466
0.03712324135174147
0.006939361684872341
0.007414462363810315
-0.02343615521316325
-0.051624780736105325
-0.04046131082785477
-0.016155371153099493
0.0028422957604647726
-0.0006217107919822516
-0.0023854925189936817
-0.07025033266267905
-0.025802479465196683
0.031091367329047413
-0.013707071755684825
-0.01736971486039105
0.01774829648638087
0.01270721826428978
-0.01251307944679516
-0.012286229045557833
-0.03243596683904739
-0.033498033988079906
-0.021918266049110714
0.05101025816204324
0.07308333103482448
0.03126160309500996
0.023042038857013294
0.016135515513352745
-0.050547779181287916
-0.04764488547926514
-0.04386028391319552
-0.03649288706775548
-0.010350805277399788
0.0025254621663850074
0.012105106915786298
-0.0037088522408476546
0.013869146120413366
0.004237905832649026
-0.006002423520805868
-0.01729786200568189
-0.025919857516434046
-0.018908299353225917
-0.044185215865048755
-0.019119211545077494
0.008131777967121518
0.015878681521827066
0.00458692143131513
0.009994502034735545
0.0013167885660252806
-0.016159064869394814
-0.04052802430247239
-0.05420114817014944
-0.0223637103167085
0.01138668450833181
-0.023623913272005435
0.009865283513950301
0.009274437147164985
0.03020340156598207
-0.0031680425958512948
0.018710821394370056
0.009833663294561422
0.0037147187741736823
0.03839441850161367
0.04004251453277195
0.027702151956596695
0.016063221536251927
-0.009586770318979768
-0.016729749404119414
-0.014067505925740976
-0.0073681019838410695
0.007562891941720153
0.005615774278654267
0.018561563396356902
0.015019306380491846
-0.017181334664087798
-0.009778971898448539
0.008151898501582478
0.0016602064781571259
0.005394389741121817
-0.00875698611129707
-0.02115822796522867
