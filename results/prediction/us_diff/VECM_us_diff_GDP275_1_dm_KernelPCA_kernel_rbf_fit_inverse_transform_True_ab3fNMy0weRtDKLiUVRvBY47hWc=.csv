# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; GDP275_1
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=9, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.001045292500057979
0.010361082791097355
0.006332434071763625
0.0034470183283562454
0.008797875009707079
0.002038656252160026
0.004351053826659604
0.0017632722434574822
0.004453865194188667
0.00391404260061187
0.003335833212919584
0.005794128276390369
0.009686367138869638
0.0030658188224420603
0.006757907884532157
0.004444637369284582
0.00774015976770967
0.006144993527933173
0.00027468996316722366
0.006178838271677904
0.007312988356318883
0.008281440365142537
0.009089293740223655
0.008932155485536664
0.007048574354266133
0.008855887026139415
0.011989354580002368
0.0045394037844363605
0.012833139116197257
0.003994254572387971
0.006110895058162317
0.004740197754623508
0.005460582964374122
0.003334136678593539
0.002947296899079494
0.0035309868022867404
0.003862961143771431
-0.0012185947424614915
0.004634997741945991
0.001247567014354735
0.0042486478296161635
0.004180282993748802
0.0006263758982981647
0.004290093663571706
0.005118641406391852
0.00564077201027581
0.004680032838548318
0.0024671840990097053
0.005234009002758701
0.004492959663660488
0.003428469377594142
0.007332734703161375
0.006562982359733105
0.007562501660721148
0.005228789525895231
0.004294321759761617
0.0032647255412595654
0.00472349300024035
0.0032561860358588284
0.00481471173775957
0.005345344146114984
0.006108639740914591
0.003794991414386134
0.004042245457806578
0.0031964540727908866
0.006693461156962263
0.001519567173264446
0.0061445412826295325
0.0050208876948483385
0.006844902342236296
0.009238827037245276
0.005526897302863913
0.0064924883793861035
0.004620444859081703
0.00887944114733936
0.0032727902236916575
0.004418373253602703
0.0029342879381770662
0.00401444418541451
0.001886624724152603
0.004452062489579682
0.008867400198104234
0.007229154502920891
0.008668168364644502
0.006086526320887282
0.00698348728369094
0.004650596450469332
0.007832023136491378
0.0035703501340308133
0.007927936454676009
0.004710136715640229
0.004491987741333008
0.006971845014524608
0.0059210074255211294
0.007518421811835941
0.011214977893879333
0.013799545102340051
0.012670978617779077
0.012077944016008224
0.013204560103041687
