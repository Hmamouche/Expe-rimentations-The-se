# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FYGM6
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=7, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.1510253068649762
0.004980366416878962
0.09331564170156915
0.001678808932501212
-0.03405406874961884
-0.08041986365654455
0.046769133497506686
-0.047637096120975955
-0.04506254196276291
-0.04354991610148823
-0.03679645262893973
-0.017500235821371092
-0.08307496437489492
-0.022671608193379458
-0.029425702494316124
0.013973339499292272
0.04482711487505342
-0.03550747334579421
0.028688113317306384
-0.010261701560685586
0.01865729301663497
0.043062658244881105
0.04661665154015155
-0.02672012218265052
-0.018770690681057328
0.02085639884137304
0.011523296585611811
0.02781557842991985
-0.08154924808622435
-0.028097291665958424
-0.06396268964833995
-0.06587618666775029
-0.007130717104799834
0.0117500940757371
0.022077848833182406
-0.02042219017002543
-0.0842936399877546
-0.03864541978240909
-0.0238055066786585
-0.07900650120085455
0.058097608688848806
0.037695135579908154
0.043241515532098186
0.01895451938361817
0.01670578289108139
0.0675883448411302
0.029357435704967523
0.0041798697546394335
0.01418366544440363
-0.0024550123311874936
-0.03289011918289628
0.007708087722320475
-0.015000610922944434
0.0017912259811768798
0.009558459872911106
0.033541427651456485
0.013076447757827955
0.005028990907443657
-0.0070056761289562335
-0.04964437990740706
-0.025015691931065686
-0.05608022370376692
0.018928413564182758
-0.03623787117481583
0.004813635664430297
0.047795372252456725
0.02414025403997436
-0.012404506322830973
-0.042152613267275166
0.006366080582405807
-0.005032541992459673
0.01356010920037463
-0.048784003226724944
-0.14460965267547052
0.051918449733746215
-0.04886013928659699
-0.02013563576268587
-0.05251293332671682
-0.02624979057865359
0.02437243681965188
0.02247259304350242
0.048593612420534574
0.04427715414815228
0.019626000534395477
-0.02537725001706654
-0.012124765141595443
0.02212729466939434
0.008177456485417999
0.03900423435038476
0.038271245184189076
0.06012641667147294
0.008324321839127204
0.011491648283494287
-0.007755821359960651
0.032222932934821556
0.027362190130324503
0.012635185910240631
-0.06841776004367506
-0.06388157248000972
-0.018980872698755192
