# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; LHELX
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=5, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.08233969962140178
0.015738527228708325
0.03478181353170047
0.0028745321385000647
-0.008276986072455068
0.03428317541886734
0.042363503491188344
0.002755901887717761
-0.0006935820478406844
-0.00045422620553616866
0.014527518996527471
0.005735975135511898
-0.009582256583955798
-0.01627285352299908
2.0250697233151725e-05
0.03190932882123253
0.04886875981830748
0.021225602899432792
0.017671378175283792
0.00378646868288904
0.010712186206379438
0.022611696823797947
0.002326282359005994
-0.022950815635609193
-0.0164291388940503
-0.010412923710040158
-0.012496046671420126
-0.03383839548347503
-0.06591507554033874
-0.09988812609825232
-0.04761187382531934
0.007822299207364406
0.022615560304070574
0.002984841483339022
0.0015875428977233006
-0.02962609274253204
-0.01636791900726589
0.018946229853268538
-0.012068392248293949
-0.005655163149876208
0.016381542096566056
0.03821706464165678
0.03647098301423474
0.016017236700772
0.012507859587678388
0.028231571916147676
-0.00022044298674257258
-0.012648951163272124
0.01626138851796195
0.005843095716434702
0.00510669553643742
0.002471776108321739
0.0056041491217492185
0.00683177627745305
0.021049951660761724
0.005932171515005328
0.027953480180735263
0.029372861966302247
0.022777536506533743
0.009575179163834005
-0.024218200624969686
0.001600356570780814
0.031816030455281016
-0.020571627716575835
0.0004286340181376379
0.0014852240478001218
0.011357917193210915
-0.011988438936489812
-0.03293411644455886
-0.013884419923879548
-0.06011740500675229
-0.04847443870030048
-0.056398281633219215
-0.07051704013334092
-0.006102401945193658
-0.02329113154236581
-0.016108634159990393
-0.019868930269955242
-0.038832316738056566
-0.006775535841827236
0.02551168012783483
0.018392378510295324
0.0012512625593403865
-0.018108146351424294
-0.01578046434713861
-0.009905802257119642
0.014294678639601238
-0.0017788840927490893
0.00319979320665811
-0.0072989620809657705
0.020491820276651286
-0.038353978409923266
-0.020423527987974435
0.009797350860332925
0.006859425340659855
0.009124547824271047
-0.01289733809208751
-0.040245131503752585
-0.0037266947116636425
-0.021746107395739785
