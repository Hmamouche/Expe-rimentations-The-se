# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FYFF
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=7, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.07230123138457814
0.013643364600694971
0.04456339452020914
-0.02491007796421435
-0.04156073466797704
-0.10385815733273035
0.02631069178350889
-0.04217781576443917
-0.030889677731246144
-0.06800150689677509
-0.001153186304103184
-0.02372701466200741
-0.051203120060419646
-0.02147693611891402
-0.008935826163620814
0.003500269637957503
0.02222832217407722
-0.025598969963269932
0.03095227561932936
0.006379676619370424
0.025564582874303254
0.06188465657535432
-0.025402495208677694
-0.025504555196277312
-0.024296970503740585
0.005907956451660998
0.04812937787985593
0.011199607930164768
-0.07585342616033341
-0.020710537366287387
-0.08485349120880703
-0.08851169687631891
-0.03207861029531885
0.0035429678419092083
0.057676302741326776
-0.020481089165178167
-0.07448968117940855
0.01070088922651593
-0.07078780412095755
-0.03612499991643213
0.07241125783141761
0.015532311057610321
0.04610410013240433
0.003001848240844481
0.01861145547173155
0.06358356777089101
0.014608614411020128
-0.006619883687191821
0.017562377414258146
-0.026483021037561026
-0.013049678026815763
0.01899234772501583
-0.03327809712471378
0.008716379137157263
0.010911694346017655
0.025764055499440067
0.022365122478536583
0.010825562681219597
0.005509839784104325
-0.019744198767153744
-0.04873527567587904
-0.0347994377461983
0.03043801878888803
-0.04801596421645324
0.03962067973571977
0.030802809299117096
0.025666838755643264
-0.01447103186363679
-0.02381663842736999
-0.01348031247947989
-0.01463542035523787
-0.01560338952947541
-0.08411499392053688
-0.1350136276542997
0.04959968041510462
-0.06067998682665524
0.04234301001636054
-0.04628583942329348
0.0030197286323445354
-0.00559114854187375
0.015578741273231111
0.020052042672922415
0.03324435519904815
0.043379100993061005
-0.00600120394514712
-0.0058044979285975
0.004641076060829759
-0.013212697802063389
0.036344005015053193
0.02976034682529544
0.0460431124169794
0.016447826120483688
7.924102592740678e-05
-0.004240602160903044
0.035541627941701334
0.002512984953778616
-0.0017081880312387115
-0.06245208830492877
-0.06070094683472642
0.019259867198643185
