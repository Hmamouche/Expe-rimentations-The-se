# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; LHU15
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=11, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.008897895788357365
-0.16214508519417523
-0.040719863028176276
0.05577392686559334
-0.030308312304080768
-0.018248332338382755
-0.05485778115656722
-0.019271060075095346
0.01596740535575719
-0.02033629601979922
-0.016316540259438225
0.027049134026785
0.0049389623289888464
-0.02499409197431813
0.003956678056588848
-0.013594206997556075
-0.06867385885651939
-0.029808503898634275
-0.027890068058294873
-0.01877489129748415
-0.018995619985363234
-0.06712280465482792
0.02649171994341204
-0.012217691883296352
0.03323664353845582
0.015258608327336106
-0.007332952512950217
0.03177222646722769
0.03986984300029725
0.07251552401015403
0.09059690404699122
0.016281916121822112
-0.026411702158949998
0.05754907447390694
0.07876378541992184
0.043490337698279494
0.02807940053093686
-0.009602024352985138
-0.06262733823641534
0.002353924133197304
-0.03125639794893918
0.014710919596023499
-0.020742309631116534
-0.04893309065521284
-0.03906679367106105
-0.04348244572578431
-0.04660443590523198
-0.011943501024024458
0.007447523370806385
0.06034461147336333
0.0012385650202548365
0.027122577790507335
-0.060493412194398105
-0.05391076542639908
-0.03190327613427128
0.010378176051982524
-0.03169150192740616
-0.03725836412896309
-0.006376677444557211
-0.04480431694962381
0.033761058596933756
-0.011911612972695442
-0.0011192012641030123
-0.03422658161255801
-0.03432705271979099
-0.033052301706651174
-0.002142744740744238
-0.00799737160923212
0.0213200767402847
0.0015188753277384243
0.06403200397183777
0.05003736546660412
0.10208215623957562
0.09573050186258972
0.035047689759267725
0.010535904556736177
-0.04055984442111351
0.028275896494145445
0.051596511071683844
0.04710804046041479
0.036332006999795385
-0.03030336168671289
-0.0037791165332646624
-0.028529934202663927
-0.04647919745662776
0.003994462355872249
0.006177366560608772
-0.036003672470922586
-0.04249297947702263
-0.011138984938489697
-0.01697513984640891
-0.03199727867914655
0.02363008646170064
-0.053652420525771655
-0.004629669952245294
0.0049542965033624875
0.05870428324624141
0.036660742372985534
0.023537872057995388
0.05882227077547737
