# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; LHU14
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=10, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.018676903145511736
-0.04845172326227934
-0.0520054967779209
0.04971861310394048
0.08149226125444702
-0.035537464201298774
-0.019123395961290382
-0.04133794142565765
0.005187642029134354
-0.04511926683131473
-0.0626374473572482
0.036974424813696664
0.04505953762384014
0.014989566575655007
-0.031219115619734054
-0.0069998201452083735
-0.08422266298505113
0.04365691050436753
-0.0032399066152497494
-0.10564723028978965
-0.018396365724870998
-0.06747323176456708
0.06017149473766918
0.019475191972751278
0.03117728284042654
-0.020029730612206392
0.02171221803082874
0.017111174508801185
0.02123569080089333
0.08809465768219588
0.052654790861229483
0.03006478854915388
-0.044229210695969745
-0.0038089904616689305
0.03118212726162005
0.009432241590533495
0.060015815179711954
-0.04581911006556929
-0.027410322752091997
0.04616842667109326
-0.05127237705074106
-0.05682516990104104
-0.04907512970815193
-0.018400870500535838
-0.014399582387688068
-0.013500848740107646
0.0130143766450459
0.009667653224004855
0.011447151444298876
0.03769321258236293
0.04655960193570707
-0.06072834235439567
-0.03754776182185378
-0.05717453135761943
-0.04071537143261066
0.014807360808718067
-0.026831636114161266
0.010871962971509441
-0.013704581901553839
-0.01624438792772564
0.04160953529007552
-0.012794871538823008
-0.049736951215623465
-0.0008950641799453649
-0.0478342590113392
-0.07374616348481602
0.05409496052671141
0.0410400657134262
0.023138836669827446
0.01863797515237453
0.06656167166237395
0.020795764093481474
0.05714950512710512
0.06875952882720898
-0.10449500432300816
0.028575547541165064
0.04700991939606659
0.010346249406056912
0.06320755819126253
0.014890134110322466
-0.04811077111704338
-0.061659403873771466
-0.02927416398912088
0.031975710322089
-0.004534713365913339
0.024402309309716583
0.007261511743028692
-0.012692022349523505
-0.010433652271977241
-0.01867314257264444
-0.04858301393912342
0.003214766745958657
0.02025870501441027
-0.02556539779558144
-0.03844079170372932
0.005122267669894986
0.03449272492182015
0.0569984773053364
0.07376991056849315
0.050063518502682425
