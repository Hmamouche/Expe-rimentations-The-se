# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FYGT1
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=15, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.22228633735694703
0.2124915445178402
0.2142895157530476
-0.02314433480231451
-0.09288230504111364
-0.06201232545646415
0.08812706492347464
-0.002090218575643836
0.0025523623209680463
0.05730029079680522
-0.09336672938724899
-0.1589518069888825
-0.02395898816414034
-0.06899152257052107
0.03970989152810636
0.08088064337864524
-0.09144706956969353
-0.08787036038035052
0.1328458657672289
0.010578572220335841
0.01449552237547444
0.06135053127962047
0.050368129036609546
-0.028069279102572835
-0.07731518104901067
0.059221947814361436
-0.02984908134983549
0.014953709864143357
-0.05336602772232002
0.03987941187864969
-0.15288105613092562
-0.07579613012844189
0.08560473097850382
-0.06267868858200257
-0.01981784223488349
-0.004176311709033806
-0.15061499551909704
-0.09717832671358209
0.020390219652663102
-0.05128255545905497
0.11158111681054692
0.019181934385342096
0.020791713691879675
-0.008914548980547016
0.027263925169415233
0.12463762125926864
0.021103670750687753
0.06143808434008247
-0.04178596197730432
-0.009663232170951386
-0.06878198214421354
0.033522378093165356
0.01599593580709593
0.034881424327332905
0.022336013475750715
-0.03855397294630591
-0.0013763406116746573
0.021947653228717386
-0.03640543766102182
-0.016812995207048514
-0.07709409819351218
-0.07018554633775542
0.07694661317785942
0.06314712237016692
-0.00685548804376325
0.07879743047174148
-0.06264212079770733
-0.036986126534301965
-0.0260700297214499
-0.01874362120930415
-0.06803798204039072
0.04549401943384076
-0.06093080760590269
-0.10107730791193932
0.025927834369135296
-0.01099643206196822
-0.05530687512182695
-0.0655159261097698
-0.04770192542932407
0.10580609739576459
0.0133092011949048
0.06975534795164758
-0.022495017086317855
-0.014293902411115906
-0.02173400165855954
0.02572547790324447
0.01806116683244522
-0.03981579088856146
0.05755225342604449
0.004321996727628015
0.06543612083940356
0.026044381912449326
0.038810871064758674
0.0045390762049559805
-0.024487609458914206
0.06213900548639284
0.009069382080227402
-0.03430894548835416
-0.12970663113966457
0.009500571139594065
