# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; HSNE
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=2, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.0817011976038984
-0.02229485043877592
-0.1553443762986734
-0.07816384006283511
-0.01056145168468681
0.022027233002128055
0.12158565086229817
0.07283255053433547
0.10317004154395354
0.01263383359359781
0.011814672959906417
0.1262806432179598
0.0757685143241934
0.0463618025052895
-0.00676491897442243
-0.000253421277130874
-0.027510048321055924
-0.04633551465932777
-0.09534400216359269
-0.012094777812255781
-0.000530321341557536
-0.02854277656216695
-0.0695308530142849
-0.058627467925113586
-0.012633605225940252
-0.018921346454594173
-0.0368290746726514
0.009721452751421986
-0.04446413527107072
-0.08847724759331199
0.010288800787918684
-0.002649777416434306
0.021683151904648125
-0.02614331128444057
-0.015920460621608283
0.050898347451009573
-0.007430136996388476
-0.030136016848958808
-0.024232296477408072
-0.04104616511277798
-0.029171720556971103
0.0304519037262967
0.0430814641422398
-0.020049255667331657
-0.02247900913496386
-0.04814932122723675
-0.00031542045517708034
-0.010744587423634065
0.00258647637941075
0.018793491079630128
-0.00380650961791133
0.015000210951953422
0.031432608792462347
0.009892410190330186
0.0038400856480914156
0.016874042893093497
-0.015274018718206878
-0.018479550746675063
-0.02675722803426385
0.03457980856242415
0.03148939083331001
0.0007876351736909235
0.015611550246928532
0.04066366400813305
0.00965434468121374
0.01103773815381032
-0.03984743663712033
-0.021714005839517818
-0.02992907966736788
-0.030849706468925125
0.07191694221583246
0.07650714998211958
0.05301611030652581
0.0416329226624251
-0.007366167333414758
0.02420937190989865
-0.0628969708365169
-0.004804061987511045
-0.030132856669629622
-0.005038993455769053
0.0027970552984953566
0.035738960714186314
0.023536425223746032
-0.027272095904706833
-0.04962816944885169
-0.025402357642020125
-0.0013760563179041948
0.030234708113936457
0.018338228555446284
0.0466849683961672
-0.001687748629746872
-0.007726640082554883
0.0009092224659606452
-0.03709830527973372
-0.01858659685530146
-0.021084032312764893
0.038967767514988566
-0.010649679029970455
-0.0282424181902385
-0.041021332174899144
