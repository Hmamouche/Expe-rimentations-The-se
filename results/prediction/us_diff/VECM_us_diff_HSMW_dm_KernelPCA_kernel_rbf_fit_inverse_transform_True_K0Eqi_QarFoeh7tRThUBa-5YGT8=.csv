# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; HSMW
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=6, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.15088464859087203
-0.03298922816041644
-0.05997319694703664
0.01326674515683847
-0.07449978596557047
0.07117873028837185
0.14568825914513892
-0.01642055163116115
-0.051531212732923236
-0.031007820257322693
0.014253128731396253
0.05005220028072254
-0.012337307160363405
-0.02351042908772104
-0.0375654251220488
0.05822584131034319
0.03537851987573825
-0.028353384240028045
0.046027718088142655
-0.1511121138529323
0.0987470380914241
0.020987701107037595
0.013971559029905878
0.02780644047627248
-0.01565618195408073
0.025795563691456434
0.005772248756472926
-0.06788658109605274
-0.1485127551585291
-0.008277127773303918
-0.03143709705443806
0.09871348930118429
0.16754214508742693
-0.00516590008512529
-0.01984661241920049
-0.001513249440844383
-0.009908331981294878
0.06472910891318982
-0.015230845073948535
-0.035914453839435714
0.010496347423418782
0.009743752349335277
0.05921056990129507
-0.06774595167228556
0.07440996133105911
-0.09300608057677034
0.027866017811174974
0.02834444111635073
0.03819342761705543
0.05823731799752264
-0.029081881406613296
0.005230613206085803
-0.00608292062570253
-0.02166911595600122
0.028286544009125697
0.014730145347295165
-0.004191643058275418
-0.019975957282770275
0.01996319962047674
0.002308910463855122
0.05526507338312198
-0.0034642608177711323
0.0634451561907371
-0.12348482528190206
-0.009367643993159164
-0.005501604238133347
-0.00430653090600723
0.03224633991064287
-0.002270383414401096
0.04416709067983092
-0.04684803475362556
0.031073821666847936
0.0365744483066567
-0.009770624658064282
0.053968187420147355
0.03591507220074752
-0.11102923233966966
0.020682395862076063
-0.021330357849785004
0.030401402533252773
-0.013278230780044302
0.10546119442359096
0.03260193182025638
-0.0764611332792423
-0.013834953680474788
-0.08208987407787657
0.0020590554552949066
0.05067365979444097
-0.034977473312287705
0.015168090233054567
-0.006238530501268574
-0.10815245560815158
-0.06800661564989556
-0.03046148155233165
-0.019624735039423063
-0.042281483217768506
-0.0875899202178057
-0.12854867299206602
0.04887271641629179
-0.00977669259967312
