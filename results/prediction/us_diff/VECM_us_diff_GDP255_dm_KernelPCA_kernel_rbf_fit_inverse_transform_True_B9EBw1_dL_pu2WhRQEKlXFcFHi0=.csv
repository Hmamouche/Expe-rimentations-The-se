# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; GDP255
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=10, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.005447256532794096
0.007841754573016365
0.0029413344219426617
0.0024996566447023976
0.0024291410503811464
0.005088341184693233
0.009672683256686347
0.008473897899824476
0.01142939592555422
0.008708449201775574
0.0038996059167591773
0.004382576023486316
0.0041561813162337615
0.003573492123987808
0.008090537744109118
0.006387070137640033
0.0049589749574581125
0.006325143203707849
0.006884028450681577
0.0065189868929542046
0.005362132507713509
0.007646661684688664
0.002820278552998674
0.0044955008486278
0.004420523486750909
0.007168179585932206
0.0034956768617000673
0.006636694021968567
0.006846849849653223
0.005050026538982869
0.0029674560110566186
0.0030344875693390496
0.0036195971792882258
0.004627277947752655
0.007609071402699922
0.00561660748723554
0.007662515975224092
0.006407420405802205
0.001593644637287023
0.004612381416701535
0.006121364173066915
0.006373440517802161
0.006731864970036697
0.004751423530841769
0.003771478626556269
0.00531676115949381
0.004107239709940697
0.005623954648798614
0.00708946330749971
0.006736739698969817
0.008613042249138508
0.005736834366581154
0.004824445519125999
0.0047110929852286895
0.005595598441500081
0.007926655750101803
0.006811496686648234
0.009619716569470442
0.010357418916192362
0.009221749186614428
0.010336923503402545
0.006860028989935618
0.008719311293750098
0.008983855383758786
0.00968935009934786
0.00806291963842771
0.01099896801440713
0.011464344009502322
0.009449993597206323
0.011061955378735572
0.010147557533165687
0.006034809863394979
0.0022551577565062793
0.00676342913671438
0.006722241773222
0.0049123707587993976
0.004278788771699829
0.005486101105036225
0.0004543394144642275
0.008104591056527567
0.0034192684935197676
0.007318301790039735
0.0095275201689669
0.005983886779678521
0.007134766871363952
0.008979498824073194
0.0069024277271195035
0.0077550598685317395
0.006799403755851592
0.006655231511645097
0.008213835699867012
0.006406399146270464
0.0073252334475519525
0.010490169184794318
0.007021203896604206
0.006199931391966337
0.009861317865977336
0.004517845761279787
0.006222023008454517
0.003495761031401542
