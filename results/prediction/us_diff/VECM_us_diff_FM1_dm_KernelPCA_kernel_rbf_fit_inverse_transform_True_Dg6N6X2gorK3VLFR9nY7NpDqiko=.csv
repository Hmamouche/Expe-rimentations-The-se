# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FM1
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=3, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.0071986161188812405
0.008414867139205955
0.0073933231620932965
0.0075647371253182326
0.006423224756684341
0.006703803404156472
0.006590398552935787
0.008552482593765105
0.009539787563288764
0.010377235372484982
0.013064818971614735
0.013259267241587627
0.014781113676288064
0.019101225836321255
0.01523570511937129
0.0130321114282228
0.003464198456908753
0.009699542598801757
0.00798017295742829
0.01155970514096474
0.007633902397666162
0.004415762441423175
-0.001526406267343407
-0.0015048240253717152
0.0040158241025500165
0.007309959939155095
0.006866415942824859
0.000562231722291542
0.004323259476990289
0.005049712299455019
0.008607141180745518
0.011645073702628106
0.011260265964704332
0.014489283946060003
0.023709365621628706
0.01673813236927147
0.016859365559492103
0.022358339798939135
0.018046484244433596
0.02168635547173626
0.015849488145936227
0.02629987397347664
0.012873457201472989
0.006934744287061488
0.006658600743978081
0.0007407787121771936
0.0013273747542065547
0.0006265284244372517
-0.000609504380700741
-0.005449812327324058
-0.006784962204483846
-0.005997329243611853
-0.008682182174465449
-0.012353695858657816
-0.008390683372090544
-0.009129525144495668
0.002223218401570646
-0.0015831953579344051
0.006646844444104322
0.000458714466816402
0.0008743004533042214
0.00687312414053178
0.005016449097481905
0.003933235031579018
-0.002816876897206868
0.005262010242047323
0.0035888840839281384
5.5620445062110524e-05
-0.0038389842586889224
-0.004803758199721685
0.005655328620547241
0.010688167834320531
0.028085449568793025
0.014211975922417854
0.006492741146641056
0.003436935606551017
0.004844704784980604
0.010920192886210985
0.014892521084450433
0.018762029178963253
0.01594589017264312
0.006812035163578169
0.010115507615464175
0.01583756384266656
0.012384334863955631
0.011261373325525566
0.005252168459405083
0.0003525830407137614
0.004526523973680911
0.004556050589990247
0.003979019433240937
-0.0009687239628552094
-0.005218848567522269
-0.0023610354163277987
0.0028680629332716034
0.0042142423427924515
-0.004280256009174069
-0.0009206001384762517
0.0023782649721076944
0.004266064422253352
