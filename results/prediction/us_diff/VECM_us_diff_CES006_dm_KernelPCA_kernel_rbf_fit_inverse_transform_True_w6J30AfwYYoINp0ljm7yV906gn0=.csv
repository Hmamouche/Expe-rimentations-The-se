# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; CES006
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=14, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.07412252649312956
-0.005529529647864094
0.10824108357773018
-0.09418469205369809
0.005754679440100281
0.009741020224807363
-0.04137178272449553
0.026367012162685506
-0.04543339616901792
-0.009108418939737913
-0.021595290306765894
-0.04339885346787013
-0.0794030648295898
-0.016481704156454953
0.017579303019484262
-0.020165932129781003
0.05984202237960477
0.0007105359556473703
-0.08776815684691658
0.06209849725197292
-0.11975611387322599
0.03497153233847444
0.0075162586338859486
-0.039432245858482665
-0.02889688066829811
0.0038188245502728518
0.02600925091001321
0.02008264455145456
0.022417835637284233
-0.07018021489814909
0.03557892426462764
-0.0729737163875272
0.0019135214931324668
0.008723675989821753
-0.05778833522625295
0.020531757371611847
-0.06784335225778652
0.032835160721328964
0.06795029354697558
-0.02087132853467703
-0.012667707000068064
-0.028459384770623607
0.0029664206536308205
0.05037242580983345
0.016089293983513754
-0.017199879830959806
0.033767409907947056
-0.04760171970591525
-0.023689255658520293
-0.004310375357067079
-0.03187707059320817
0.06620047812150456
-0.003983928361337111
-0.03165829875326496
0.04220768376428918
-0.01444438052209121
-0.004399288358741326
-0.004711695587962858
-0.02260332505085447
0.0302669376035401
-0.012776024888005427
-0.027402588481376656
0.02393182097952666
0.0012749273062600038
0.012580820055760462
0.00538485649131684
-0.02467430367695954
-0.033258558086815146
-0.0108112931487544
0.013749340279640771
-0.02297341844305032
0.01699700439163552
-0.021904451752356772
-0.06890838603540818
0.08092050014934843
0.01278716848601846
-0.024895963675378614
-0.02500244027939153
0.0017646048190923871
-0.0039639635823115055
0.010876504885903072
0.011122116262572332
0.03838947700229967
0.019018614552761753
0.0034893403250273788
-0.030606649787005093
0.03466197365852923
0.01024685144081922
0.04130474383352279
0.03680527022771402
0.028280541004069668
0.03264471788806608
0.01639126916481834
0.026260469771010847
-0.03177139444993185
0.021681019911778652
0.010794554341357239
-0.016489232931181968
0.028015875439690092
0.0130907406560778
