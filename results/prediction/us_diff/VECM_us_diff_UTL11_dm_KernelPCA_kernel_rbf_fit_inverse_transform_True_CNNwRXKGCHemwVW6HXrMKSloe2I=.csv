# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; UTL11
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=8, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.08881025666208153
0.0822965452729987
0.08265182800701866
-0.07543937218310762
-0.0327920063303128
-0.03548678642195051
0.050187988370327265
0.009535705228612412
-0.002893156226118637
-0.03545686920968615
0.04175515765354165
0.00112340595664525
-0.05303720663542298
-0.042003807908214556
0.009382703953125945
0.08002730370145056
0.06121635782651019
0.02426854885994721
0.03163380405023851
0.02553214218279532
0.045332709817552726
0.0882840140714326
-0.04660075141327888
-0.05678473143529869
-0.061855860812284565
-0.007090192275429522
-0.010392079120655484
-0.007879738800114618
-0.03610111763967545
-0.17761490716834383
-0.034640692543422724
0.03655581340693435
0.027959349605322127
0.015061923794946324
0.018525223637046312
0.037203456054536
-0.043886883664883955
0.04359304417149624
0.013540341386643291
-0.04666029699787268
0.03418360510826988
0.06651664563151859
0.057020109235169786
0.03715077592588314
0.007229430325050696
-0.0007905020864395111
-0.02429377019275472
-0.06506270594376024
-0.045300151072935745
-0.037824685373289826
-0.014513607304578482
0.07223574660749735
0.06704808032047034
0.053525449724312044
0.021584712135437733
-0.0008812578381759001
0.05019569788515049
-0.008531672598224307
-0.0009198175291172152
-0.009876515310892528
-0.07051701937383208
-0.0157025944941609
0.03851221403776117
-0.021703580938251744
0.045383236838489494
0.03163742347683589
-0.03940143406130312
-0.07109673990539532
-0.05803310867229572
-0.06806433190919892
-0.08858828377125055
-0.031138051707515844
-0.0800863381528993
-0.10542886397876766
0.07734057632918648
0.012720487250704635
0.01649500888189427
-0.0318171992232168
-0.0669446149107444
0.020510287001564546
0.0659643631782312
0.06122913192409196
0.030070341991102163
0.02019503581366785
-0.011347746846491414
-0.031080826328090505
-0.028709817008215555
0.0033919126920428726
0.03652581659690082
0.017666355408262637
0.06545587206502734
-0.04330679672636924
-0.05277207226055058
0.013571900795209042
0.03537249001121252
-0.033910184433454095
0.025214701640982197
-0.061766994908441154
-0.07175947220074047
-0.02140925737319402
