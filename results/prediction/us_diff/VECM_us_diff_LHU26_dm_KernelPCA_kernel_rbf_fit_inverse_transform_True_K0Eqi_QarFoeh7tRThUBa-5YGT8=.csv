# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; LHU26
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=6, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.03198022585448407
-0.06785616135444036
-0.1048805575507106
0.006957116584493488
-0.03192467654549352
-0.00857843564639877
-0.032472975371302865
0.008362247926822115
-0.0032431170989556234
0.011413770360409116
-0.004873789321712625
-0.036629629481569496
0.046646775044541365
0.01582074213431072
0.0075788714477921845
0.011431215920802887
-0.05807484360163086
-0.03817235700595117
0.017583542562856096
-0.08414897736469618
0.006075340363642604
-0.0963910213942939
0.008285054414571169
-0.001241187251999936
0.0367765986650258
0.01583557056744554
0.006764914967154444
0.03152184451919943
0.07277092635531021
0.0996482707947518
0.05191857524486912
0.03771673270646864
-0.0961207623913296
0.07621994082470443
0.025464952607649784
0.04784142650205245
0.012543684273925545
0.018507749250981385
-0.0320860934835173
0.009380102532236229
0.001580132746924378
-0.04101229313981296
-0.03131999352867075
-0.0855672319081764
0.018019816817349828
-0.07067770880050243
0.025503944635347484
-0.007252259629487126
-0.023230112304657553
0.03165858507377874
-0.01887188275499252
0.020931900291099297
-0.026665563504523607
-0.05080681661422276
-0.019797697122929857
-0.021624578139500528
-0.059938261206825855
-0.028637068730648647
-0.010676056172094653
-0.012594116954575029
0.027927450691666578
-0.03375781938567586
0.012922513922463888
-0.03333934299379944
0.005516432007996602
-0.04600864082762423
-0.016104102433952668
0.005237105163807117
-0.00021231349976815438
0.02936327201062741
0.06654105145342618
0.06802606075767997
0.06798917989919578
0.077728883153203
0.03148365995375329
0.0175836597309407
0.020430189475388636
0.0505471656612038
0.0390718424811584
0.0007231342015225979
-0.0016577199848673155
-0.06348671065894601
-0.04422188111891832
-0.01408630716118129
0.007208955733795826
-0.03070781673661218
-0.003772703737319999
-0.01078139596562011
-0.018746856035242378
-0.012225076844712222
-0.032440677311734466
-0.03022184023612739
0.04117388047934437
-0.040056795860398814
0.01780970495487037
-0.017943194761845967
0.01440400800872483
0.04138661275005057
0.04884084749283981
0.10537400959255604
