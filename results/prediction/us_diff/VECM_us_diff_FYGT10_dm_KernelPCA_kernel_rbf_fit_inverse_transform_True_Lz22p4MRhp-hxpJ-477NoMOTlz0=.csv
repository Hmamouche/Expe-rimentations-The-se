# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FYGT10
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=11, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.05939574977254686
0.10697791381752206
0.07294419999873836
-0.08675519408353445
0.018567114693495416
-0.05198711931337059
-0.06046068300477435
-0.03946629674087015
-0.01546240964062768
-0.020864343003545283
-0.08262604178611628
-0.07633246506305598
-0.04740216204387947
-0.008442900225551206
-0.03861777967887896
0.053632654607670846
0.04852750566844517
-0.009682693505509352
0.07335718669675044
-0.03992473429930402
-0.022869887548452983
0.09241506074383003
0.030814364590031475
-0.09804065116373147
-0.03403970208720658
0.00830198892739008
-0.010686349768320742
0.02466128719252267
-0.054713208578217876
0.031839130350498554
-0.03752366158136628
-0.07888736784905925
0.0728522187118449
0.025653221726607296
-0.015038637815022387
0.0043118887481774365
-0.06897037749924441
-0.00881618558942119
-0.019862102905162805
-0.06281559036286874
0.01966174929159999
0.006805869203683763
0.04506712918363942
0.031011676146057914
0.06780908171357826
0.10954972411630162
-0.05395051897304909
-0.012195145705249523
-0.038097613833474084
-0.0363184890750758
-0.06099911984379199
0.03960983013540798
0.07393636209791166
0.010550296939308024
-0.013990769394616699
-0.0002269961798074295
0.02598855078611701
-0.028301014943406352
-0.022704169291214132
-0.0268052515913261
-0.03336238231991785
-0.05820645749434513
0.06369549332986654
-0.02340755377323403
0.013369916819904466
0.06733908411476286
-0.03009019444645341
-0.07454597763145301
-0.017385788137143407
-0.035964972070276824
-0.025812610762337134
0.01654436297936729
-0.031784208374646974
-0.01548831550234766
0.04857790851180557
0.0037285175122605888
-0.06009915231356551
0.007367919995265947
-0.08036532892509475
0.03068396992722124
0.009578030982722384
0.037515579528950876
0.022441800584652512
-0.04398804499456822
-0.04609215040582942
0.007245358005644138
-0.022717057544979977
-0.014145585763040383
0.029355924510313235
0.04005248489946725
0.05489393336837267
0.04456143640784293
-0.003349407803154871
-0.006188829999762351
0.01480091829036967
-0.02216664788017175
0.020865934599974257
-0.06162584929599218
-0.07484380310800967
-0.023507876446882816
