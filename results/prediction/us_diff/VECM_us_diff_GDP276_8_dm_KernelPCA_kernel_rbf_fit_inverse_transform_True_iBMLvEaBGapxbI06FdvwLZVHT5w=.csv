# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; GDP276_8
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=15, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.0035934721839884103
-0.000962451228559633
-0.00011559117209197257
0.007737690834832082
0.004707506087755919
0.0030872653294115315
0.013793373100054419
0.009133150275873652
0.005998904475874261
0.005186673028482982
0.009968885122144359
0.0086402921847092
0.009900181442580135
0.011415802337263905
0.003224814973709606
0.0037995339789943313
0.00365479922136849
0.004936417775813137
0.012459240495359257
0.0025685198596016763
0.007470649153960082
0.0050380164754593535
0.009639183346900199
-0.0008057882034901657
0.005278956278245914
0.0035522875626400234
0.0028631067573262397
0.011155945813652061
0.005113453300416135
0.012101082265345014
0.00010985161501178391
0.00022623998110248792
0.01288942094347215
0.007207620254639975
0.012555991366905184
0.006043019334318239
0.013370561065820777
0.0055374774726760926
0.005689924445218602
0.005964717274303578
0.0020500460801293353
0.005270047098044153
0.005861643758284466
0.006248021522956319
0.004664851226468071
0.002725500128361958
0.001885368007809599
0.001980987616324586
0.00710678846808009
0.005975045574866242
0.0012218937242332265
0.011640361732128672
0.005820809252688289
0.004752666933430797
0.010283377294711295
0.006418383774435803
0.006529664160967819
0.007963665058985468
0.0018934735367992055
0.002994344658885402
0.003082398371018136
0.007207924833160623
0.005989046012863473
0.0034511511347817426
0.0029674855603828453
0.007980050754792039
0.0035707201163421806
0.003872139123015409
0.0012815607158295332
0.004975008577575777
0.0027967345188894205
-0.003188096949405864
0.0032533936580211196
0.014375855361644027
0.008831866731529792
0.010567101142480034
0.004975922821739037
0.008790241995998888
0.007204244923678825
0.0011118030987345233
0.007088475859871001
0.011086285470903331
0.00897996468317402
0.01269359369082678
0.0013949544052806221
0.009125718078576959
0.008487424426527772
0.007562976678078932
0.006872131399282791
0.014478328146915903
0.008679267182335854
0.008530866722892914
0.00917025811047759
0.0069010257560826755
0.0073580724525982456
0.013674418676600844
0.011109148701014242
0.010702272103980953
0.007729214859271536
0.011505019380711887
