# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; GDP276_6
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=3, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.00815441406083969
0.006910428792963758
0.007466266819868709
0.00733338588989179
0.0066506097008915115
0.004715031400146207
0.0061724494115774735
0.00518744015907689
0.004640807184089496
0.004505308854595753
0.0038837525938432804
0.0037729470586024792
0.003710799566127532
0.004424837700625595
0.003116455952810035
0.006091045657728029
0.006171025864025509
0.006469891020316145
0.00841909373558708
0.009514788022416152
0.010439689225525746
0.010451943475775721
0.011075406829271052
0.009839640428615385
0.010304182938089953
0.0100470310055696
0.010255217343198134
0.010298791361342973
0.010258120611752226
0.00975247779938678
0.007841182070918534
0.00817450714893089
0.007551275861612968
0.007832975004054811
0.008212447990008313
0.008812671346061465
0.008446064963915381
0.007728210738132022
0.008364041412517738
0.007680505399536717
0.006990128640829507
0.006309395994825343
0.007206103123501161
0.006597172055730643
0.0070053685414988975
0.007546650204317651
0.007564708055437286
0.0052990897083383046
0.004740592776553441
0.0052356977403648745
0.0037426629714913152
0.005727817410361611
0.004943005062856435
0.004998507090506155
0.004545151048427189
0.004820343469944099
0.004410221244265719
0.003435360580576571
0.004495031904718081
0.005741970255615247
0.0049500611693865815
0.004575214681689852
0.00468125920062824
0.004258716375388737
0.00531250900550789
0.005553782079489826
0.005231755449298616
0.005881156121436613
0.006841122888260793
0.006437328192133898
0.0093578144395827
0.008043802461267086
0.0070331905062233115
0.005180682211684197
0.003904421858855334
0.004963724213669649
0.005901250514665063
0.0068807942958402715
0.008491641619545374
0.010085826111104933
0.009554624183335171
0.010693717868453367
0.010371355062590902
0.009212564296764222
0.008434907278732175
0.0070533793291032995
0.008672664973576995
0.007893463298370424
0.007936208785744966
0.008505902318125246
0.0058305683302803816
0.00735974554585616
0.007640094502529326
0.008009110011125302
0.012047660909666622
0.008799374197020414
0.009799194249601198
0.005307396607719925
0.0071315845458199524
0.005758936758732132
