# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FYGT1
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=10, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.10737777334758486
-0.04490413030603954
0.18971820310407714
-0.0216075301265011
-0.046612486744970184
-0.10797546934750434
0.0032994290718806607
-0.053833683918805005
-0.01509580876750808
-0.010173635889931587
-0.07518444078332083
-0.04978261996159483
-0.030924905216077835
-0.005810848524533202
-0.053535054429791465
0.08540882986381657
-0.018880155496001333
-0.0824005556700953
0.13066356611665464
0.006406678589817048
0.046142143299950734
0.10958507214908299
0.03708412757487745
-0.04171214838910403
-0.06475782340818745
0.05048879347184894
-0.01044126319407579
0.01093061758408876
-0.06516607868203593
0.029865918147896017
-0.13636078432295515
-0.05436205712308755
0.053028198836023444
0.025766811131372847
0.006978304897901962
-0.022541118400917843
-0.1441502724402246
-0.04938370252971883
0.025399823425547738
-0.06876591904913901
0.08981674055288784
0.014965972308082054
0.030965075427583078
0.0008851725214451579
-0.002273679643996593
0.10014094504065911
-0.022307653636203365
0.03417208356582906
0.026449787264099224
-0.03747604854843982
-0.05145913198354273
0.006182373670542214
0.0012444283834324565
0.001331185417356647
0.024812310573875042
0.014356506494109147
0.007676467119753699
0.01481190352083691
-0.044732755217945896
-0.02913478085303537
-0.03681875159868521
-0.07324384038489037
0.041410545225627225
-0.024597312355940596
0.023594130817785924
0.07058415795909187
0.02448062410314869
-0.024610317886075626
-0.01999853987936475
0.008081941331883292
-0.03957666922925378
0.005849083308968436
-0.07274911927302871
-0.11048277084444823
0.08652582071286186
-0.00494266300479607
-0.06647334797167412
-0.01747502277997945
-0.06260898705657927
0.055284391148985354
0.002482259612668857
0.04414748608781756
-0.006742476049203512
-0.03308005378084457
-0.010915490191190345
0.015222590113481503
0.006938995968513689
-0.0012854515032642904
0.03905595473786381
0.03774898815418548
0.02540822384658404
0.024284152557941294
0.024556252550328835
-0.02640428017462798
0.036286490035319016
0.0283720805945675
-0.0044389543521893655
-0.0706392274519615
-0.0998459971722945
0.002653010533072604
