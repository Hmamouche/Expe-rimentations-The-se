# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; LHUR
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=13, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.10544071063503341
0.0047527701330009635
-0.16301754862348242
0.08426400959214228
0.06649955268885739
-0.042287187315847505
-0.01774846210349941
-0.022912290636020384
-0.04191807738856896
-0.10441388488400222
-0.0585892153115574
0.026893502819871178
0.022987827621227086
0.0055334210640840785
-0.0002668740660429249
-0.03588746060377368
-0.07982810233345444
0.016730628425850554
-0.07006641945433921
-0.04385324833489732
-0.038611663633723356
-0.1072264462248633
0.02213113297251298
0.027012268136096612
0.030114759440898843
-0.0027091898416866872
0.00813380201259057
0.008163212103182091
0.02852970838545729
0.09670029456503267
0.13073117004072762
-0.004845350867594194
-0.06341348733350594
-0.0320492065115517
0.03681330720438674
0.03729331022391681
0.02609381388824708
-0.005249425301601666
-0.019380531863210464
0.026535098096034472
-0.06610019795487511
0.0005061627306977095
-0.05628123223850339
-0.0309080874277178
-0.038548034405574
-0.030042715108216957
-0.03769009283783614
0.002259645100000913
0.015408519693825168
0.015551042856934094
0.027455605055410615
-0.009058002358609075
-0.051836541122245124
-0.03505364181873673
-0.0516338794055544
-0.0027188074209851913
-0.036401949995326865
-0.022952344042015224
-0.006268585008495937
-0.0008824868201225862
0.025385425556688285
0.02400666811743875
-0.022316363682581886
-0.006860240754417112
-0.019583036477912263
-0.048536652450897624
0.01849716836995516
-0.016113523032608438
0.03383590563909707
-0.008121454635591527
0.06301733600568546
0.04555171618834183
0.07196150141265298
0.06849622124272566
-0.047258123836334015
-0.012269747079121606
-0.02324875648503033
0.034589810436845975
0.042050932562156335
-0.006297849106357155
-0.006874369215695255
-0.054474641381639766
-0.029626042243271648
0.02768354732438888
-0.024316094109500738
0.011420304582288907
0.0009016262173586337
-0.022870971839103367
-0.01310975103073872
0.0023953725880724247
-0.03576488689734908
0.004885672526777254
0.017932673108073543
-0.04390066870324573
-0.025810166213695824
0.00134880135344154
0.01154226030842688
0.06752267812676115
0.04539121466870104
0.06503285908832518
