# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; PMP
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=5, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.12561822946885884
-0.24851636897246754
0.1076768874046391
-0.10232645362860493
-0.08697193880613022
-0.04909232086672516
0.06885049357540808
0.1209212358128874
0.022688714543925015
-0.04120137994343323
0.08414559976553154
0.043542017293496174
-0.093708967900083
-0.026620686423374902
-0.01589271486992415
-0.004488372533711885
-0.0427347852493622
-0.05402923158699132
0.11423913895004922
0.008529357250157295
0.057985827714494675
0.035785968474722374
-0.09720722918619001
-0.053669744427913335
-0.01665391609120241
0.027165111191710077
-0.0030312557036323642
-0.01601944723079614
-0.05693050015129987
-0.014143611266883878
0.21340860114909485
0.07391806605532297
0.004895016302889681
0.01861104458180389
0.040491245563023595
0.023414259222129915
-0.012918678238876415
0.04481416143668222
-0.11273251110396824
-0.030372559164605798
-0.004227091913676054
0.035098626326374394
0.035440993497387205
-0.07198539217174801
-0.011199905961030151
-0.025356559530123265
-0.1292066566587266
0.04916069193357498
0.03027501263129751
0.012999931816043395
0.028473775954832407
-0.043976111518177735
-0.037155779430875724
0.0020608635158045254
-0.022117862269829792
0.08792908141354121
-0.016414730679673473
-0.024283215627941793
-0.0038907380225061614
-0.03514067695655538
-0.04647943754056562
-0.05551137524266996
0.06336670100196931
-0.05001331703020588
0.06768097947402088
0.0596603094272944
-0.00041212995683664357
-0.08825950748240585
0.0021520003801788895
-0.04126768922756653
0.02408090901037336
0.08031993118362045
-0.05272082792389211
0.09978003968944085
0.17912755919724177
0.02550726578305569
-0.06872811231155368
0.006009200978511747
-0.09729257492557417
0.11685721228314241
0.031212901360581427
-0.032145983449075044
-0.04131039609527878
-0.07997750702642847
-0.014788464991363676
0.04974966050168384
-0.07452671743627082
-0.03079534128536933
0.009164623366674294
-0.09434444026862474
0.10047468926611088
-0.08031572511672995
-0.03101549173767284
0.02838699840030684
0.007498370256364337
-0.019835269027408047
0.0004473886143122191
-0.04095889708112344
-0.1084716327220802
0.1218557359613972
