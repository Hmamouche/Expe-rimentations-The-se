# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; EXRUK
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=13, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.051315296764627906
-0.06814437537404983
-0.14758833391742504
0.03523883334787588
-0.16162812556295197
0.08993806814515423
0.06163475591346188
-0.04343040382656955
0.006358652173968448
0.011207332454785404
0.06795961820216878
0.07552727471621565
0.022099614368254788
-0.003421115028602661
0.042163468784171224
0.015018310137504869
-0.038840858408897026
0.10711462257149584
0.01799701176656933
-0.07225900041847301
-0.08176144214495659
-0.05465126801702939
-0.03694077387200057
-0.05189428465807375
-0.011587068394046952
-0.001635760738729359
0.03952741593463577
-0.0404437746428439
0.078439880953247
0.0948461234757142
0.07859244421906961
0.042190507790226185
-0.01459668008670683
-0.019689988613620844
-0.09218951277781345
0.0006725963981157393
0.0761628381746194
-0.07545267271036325
0.02393683743521191
-0.012394229923421802
-0.024540013387350464
0.04605929718499724
-0.0741722411863021
-0.016058339635339043
0.018515441798311157
-0.04932733674971079
-0.018495134466232167
-0.030181658845272144
0.03677441089949307
0.02454043969111034
-0.032540496590814594
0.07494596566137127
0.004518673073386558
-0.02594952614254527
0.0181776037248836
-0.0006404942593966033
-0.007198742673313512
-0.027401332406452823
-0.0005176673154617321
0.022620474345536355
0.027232412441523406
0.04448793122531158
-0.002367930170734868
0.021613564458234563
0.01806526182759413
-0.017474361206512214
-0.044105866455879224
-0.05111900665880101
-0.005386088851228777
0.0034891870940755403
0.02445959355235425
0.03498412731082576
0.005193265770911569
-0.041813424695351764
0.005220865640093719
-0.02912883998019518
-0.016478943486669874
0.07443609452209213
0.09101361638963734
-0.07050967214622911
0.023693689312423716
0.056213432085689304
0.006957920555079995
0.03795212001030897
-0.004597378928316236
0.012283178319497996
-0.03291119380798275
0.025775067665279535
0.02054171780550795
-0.02412642424075974
-0.01983435358083691
-0.023771384592975835
-0.012739039367213063
-0.027476635577868655
0.1038231930860367
0.05549584649015846
0.012750286634320136
-0.017653007523292484
5.356143513023624e-05
0.02315416978473015
