# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; EXRUS
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=13, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.03448455597559727
-0.002076670815681251
0.02387104455081533
0.055810791475563046
0.07214886733653005
0.033840826955386386
0.027277032022879018
-0.014420194316187632
0.016470786216142595
-0.04068964539229854
-0.14428102691276523
-0.03820905746272223
-0.07378411537004222
0.006434658945140542
-0.09310024367921842
-0.039422281545417645
-0.0046408184814915356
-0.06316612596278365
-0.028935454960714695
0.06455137156598101
0.030991497851875244
-0.039699253280453345
0.007076621647772542
0.030733075452567166
0.02364959008920326
-0.007200017382648999
-0.05039722910265754
0.06469839772458812
-0.09812437465326725
-0.06691379717824472
-0.04491993129426105
-0.004407284837345261
-0.008893478097570956
-0.04894844827785988
0.06538704426321881
0.0009924172905591121
-0.046812714786089435
0.06336682659164718
-0.01697602226900631
-0.0325179747495914
0.03225766720874382
-0.0765863157573567
0.0177353127527198
0.021046179748940165
-0.02718186752710751
0.05865790608358652
0.003253842569742652
-0.05074305793628345
-0.016681311304450913
-0.0181001256240581
0.025112767310436414
-0.04060729724761774
0.006200828186645618
0.06744978677482606
0.020326266054431448
0.021512369237301675
0.03495189166272666
0.05449768270744558
0.023396814102155952
-0.004191189517545239
-0.012400675454293198
-0.05291459808164641
0.008245892674871566
-0.01845902998323019
-0.04669190284716952
0.027661525010387246
0.02597120021644966
0.028678164726610743
0.0036396287928531813
0.043245043395092664
-0.026880255672437826
-0.01383002928722478
-0.009365388319108275
0.02881954411909633
0.0348307289756735
-0.03435493484905743
0.00559856570312992
-0.019534038089473772
-0.10894022382995078
0.017203851341219226
-0.042387727927413854
-0.09333188404495227
-0.014701469973620334
0.013398889042733795
-0.008294307594497395
-0.007830565129496092
0.006483492706348452
-0.017907384152063284
-0.02449822418157309
0.0273949076963873
0.02241139361878791
0.01279044810373002
-0.00794133666528932
0.06690385625663733
-0.041673784761715335
-0.054801915714529005
0.01937491058937023
-0.0009910377614908445
-0.04105637415820247
-0.06046580783010041
