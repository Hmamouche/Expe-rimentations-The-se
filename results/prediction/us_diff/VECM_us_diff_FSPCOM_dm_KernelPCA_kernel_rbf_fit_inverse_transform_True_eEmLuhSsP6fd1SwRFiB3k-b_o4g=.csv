# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FSPCOM
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=4, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.005205521376178737
-0.0030920897929265366
-0.0012987312637881775
-0.005327973510000528
0.0007712443137209956
0.004059969338310098
0.005549826847605883
0.007456329252602841
0.00496602967085741
0.006673596092436157
0.011479999342779585
0.010050053120710807
0.0017333449790255917
0.0058359716229543996
0.017284597220241806
0.0011843933520352937
0.006709030217132066
0.015747773231008448
-0.007670692222794668
0.03359976732549493
-0.0026290794788248256
-0.022174414252674298
0.00467147288909136
0.008367629362844983
0.013496146849038548
0.014000380638438876
0.007806674869292961
0.009119352609364789
0.0038774961901004196
0.00036409404476265693
0.008720209656243321
0.0015934846039176236
-0.0008495306348961115
0.0059278214720681205
0.012850876461638551
0.003943886564395543
0.001685023746700039
0.00862228867616188
0.008601298389871125
0.008607995066487729
0.010984929883692726
0.007158325660709627
-0.004434504618185685
-0.009433732816167073
0.001269535222950288
-0.0008409435579383839
0.0010444668677404957
0.01888575696347889
0.02369939263008303
0.02416243072356793
0.023672637003059698
0.01674524581297339
0.01036876748328051
0.021365447934524325
0.02621409110469518
0.026826970451796352
0.045486273544857445
0.040036602530083405
0.049693542755768115
0.0374130519332116
0.05570933254075889
0.03800552540431411
0.009941522783031273
0.03503418864210128
0.04294629698155506
0.04824148075313782
0.04150261265444065
0.028378602823831173
0.027764792387229443
-0.0031510507829942136
0.00542585456887701
-0.04474315171454122
-0.05714360663042313
-0.021392231814462558
-0.03582439044629952
-0.025121232584240878
-0.07083663241529711
-0.008550964506353955
-0.06777217917111146
0.008702798744981066
-0.02393657893273106
0.030777798352963945
0.034085581793113474
0.016513788140935397
0.014711986639576076
0.029460080594687313
0.02653837365246143
0.011173702166553032
0.01807293069466817
0.005061865630582464
0.03176207670926689
0.001746253552414449
0.00807738029880925
0.041365439775359183
0.025730709447465167
0.04333122690126054
0.0035936223349054153
0.03221535016424825
-0.04583468135134753
-0.004919852858390234
