# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; PMP
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=3, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.16017656414632958
-0.2038378111753381
0.091075466247352
-0.07417754881183614
-0.15814482244346692
-0.04432863952551337
0.14295936649643506
0.08484089637334244
0.04056817117075731
0.014365616508342699
0.05165745262796696
0.016980604994035454
-0.045333917747871676
-0.04342878313138157
0.04047731175115965
-0.043732605579043834
-0.05982826271304921
-0.07474772501014805
0.03169594328455589
0.0312823861912565
-0.03235907047243672
0.023129997394363874
-0.08108011118504248
-0.0406597100545954
0.0206288438610068
0.025653460163394207
0.025738797412710235
-0.04827578612582721
0.024824193775600422
0.04176707125801302
0.10215870710240005
0.08122977279510663
0.003457232672927482
0.02577405369993665
0.06979789833877065
0.0006977983522668263
0.014164082088073546
0.016923757181144795
-0.01836468830249202
0.04981412543892481
0.021792443215644705
-0.00577044619392884
-0.04503455944413151
-0.04098365162210528
-0.08814132680576485
0.015953347118483636
-0.08883118130068482
0.015176594401183534
0.031368478178896986
0.018835253876969084
0.0276881132668126
-0.03504968819672849
-0.02059286991812157
-0.014718559861228688
-0.05080699255119337
0.03927169157745782
-0.028913518833681335
-0.026871671318901787
-0.00039229444361995203
-0.004773389268780879
-0.03214596395416136
0.026855754155127864
0.0056665558075265875
-0.050614358378294165
0.03153224331459713
0.0067444520705153485
-0.014165952666464865
-0.059858161785435196
-0.018538793486004193
0.0057809588446979965
0.08367100517453052
0.036856772829420556
-0.010076473994872416
0.07887759560080197
0.134600503857384
0.0031228840676148714
0.042872012835497686
-0.030790810809707504
-0.014021669957746275
0.05541197024053998
0.029290863968781045
-0.0016264921286499243
-0.06714205366917966
-0.04019788438576128
-0.04000945629716833
0.002138277906312937
-0.008192048165012782
-0.03059605277058671
0.017060584429279567
-0.05651127816741045
0.03054090400594689
-0.08035464387783578
-0.07126354193308054
-0.013571028425625183
0.025391398408466728
-0.0029038814095400795
-0.002885331071392244
0.03164540183181067
-0.07060989147776645
0.10130107859333369
