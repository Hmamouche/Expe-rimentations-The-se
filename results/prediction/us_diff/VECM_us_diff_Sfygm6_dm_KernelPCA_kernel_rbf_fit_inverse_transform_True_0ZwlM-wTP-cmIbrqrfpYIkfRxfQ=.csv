# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; Sfygm6
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=13, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.03194433944754976
0.15819729863013332
0.22516253200498615
0.012413296714381059
-0.17819355991601743
0.15733889485657565
-0.09091643073852032
0.18746242991045037
-0.29811444864635794
-0.045584968958666275
0.0808866659878639
-0.16302522924812227
-0.13795206586686773
0.18309045278806463
0.093252251930781
-0.023456392705918344
0.11485959376901327
0.07259034874963999
-0.19759105102798954
0.019066412825056556
-0.1495416876703761
0.0581191566071051
-0.1199590401144362
0.0708152640267095
-0.08383644496378952
-0.15082638818441466
0.008874580832729018
0.13773519604357473
0.01349611031575796
-0.04035916315742149
0.05438682612301808
-0.1442644060465394
0.019105655348838887
0.11361858638937536
0.015155384577335305
-0.015176987954029764
-0.08965132137772869
0.09693609941154875
0.25607339523982164
-0.01673831818075227
-0.0012868308463014483
-0.09419684410259654
0.04544057733165958
0.11537393849745098
-0.008591452590925707
0.07221581688469049
0.029770892157653488
-0.12711262058983802
-0.1178145302058794
-0.030450668861911633
-0.05980024875837893
0.08947929659164419
0.09249298223746681
-0.11067474467716866
0.05338747710522992
-0.049007258184700045
-0.11035722389842377
-0.07403540860248224
0.0373856280215412
-0.03021960233565373
0.03271639842734691
-0.02240204354819385
0.06506091699501762
0.12451617765854266
-0.007530981487125846
0.05738463102073486
-0.10649490943818125
-0.05276536564135109
-0.013970361133159854
-0.010812041805634255
0.037129684897818024
-0.032200985502449986
0.05010062158664955
0.1487279388761783
0.02571213995851994
0.015300040537727752
-0.2318300147058525
0.09093235296189113
-0.030845437066655358
0.06758423301691671
0.009882459301609817
-0.005213962996011526
0.06596708655196733
-0.0833388491755134
-0.05411767651881654
-0.08202579072421914
0.010175096451257332
0.120989429835014
0.006903661998749949
0.006932342682200519
0.026250841769565686
-0.006107420258390234
-0.08998155880162279
-0.04685583186218783
-0.1125573584419866
-0.08175914353647123
-0.04953899697407052
-0.008239659030459208
0.060933575165481986
0.031200764667294316
