# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; PMDEL
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=10, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.005987279631584186
0.0918226694744418
0.2584353535353239
-0.2053087682002037
-0.06831737418823487
0.01570543714550384
-0.05270530132212517
0.025066194262600254
-0.09727323695382273
0.09407598830158236
0.079642440565986
-0.004898331188476844
-0.05961355447010411
0.0026952899997282447
-0.057635945261007085
0.02579492207830033
0.05136469707808461
-0.025742480099403386
-0.027531514859593025
0.026731831831963482
0.038306653779322235
0.011488584785948386
-0.013155921356062604
-0.07137699081064541
0.01308289203334773
-0.06373489770408629
0.00321486640283564
0.062295685979841614
-0.15924025099545783
0.013897985739548289
-0.0001662189300402142
0.0761397426364068
0.07757630656110809
0.042645951462460564
0.040945888787486534
-0.02597975674945579
-0.009065716114633297
0.0465183747973851
-0.066937982341519
-0.12607557289442514
0.041506100903309924
0.03951981644405891
0.1080589997507502
0.011213961181380074
0.008947045892429537
0.012679216265927929
-0.11444635697219922
-0.022788323814277273
-0.05712222134061383
-0.03450675078495623
-0.021568951956768195
0.03256506015116169
0.044169195348884305
0.08888039325439097
-0.00042156823120246406
0.006994495494812791
-0.008543007035407151
-0.011379848823393629
0.006435414965627471
-0.04359861790592111
-0.05954122963257357
-0.038875521952983445
0.10403118599562491
-0.04422481461350649
0.03764307801520177
0.054143725913504845
-0.027546004841659005
-0.025105149858147168
-0.06711036301627736
-0.05334471604179107
-0.08325743195658511
0.06346043355783426
0.04274749664089114
0.04676229238093847
0.026533294557024065
-0.003078716518802688
-0.02871916578515472
0.04289112571529318
-0.10132244160651346
0.0553052702412624
0.06455292378170152
0.09061316866174232
0.05448838328860549
-0.04039210842368269
-0.056145593572367127
-0.09335849331006302
-0.06971391521122366
0.03359429769066032
0.0025090627719888264
0.030669397854136436
0.0260171322778424
-0.01556433515404584
-0.026758285307158475
0.04577385366212642
-0.0016199533451682763
-0.02320779972017523
0.043105834219127774
-0.012846059813754421
-0.027394908526394236
-0.045490514385857925
