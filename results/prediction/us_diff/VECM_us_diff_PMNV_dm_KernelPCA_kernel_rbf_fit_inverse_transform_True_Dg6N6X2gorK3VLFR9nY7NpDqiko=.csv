# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; PMNV
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=3, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.24358405978487266
0.13455303696155424
-0.05894910276983524
0.017182903398474378
-0.20767024673251985
0.04383292681349725
-0.10051965782905259
0.038752371597348484
0.00788786140295131
-0.00835103120346458
0.022350923171631214
0.005532720630759727
0.005032581600832189
-0.07974369993100416
0.02546228858296727
0.03254958469491487
0.06471100843961418
0.010190839254797454
-0.07043110914435387
0.08823642895257972
-0.0436953164732615
0.08527291441457292
-0.06295951238006076
-0.0556396911706614
-0.046405622853468546
-0.04071445551711727
-0.020981464883196858
-0.011758383256937843
-0.029534511495060534
-0.1180153895123078
-0.057530210334638995
0.05488310980378664
0.14519304258407653
0.06961905655230088
0.13372682726331753
-0.10877352294360611
-0.009650216419492378
-0.0011506611382518864
0.014911145992491394
-0.03237647437194486
0.04397621772265977
0.03962881104343692
-0.008106606789996237
0.1156430260262582
-0.04173784183840174
-0.02882539386885425
-0.10091321272808026
-0.05555095846249702
-0.02031622657292817
-0.05351960515361359
0.05369975355414981
0.04002569742013974
0.0020310313322595653
0.041204339159989244
0.02902831755881481
-0.030985222180823127
0.03737119678390574
0.009551114374393953
-0.0037836925460604017
-0.024322524305553823
-0.03931387490511143
-0.015499385324820826
0.048214919135421065
0.01073644707327757
0.02580997585961179
0.0802291813846488
-0.03694639734923648
-0.04128155107354432
-0.0353893887468206
-0.056383028682410144
-0.06811818164998028
-0.01903821140574457
-0.007000628058315472
-0.05574762205078062
0.13929041921596208
0.004017897512757839
-0.028438170303142087
0.06400615053568533
-0.1286681839085207
0.05964732968499849
0.0832280630803318
0.12608257435904183
0.044918541750575455
-0.0023329383911909916
-0.0353583687618986
-0.01809884987385993
-0.0019253969542953435
-0.043570918911922524
0.06350599674310148
-0.015138953098650992
0.04536290159812722
-0.07324431424726195
-0.05991477121932566
-0.04563064341821156
-0.03487922946167356
0.05628502862786208
-0.02687992953531368
0.012085047849677242
-0.09085381502711014
-0.010198088254673228
