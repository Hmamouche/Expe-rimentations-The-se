# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; EXRSW
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=15, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.01388447847076306
0.029280946935554206
0.10850306371894891
-0.013812422341995808
0.028431860259231435
0.02213086645443511
0.05497056653037713
-0.03747894391271495
0.02584365716263867
-0.037173256124910806
-0.057916802922422905
-0.0017349768382892254
-0.11788284182040976
-0.0208418410975997
-0.006208904630827185
0.03318199727847207
-0.03166336621997054
-0.04608191749194342
0.019974682574010603
0.07778902594060917
0.010715568243355364
0.021828958654647464
0.022740080825586935
0.008660416687069813
0.034807523367646996
-0.008273605946103655
-0.04554402863693656
0.022135408953045915
-0.05283394944953071
-0.019713586034786738
-0.026847715139718176
-0.049298039363663414
0.002783690758620099
-0.023242290136113456
0.0010976609259166215
0.02428887874045341
0.0009377470886445822
-0.001474554482417869
-0.024326223442811736
-0.01653874444963064
-0.005014582914588214
-0.05229031199499726
0.02668758087977876
-0.002304034033471644
-0.030035204882899107
0.03738863738453321
0.004389453006463871
-0.03319410213969351
0.011419398776385984
-0.04509601476058819
0.013708128711873857
-0.035857308542215895
0.028880446235037337
0.055166090282353406
0.026113042807130848
-0.016867890874571065
0.004041300422437244
0.011388489581854032
0.022237918242154923
0.0042235328060600446
-0.03506107182600588
-0.04199188539599396
0.04360594104827861
-0.01403625785608785
0.008449056040544994
0.04150491682210468
-0.021441889122875604
0.017803099938264854
-0.00683042176094659
0.01044803272765307
-0.029842794374527852
-0.0006659321723171937
0.038277565304456584
0.012818124947310532
-0.008123460862373796
-0.031258332469741704
0.02322501525863751
-0.03563802942599065
-0.07470236372422705
0.04226844856603901
-0.007292071213155013
-0.050971316042486295
0.005997599478037595
-0.0052923770173281285
-0.0050304871731528256
0.001780631989814976
-0.004989247275051994
-0.017489673936009663
-0.011251823475307898
0.012295082394841568
0.025670001885011128
0.0018023550019980192
-0.00855635150202189
0.06947760434401204
-0.02932814377188996
-0.04896493230739973
0.0461325765754937
0.0068058235501924
-0.029382526279628746
-0.028201868772970495
