# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; Sfygt1
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=8, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.14680239498751396
0.25484132354241995
0.1401743672263857
0.03923675817184871
-0.06564118292810969
0.15238335402474804
-0.1261798678928911
-0.05383760396013291
-0.1538882499659724
-0.11021620291561185
-0.05184563525539243
-0.08564593158746389
-0.10929612937934513
0.11728930670752004
-0.07313236266140571
0.07967195873872963
0.16809996095682456
0.10566482548927043
0.07955256623465258
0.022705461336509067
-0.14007966601994445
0.16289619497755461
0.011293924563184263
-0.03247627934955529
-0.11795817003715733
-0.11053009527541455
-0.06039619485619628
0.0725428896383564
-0.039315132012230254
-0.035681993912931816
0.014400022167153291
-0.03949765631329031
0.12653027490811725
0.10311935291659734
-0.1222841650073774
-0.051734919399096975
-0.002471710124441178
0.05745366588190542
0.08187752147425452
-0.08460014207557348
-0.023413183191244864
-0.007898968843160548
0.11687018097892377
0.13614103633258012
-0.007621927343156629
0.15344009363432373
-0.0006593845149206329
-0.07844683729019591
-0.0805671908271364
-0.123210181672556
-0.07604012209212407
0.07673739371565699
0.14759974002693216
-0.029074882843638555
0.07033925998038151
-0.013493308583439948
-0.06986597438446482
0.07184341636447421
-0.06383917200210626
-0.03594507161899519
-0.030961039094957017
-0.06981036293561158
0.06261829994233367
0.0602774305861964
0.008123602040860552
0.07139472473641686
-0.04599636596457309
-0.004599001815704648
-0.08467619094009611
0.015891464889923133
-0.15984516970899132
-0.012382431910648986
0.05704579813038302
0.015084918777381133
0.0452763001703359
0.10074192231992554
-0.11564818685327333
0.017464996096706764
-0.18626481656727578
0.06048877934213931
0.05548062317843771
0.0852326338394044
0.03927408502128728
-0.053928851078689666
-0.07317955873467302
-0.048981351003569415
0.07566328298831083
0.033579326358539885
-0.011709534723397318
0.02855390006036211
0.02684825735470291
-0.06607266407971771
-0.012495581220501708
0.05153881465052914
-0.12020284536524928
0.025949373607834978
0.008639068253527043
-0.049433545583898834
-0.09735354317388786
-0.031663572210834164
