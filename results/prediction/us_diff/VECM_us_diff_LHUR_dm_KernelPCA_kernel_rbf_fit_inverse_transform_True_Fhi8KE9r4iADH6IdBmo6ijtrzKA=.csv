# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; LHUR
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=12, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.07505520866122024
-0.1039990228483425
-0.09748363539598559
0.08671967011909228
0.029336834600404763
-0.040039216385838995
-0.014371182171077766
-0.02258201208375714
-0.00999601549432668
-0.06638365151119321
-0.019587664479970023
0.035134777110280194
0.012278497873614295
0.006281703454816996
-0.022679001353682456
-0.03576376258578751
-0.08287223027368634
0.015960708809485684
-0.05356456194652383
-0.06545040561276676
-0.032651138009345826
-0.09419659393965893
0.014112415526131208
0.025311728492190322
0.03764592642016197
-0.005808745690460027
0.007290694856266981
0.012054071555349207
0.036057610914480585
0.09853004138144879
0.12456238492569015
0.003973061313613177
-0.06395377439448363
-0.025023596375783136
0.02585381481558334
0.027783503583801154
0.030468929154619868
-0.0053426368334031134
-0.030757310209106235
0.04327167031411855
-0.06097149062041385
-0.006793199330656315
-0.05427325251869477
-0.03070060844285922
-0.035190419625563865
-0.02202495494273348
-0.036514475261176293
0.008865257285563203
0.019358213434178522
0.016018963962237433
0.03200345404860072
-0.03310641071766746
-0.060208258613599214
-0.04413388696762997
-0.03735855562452459
0.009393279656575753
-0.03898190117973763
-0.024787813817378164
-0.012732928505806297
-0.011384074691976304
0.01799184104074257
0.01011028460839913
-0.02638533424922914
-0.0027274606273922485
-0.01994798430503282
-0.04273726460241374
0.027619827445446804
0.004299158065315394
0.034918770071171105
-0.01373427753842141
0.05816332968829174
0.027941323591557883
0.07655441966608385
0.06970326956710081
-0.059295146008528116
0.010681258906261357
-0.014340723749683607
0.024529296204156798
0.055795983568234356
-0.005587891731207885
-0.006303547989502986
-0.048857609302714496
-0.025067546744366648
0.015276106237341859
-0.027051334810190855
0.022375220257719056
-0.008381898376231503
-0.020393821558894187
-0.026461331371956866
-0.009206028127502845
-0.04731755283437787
0.007047451616499351
0.021678036925274997
-0.029922644952057945
-0.02226152347485382
0.008559714156224621
0.013641162262740132
0.05348153577097024
0.04911069474793338
0.06610523610556313
