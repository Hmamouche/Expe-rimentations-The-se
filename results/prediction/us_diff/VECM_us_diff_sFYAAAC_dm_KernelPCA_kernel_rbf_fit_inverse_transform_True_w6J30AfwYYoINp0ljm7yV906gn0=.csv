# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; sFYAAAC
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=14, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.3547917226871443
0.005645500294111039
-0.33229758297240947
0.1336679825663479
-0.009443899791532338
0.007184935398464126
-0.03703119716718302
0.05641955601397319
0.045339724476730375
-0.19639631510923325
0.06135898800301282
0.16986923091406633
0.13721678579897295
-0.015212675230276362
0.05344069817757279
-0.07554629984163483
-0.07891617387429663
0.03925056678492752
0.05459007345456094
-0.1059159516854799
0.05827192429617534
-0.06597005802770908
-0.05613812023772277
0.1052244822341937
0.10241503264920347
-0.07435051190737021
-0.007593043505570954
0.05294750419231113
0.011063764657903593
0.07372582303911027
0.23370117104595226
0.05139962562468729
-0.24875036217117605
-0.05049673554431419
0.03452031368847444
-0.008933465159903295
0.16467952925205703
0.015047612522212834
0.03181627939680352
0.03296355902454926
-0.04954847212543899
-0.016312736878152306
0.04751294299318202
-0.00667960362850761
-0.09750732708109264
-0.05417628125048715
-0.0384473927613955
-0.037656082845719016
0.03558857017890625
0.008328253565497441
0.06640397268896935
-0.0005655504937645169
-0.05518625106179207
0.00956177114212652
-0.0634757328804188
0.01834088169426418
-0.013024106166217332
-0.03615157441487251
0.029471203173285537
0.08843754417899347
0.02600283791537115
0.11487270799777866
-0.02182261366401715
0.04304379972967268
0.004882058761156089
-0.04851097991357155
0.032845513797075496
0.053181731449975594
0.09052673114545254
0.018273674166980253
0.04687913867392316
-0.014129676047110667
0.051471419158764645
0.05541227759192174
-0.017554891600106495
-0.027813934281594968
0.042241303242948995
0.12738346201691209
0.04626801580908202
-0.08203955770820179
-0.11038270439614753
-0.06580669196280317
-0.10243210406242452
-0.05411328736801389
0.03193897777096031
-0.007825305626785004
0.001896982731623434
-0.06426372441883739
-0.06243454000042768
-0.02189890108530687
-0.039413399706985375
0.007190754886861349
-0.03545691015039526
0.006971229889100511
0.0013409034176098816
-0.03173502350296781
-0.07905032361839492
0.09141970401512019
0.11489859632342603
0.07868287133755603
