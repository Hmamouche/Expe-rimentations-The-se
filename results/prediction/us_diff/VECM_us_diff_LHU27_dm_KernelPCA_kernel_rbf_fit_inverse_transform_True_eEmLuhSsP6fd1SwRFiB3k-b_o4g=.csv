# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; LHU27
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=4, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.03591175570405352
-0.12882477293490766
-0.09836322952876371
-0.0450345275523371
-0.021914081442379826
-0.0217149859747545
-0.033992787700376295
-0.012338197598761312
-0.007253319053955463
-0.023726183553397263
-0.04185158987069164
0.007966761602125389
0.011906500773778783
0.002222931089945719
-0.030492971360539762
-0.023699059297082156
-0.03968237782018991
-0.033608541350134795
-0.03374320197819197
-0.008947752699544509
-0.03326263405195031
-0.01609370579672693
-0.03234186554547108
0.004707564227307611
0.011812549149478171
0.017815804790777004
-0.003444488058764091
0.007287462896619154
0.03336861432522811
0.056633551512516506
0.06239150068864848
0.046819514041227805
0.013259936099599155
0.045294109570210794
0.08765406186615955
0.08762692763573429
0.04323335996937436
0.018375465926401713
-0.044329795097960985
-0.028782140405608954
0.033213344539660974
0.009922487184610158
-0.04514439780996772
-0.05804041980789535
-0.04580401852377394
-0.030382585517215576
-0.06180861418890166
0.019420593145194294
-0.014834046175693458
-0.004567219627461851
0.004374984345322377
0.04031686519523492
-0.032421282839498716
-0.04933442016134416
-0.028808369646220463
-0.010341361073689808
-0.018966386977589803
-0.04824594408919636
-0.02091047006872125
-0.01814998742321533
0.009243144939170929
0.011225353662219098
-0.030749079114333392
-0.01837874149255527
-0.007883955465034705
-0.025126736341956776
-0.04017271835634037
-0.001775578312388057
0.024846025123287964
0.001138746005717289
0.03686234652672333
0.03911288303697061
0.0625671315726163
0.09479699534814268
0.06012087077294689
0.04741241030806715
0.012092682162909272
0.04741717015204893
0.035901631974378616
0.06859480333526051
0.02265727605310327
-0.027540218108064757
-0.032173913377959114
-0.024137177720123155
-0.04388016100228664
-0.014112198449769893
-0.004056975836735576
-0.029148739176452627
-0.040882381130000256
-0.013762226985603677
-0.035491553168899216
-0.013358070251957465
0.011358936833809504
-0.031120376793691812
-0.020576889266800763
0.010978443514663344
0.03306556146790391
0.02129102462161731
0.021827752512487312
0.04508146142958112
