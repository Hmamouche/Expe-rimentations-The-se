# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FYBAAC
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=11, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.0812899117207384
-0.024640322844735205
0.029068449372511636
0.022296607150617785
0.011541198811581312
-0.0741396632417513
-0.014852990914244194
-0.041432170234050866
-0.0064674980898287646
-0.018601573832499323
-0.05121024390051701
-0.03443733870290455
-0.03248012723798853
0.016911057069976483
-0.04150228216280599
0.06343522391567702
0.023775261645398296
0.02500132504727233
0.035505882595975244
0.008654624128916912
-0.009306615067974203
0.02074749820634369
0.013877405979970347
-0.09290882158060856
-0.05611397655211563
0.004386131108328549
-0.022408114829123776
0.008637334674099391
-0.0296844279911409
0.0318333472582244
-0.020654872510538713
0.0077499381258115535
-0.014554922038470475
0.02193353712184179
-0.03326166958763426
0.0020344352442263746
-0.05645257224604299
0.009133780203266897
-0.015091967502000442
-0.0137000201856013
0.005679196663850781
0.011361715540068262
0.018607815873316006
0.03298355248215172
0.027359884115739374
0.06411122107611236
-0.02625214731404789
-0.036539397407119395
-0.03634350188051623
-0.015836371797658177
-0.03638007872379857
0.03026817663237511
0.018168212665045036
0.01493145059885342
-0.023550930356159405
0.01669441925057044
0.013625187928126212
-0.019230456989560614
-0.032351520840993644
-0.02570444891235695
-0.010520083636072557
-0.040187858464127646
0.0355955016208544
0.0006765842626247215
0.0066269744933833506
0.05727530091636841
0.004652740026628004
-0.0032831391531252608
-0.024085725617283962
0.009947367948333069
-0.021893405261788786
0.02366935225296863
0.02065494752261026
-0.028841745469838892
0.038355041945919335
0.0026753045223258647
-0.05793677125103308
0.013002028533618148
-0.04924307886530835
0.01644258391916811
-0.015437863857089187
-0.018967344035485077
0.02254617061316303
-0.008718878990956382
-0.03199066660510421
0.015891601890364825
-0.03860632260986199
-0.015668490356771363
0.005123061780115831
0.026591189848751258
0.014258952807157664
0.03214704592656205
-0.007014581433616197
-0.01097296960296893
0.04737065619411265
-0.022647740938564087
0.02668764041938983
-0.02910884041778458
-0.03271512789382904
0.011134777222750452
