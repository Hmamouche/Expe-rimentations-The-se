# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; HSFR
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=5, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.06736721095547474
-0.0777487174464967
0.04672239480508291
-0.08992803183800124
0.007989478980270004
0.12248621851184777
0.05041535042264825
0.03364599105691978
0.020713332983619062
-0.0541411767620922
0.08553360793472113
0.07364266418920089
-0.038202147766385544
-0.0031539630205874043
-0.03635544114691247
-0.1325165873466003
-0.07968890237284236
-0.03348144689231727
0.022019842476610162
-0.1072078850012496
0.01946880829476824
-0.026883856814508878
-0.022790067939171772
0.003644619691979718
0.040016119314935934
0.02666733037783075
-0.03337474115276566
-0.05607733423751978
-0.07182226684525884
-0.054018792124594595
0.06932737301354897
0.06516304638745862
0.03197042962523698
-0.032053271190517124
-0.012598387711790775
-0.011955978900300663
-0.011712847884630454
0.023167837680066333
-0.00486770363287572
-0.06168094294527413
0.022887052613200655
0.058546167815503855
-0.011015455509966889
-0.022427619670702607
0.02596894210311903
-0.0774047228263482
0.019647978808471085
0.021186516248918496
0.028733307135592488
0.054656531514479384
0.002295327685989281
0.005449861984019059
0.04815482278938325
-0.004198071242997151
-0.010473267779540763
-0.030740244131539737
0.020858122688938933
-0.028260718055974424
0.04857194207400446
-0.026852164594672632
0.012964729781518993
0.07183212016867549
0.006862929969694656
0.023185636079092433
-0.02479192909408397
-0.02728694472619217
-0.04056875821939787
-0.038024202287864343
0.037846279460700294
-0.00665093423299821
0.03568205182921534
0.04673499943427841
0.061097499469477
0.058053793991725125
0.008958278885920896
0.010158916330065285
-0.07294890297261235
0.012534924825805843
-0.04944393704559983
0.04809126359992292
0.0346316994294074
0.02948447047709872
0.03095597900849195
-0.0743125218327911
0.014938427766782812
-0.0031507170521032175
-0.0254158550924532
0.04805732434024498
0.02530561431374537
-0.019560626561892957
0.03610097595756008
-0.030232580918066153
-0.06196039346261691
0.02590328480143439
-0.0510710954151497
-0.07475213889374199
-0.03506477320235683
-0.11837850594579381
-0.03730625805635418
-0.08368750477930255
