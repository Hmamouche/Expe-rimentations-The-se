# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FSDXP
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=15, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.01768785598180655
0.2591394227417906
0.05127897072345462
0.16619817821437227
-0.05702519651888444
0.022485687179038802
-0.045824603696962986
0.10963433093315957
-0.05615441774266945
-0.20042275758967984
-0.20531899246469626
-0.15001380405774636
-0.042911092898782956
0.011811088635301586
0.02477616273880705
-0.04009580766936396
0.02629967180224507
0.08473996375161101
-0.00938584047075753
0.03003745585603229
0.01770364689330279
0.06666922875883236
-0.033518813874581686
0.030355554743979638
-0.019220423701453698
-0.0383678984904525
-0.017357366931748566
0.0285907934598848
-0.05467718897282576
-0.05435971786092667
-0.12129630833605955
0.0355165860557885
0.035754906614722554
0.0005366524320127783
-0.07085349192836139
-0.02458598194870404
-0.051714338033477185
0.0007576608580780152
-0.026749598653898192
0.01327657032335297
0.015474379868968565
0.016346832396924638
0.06594751271002555
0.045766749411183576
-0.041650748410865863
0.03573503640558898
0.03186195162587105
-0.027075972677854398
-0.07913635392331385
-0.05022348267038484
-0.030264750587002495
0.04854227265774269
-0.0278236965306817
0.015355156906996511
-0.08561599137775144
0.016374378726146867
-0.03993350720161886
-0.04900529272245338
-0.01847598912647041
-0.06630016697285594
-0.01475769421180343
-0.048099135207543164
-0.035747379953353024
0.0776526205860194
-0.04249374438681759
0.005526756571071486
-0.08804478249424319
0.0037233955059952856
-0.00012889400462053925
-0.06595982586892835
0.003359035421446766
-0.020541638668763997
0.0552867272220564
-0.08253084782702616
-0.055336702565322554
-0.02554907828145483
-0.045358817609798124
0.039173166741720854
0.04578221819711629
0.054990475217505386
0.010324825359012774
0.03865100210133206
0.06222920573474057
0.01940807610809179
0.04377322507835397
-0.033617352539024856
-0.040635466802406574
-0.0006712857163316613
0.030548991335068658
0.023481260185759564
0.011220538657269633
0.00820318014490677
-0.04376683162604125
0.007550381276358404
0.00968325126490671
-0.0061870030263338534
-0.025963033753718116
0.022964778813864682
0.009743687836343176
0.0007108057165113729
