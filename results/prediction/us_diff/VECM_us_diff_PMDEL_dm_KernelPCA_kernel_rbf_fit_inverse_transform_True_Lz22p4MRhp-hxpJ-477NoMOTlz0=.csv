# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; PMDEL
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=11, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.08028465951805905
0.16634090832338239
0.10388176221476897
-0.0933908909249861
-0.08461803114667979
0.0110799812853354
-0.027366516248122807
0.007405698993670237
-0.12987574924543677
0.167220641987316
0.049692616337756235
-0.021483091637108657
-0.06840262469140909
0.010903418316715607
-0.03375831452087097
0.0410517038930891
0.03845481313041854
-0.024662580545527092
-0.025010724311975062
0.02734847999721062
0.08693399114591718
0.01354886870611326
-0.058234679911238436
-0.05389706672041083
0.02480277559936296
-0.0888752274689008
0.028529304481514122
0.037191844164462445
-0.14961266569649123
0.033984280442775454
-0.03310472357605544
0.11569575524153962
0.06993237973835383
0.04065703628921787
0.05682945464658374
-0.04457062483335542
-0.014552839884088736
0.04795467457782136
-0.051062555908107994
-0.1088332151650107
0.0941236770919825
-0.008636174338601301
0.11680892953330463
-0.02004366636610797
0.013701091941481036
0.004262212758549103
-0.09868076960707058
-0.030314701308801345
-0.03134840775139488
-0.039197685094449035
-0.02077164212669359
0.04421504404187243
0.04017480028324951
0.04639362224440924
0.03191170059006441
-0.0012171613591494268
-0.022775602462399375
-0.010209099550703062
0.019522542435165786
-0.058176773664261446
-0.033495609212211064
-0.03664042536519166
0.07529319905902132
-0.04774294137494732
0.044472750401254306
0.04138729968676047
-0.019458076956655634
-0.004941447261887225
-0.06895700812448238
-0.027532921874659583
-0.07931939224688286
0.08825219390765197
0.02183132573745131
0.043148109421553935
0.024570232955330343
-0.005917571893456881
-0.023287706492344906
0.06335490704402422
-0.08705286731394715
0.02497708353068033
0.06728409003027039
0.093759242511552
0.03544108939523445
-0.033312014913375335
-0.05126981015412998
-0.10602707289426194
-0.054539253414548475
0.03879963646266536
0.005374416621931534
0.03442752988784825
0.029898643550273343
-0.04469836178251646
-0.030378561932679984
0.05900774597612672
-0.02606342195066623
-0.016714720770674596
0.04668217975619966
-0.01863194714353423
-0.023954418110634243
-0.03288241681551671
