# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; HSWST
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=3, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.050350279307622425
-0.0026548444914421265
-0.0146108272515732
0.006323033639969486
0.06033400078345326
0.058908612162952664
0.03824116103065241
0.06534826787463745
0.04735580041298476
-0.000866807951069682
0.0872874881116587
0.030686267024505203
0.005149076530507491
-0.11255325900392196
-0.06331393517746896
-0.03742915961221836
-0.08459345176239838
-0.009677844587393677
-0.049661421527069954
-0.044757380682885956
-0.0059118956000257655
0.012417334445806552
0.059567060020118925
0.018095671707552587
0.02458509234815083
0.01645406352335016
-0.030329707730540924
-0.02119753744476916
-0.07876298084238846
-0.093600695056486
-0.0034752419629517257
-0.034900480573041834
0.03363781472055407
0.03652987069121768
0.0244560685358077
-0.02416006449965146
-0.02651604561954636
-0.00790310592442565
-0.05599186632253739
-0.08635629212223349
-0.020707536051723915
0.1163591941694036
0.01088687564593687
0.018247263613777574
0.02378728916713397
-0.061485216750681526
0.018557649035852816
0.038998884400923035
0.060431480016250566
0.026616170997350405
0.010646553693775193
0.009233307166721714
0.02020631153493769
-0.0013603243329065986
-0.00933415681261481
-0.022363881431861753
0.06102039968566638
-0.012003687469195825
0.07403799244638463
-0.008032682386733483
-0.005489412353394528
0.09177920325184613
-0.022905944496664637
-0.024174331118966233
-0.041651395780045244
-0.014970303587174492
-0.010208139668219526
-0.010652304104176218
0.010889515859998377
0.0042076922298347995
0.053193390306092145
0.06876662564787299
0.06085481795394295
0.038205863276932525
-0.08937338736371007
-0.006957522111720751
-0.07878964046959679
-5.986191140326669e-05
-0.03444723681819246
0.06859678668431118
0.08976013291867513
0.006696929527985
0.02503129083590617
0.013322590563121135
-0.049473113176625096
0.050208026928060624
0.0013727161877846112
0.0074834837964514594
-0.0021235311171330304
-0.011888832114090789
0.023149151492551628
-0.06145534799717764
-0.03727206238947635
-0.06900004958223481
-0.0756897186606294
-0.026906925395787824
-0.026287760395494814
-0.08616983838765048
-0.07320264575347293
-0.09916666256480483
