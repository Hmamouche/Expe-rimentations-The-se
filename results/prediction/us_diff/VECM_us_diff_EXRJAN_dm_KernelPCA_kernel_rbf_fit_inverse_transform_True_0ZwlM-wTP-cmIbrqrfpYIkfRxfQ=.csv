# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; EXRJAN
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=13, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.11178398117530325
0.002038117942713096
0.014379522683331768
0.0354537893260764
0.026706214586816688
0.02283560693351215
0.04767986951274919
-0.03705469951502946
0.004680183487225849
-0.10252063003676971
-0.07091481945372781
-0.028751067446729357
-0.049295835197283366
-0.031882604517815945
-0.0278346680562494
-0.038806792566361804
-0.0020732540063938716
-0.007331186508896956
-0.0027851559528440165
0.018132602182355087
0.021593356633718003
-0.044676875237448524
0.04874303128629086
0.01226395833038942
0.04075152548531511
-0.0013823466902922708
-0.007853458718720879
0.05421009822976065
-0.06983764758985739
0.016584075459253854
-0.0021035668855530418
-0.06259341575104796
0.0061830021738234105
-0.015500173720627407
0.036304904907176784
-0.007128587800152231
-0.03758893115784219
0.020648774805861587
-0.050699152350097594
-0.04122707337046586
0.018927021023466974
-0.065350400120897
0.010397624058445043
-0.009514741942074053
-0.007750574187786801
0.029430606750223513
-0.007870739341913655
-0.029877342739136673
0.04197329472380557
-0.0034015498941061912
-0.000509532695755625
-0.008615963191030987
-0.0020196561802336924
0.05722638166947683
0.005445865825156477
0.0131316681039612
0.011712814769646996
0.030857857835232913
0.011180912079788283
0.0005021888257424265
-0.009409525091266836
-0.03675871583600577
0.0020265226606421584
-0.02037585963898276
-0.032475888723020846
0.011431809232826402
-0.00552284890686193
-0.020412094378351137
0.006152754169062708
-0.0029506018459837724
0.0031712064480487944
-0.00231792806078541
-0.043636935163151404
0.027575854894993916
0.007773387001088844
-0.03920012767987992
-0.003010549085411131
0.023275219787890367
-0.0508436659460976
0.012408876979984426
-0.028161968000951415
-0.012806992277724404
0.025401102091506422
0.010106072130802318
0.0175459180725434
-0.0023810564238257934
0.009128363351122099
-0.011913443646809033
-0.008605583119138867
0.04051132418069518
-0.005660321700759809
0.0383942515475153
-0.01531170919835777
0.04600102304918555
-0.022135688457102978
0.011443024287052145
0.02356744517371502
0.030991884754571236
-0.03748791518906869
-0.03080963009889511
