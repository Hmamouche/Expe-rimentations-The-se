# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; LHU680
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=14, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.03418746800204256
-0.17992236298958064
-0.032376520658861735
-0.019801418670204796
0.0011044522383396004
-0.013058835062036929
-0.07421198462715067
0.05590691171420602
0.07061472895772711
0.008845156107257175
-0.06088394202084461
-0.055880900201147984
0.04780829700920092
-0.07005434693966459
0.017917236936824253
-0.006355662508953454
-0.05980425290959574
-0.055223907905946334
-0.025223707429086126
-0.06107682730354395
0.004503886414208588
-0.014460656099860796
-0.013434745624250437
-0.046109201815989646
0.01355081952128278
-0.009904184785489928
-0.008838212018806
0.0023192416720720487
0.07848753691378355
0.033829634199355745
0.08122725720963789
0.08911163081679425
0.015906182924564916
-0.012737513814257792
0.057331513950819736
0.053746727839671764
0.04981465225819725
0.019855480279093733
-0.023038534166450412
-0.0019360117497086982
-0.04747862730051017
0.03720132876433044
0.028601098046974794
-0.018429027943655304
0.03174863818215139
-0.011186845208106564
-0.09826031018770594
0.0439175826738506
-0.05811998874419015
0.04112660190439121
0.004109124024613147
0.042296516792506275
-0.03635652856490337
-0.01952066099816989
-0.037266000731910214
-0.04043719899187362
-0.009000492703701422
-0.03452187334342363
-0.016782753003575346
-0.03685409166774829
0.03385426753032687
-0.01514542718282999
-0.019220286627215862
-0.01631527518252905
-0.06195652571244576
-0.021480996339603743
-0.05049785582281823
-0.03589407309460413
0.006297335097719073
-0.011329870756705973
0.05837308735490232
0.057056826367558905
0.14501929389545948
0.08696445483170083
0.031633057956503625
0.05621826188806875
-0.06031744734150091
0.03911982315099665
0.01231711865934415
0.08973269927155114
0.036633605288745846
0.03267712014322273
0.040404812373273466
-0.028043963608138406
-0.04617798727451521
-0.0014492851548973812
0.0046092031314245805
-0.02876677916733064
-0.010629010767803013
-0.03091279558879709
-0.047533550024438195
-0.03353996338352088
0.003863764576359426
-0.04269927098705438
-0.008540957145686126
0.017274510406779513
0.042901145153673215
0.03523020406227992
0.0061785418725307665
0.018631652104662954
