# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; LHU26
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=14, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.028931101452593564
-0.1955278824863717
-0.012538627953979782
0.07255518589795663
-0.0363783957894689
-0.001716141010870545
-0.10486617444403122
0.040261036285336095
-0.0421369477582932
-0.03238478536066455
0.025909888579899367
0.015840321601755344
0.007391805730650798
-0.028539056336763846
0.022008808869445506
-0.044215029309027445
-0.0722726813249745
0.02692553522281186
-0.016945575357373673
-0.061620192991494946
0.032456609895605545
-0.1172042490445828
0.03164971430244931
-0.011850514442255579
0.010552760008777726
0.02574730142980815
-0.028775818703509228
0.06416330566639514
0.046777429992126786
0.09415171705892615
0.07487835865394243
-0.018588707541563033
-0.07339340175935961
0.06186584627095343
0.06702036183685763
0.04885113394978234
0.03351848616633923
-0.04261546469587304
0.019501424737865588
-0.0017343925210752684
-0.033502228873792866
0.0038942523542376054
-0.05471664151849939
-0.04573530523613055
-0.011887005842092555
-0.03449510235627673
-0.012274773944089531
-0.001279315608566256
-0.001239761145157045
0.08416448767742707
-0.03312028387061288
0.0077324585637316146
-0.04964129662157009
-0.06127281832945729
-0.03159939207846012
0.04030187513751729
-0.08146794658542132
-0.0025287204212175035
0.0024037847982145534
-0.002206630796989297
0.03200293056756867
-0.03886734542953072
-0.0006356887041233201
-0.03942311941252016
-0.01625945483498817
-0.02647031378791101
0.03644769993834087
-0.0035846551742618506
0.0029205666286214356
0.004954798122503042
0.06216224416784211
0.050970173974945285
0.1315099667234035
0.07797664083216739
0.057096880212190324
-0.03846923942033176
-0.027426023468064942
0.08564160321482701
0.06374246283059681
0.014928664874570001
-0.023670610362023656
-0.05343427972737377
-0.03655843473373338
-0.016806733054258702
0.007316445874732808
0.0008988147605158529
-0.006725457299357435
-0.042730771926140484
-0.03391657660556164
-0.03688498399819319
-0.004803580293225704
-0.017346229884737788
0.06738493834848865
-0.10050263316424358
0.055786484814699513
-0.03715666841183662
0.03962190929592345
0.07682878326903397
-0.01780512733322698
0.07961692537762217
