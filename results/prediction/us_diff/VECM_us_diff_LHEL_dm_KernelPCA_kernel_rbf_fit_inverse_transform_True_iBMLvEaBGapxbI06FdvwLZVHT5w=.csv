# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; LHEL
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=15, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.047292727262728505
0.08622405877262909
0.01941673984307432
-0.08882398325198826
-0.0020445090090726314
0.08217194892944457
0.02713622369311345
0.0173846370170058
0.07491914446676477
0.016074633493331675
0.04914777348615472
0.008097795828013822
-0.03096197330133944
0.006851926140346062
0.0010270747253896574
0.04419469662001042
0.02505427023007812
0.011563021430177126
-0.027315862291652768
-0.05472366079514065
0.01890760446161753
0.0627129733724229
-0.07992021159362461
-0.04917909894799967
-0.005592986366673942
-0.010239906954145252
-0.024721262809862053
-0.07085704983186189
-0.060020670926381706
-0.09762275274467877
-0.03019261145469463
-0.01174447126810216
-0.05341952006848391
-0.031457419443154236
0.0745711820320659
0.037892449812470495
-0.037097870926959174
0.02126361091377566
-0.006091447645076331
-0.024810377848530622
0.05506130650372701
0.03932151943613441
0.07866327370956831
-0.017194481911834057
0.0274344946285469
-0.0002680829399540948
0.013481966245938538
0.02442587242567282
0.04299981832430579
0.0037560947794263617
-0.04157089877386551
-0.044195567618425496
0.0015580014788665452
-0.028553979216793465
0.07773195802329409
0.018860835253790113
0.01809935933804819
-0.005028380063749661
0.03666722900644896
0.002791248959725775
-0.03257109540076124
-0.045864921206102104
0.03644644095105483
-0.06315219470166222
-0.0030145378441590945
0.012524844420192617
-0.03976533346697988
-0.016188990385818815
-0.036838538732770834
-0.02811399514785882
-0.06536642574672971
-0.042596295907905146
-0.10985433811514567
-0.08109492268522267
-0.0336963474682502
0.022648997205941303
-0.03115685291033367
-0.03885190069006244
-0.049941049468019375
-0.014755827035155166
0.03323840382799158
0.06277424283176573
-0.04185847550320006
-0.04392159386402579
-0.01908303370777663
-0.010243862631069234
-0.0033001796508145068
0.011791721591399955
-0.004913199357270186
-0.013870519493022175
0.01043303965608364
-0.061787127490233386
-0.01722362174117443
0.010669867549859413
-0.010705200744164265
-0.016847569386429694
-0.0002700614523728287
-0.07826463578921675
-0.051778301573500614
-0.03246745636462651
