# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; PMNO
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=14, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.17901739907730366
0.08074405425817359
-0.09444182065315859
-0.39695380411008624
0.05953266359763443
-0.14127370046163393
0.061098697551511275
0.1104490284516959
0.00475683278654862
0.4671616331544479
0.3590007161392198
-0.11245331179938124
-0.025586104425491883
-0.024261805603823672
-0.012832079181104246
0.11605573577118347
-0.10634662552606841
-0.3453152282341281
-0.07349281180078782
-0.009647505589479716
0.03302616279685868
0.1768494795075339
0.09150294896409328
-0.1254600693607204
0.09841706149569629
-0.15588528869364293
0.03618158680824575
0.048134547766564825
-0.15183055801160297
0.20414648301789562
0.1570455280089024
0.12452739138005614
-0.07173869001772512
0.03508901796541897
0.06131501149372923
0.012284207745955456
-0.1364618380538381
0.1788985540625034
-0.1371269890899321
-0.058611684916828256
0.14856054539987298
-0.048134326045081596
0.03402605808500314
-0.11925048890715007
-0.0057566227871796265
-0.08096262629618846
-0.05782190950334164
0.13667693835073064
0.03431877280074383
0.12572809287807848
-0.15263027534434015
-0.0418282509584296
0.05959951366233486
0.011722896913203551
0.037351836615032906
0.021565998261433413
0.08296777131702161
-0.09498849178977031
0.04775420671339007
-0.008607043224401631
-0.0474474820220406
-0.07012264048329409
0.11872642908891798
-0.08194458414286304
0.05142973366521478
0.11323556740505307
-0.027907514984919266
-0.07691005544725525
-0.018330992271172734
-0.025066209983926946
-0.033465086650533625
0.173730836377092
-0.012925635901642682
0.29370472581415186
0.10368360572463746
0.08243863917254246
-0.06487779384502909
-0.18672515875432508
-0.09396768794178785
0.1669228668006772
0.07483900436036062
0.014035054958185961
-0.045589303974507896
-0.07362255358156189
-0.09695582493910221
-0.0663553571266736
-0.015592723078584985
0.10694015022217254
-0.08336255132930057
-0.09061874813144367
0.09563891679866235
-0.04323234113067213
-0.11321410046012076
0.12117497664695352
-0.04675146213344812
-0.0497202819060157
0.1057603693042753
-0.168285949300719
-0.10190079432354288
0.09826124809818759
