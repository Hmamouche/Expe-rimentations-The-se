# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; CES003
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=11, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.02969996043538519
0.08772663869105082
0.013509778821898386
-0.030593946313526438
-0.020027469514881022
-0.016683779025305146
0.03018362205849514
0.008135633356063262
0.022219180180492893
0.0011260208053552132
0.03218549880127007
-0.017045166681615525
-0.01891992142919346
-0.01155126191809758
0.010611524017204606
0.043036465905169394
0.02825206125608727
-0.0035196726711359994
0.016832969321456694
0.015372823634618817
0.013774635993910454
0.05626955641076243
-0.020206601683482688
-0.01474036197669413
-0.043834699130340944
0.010608880442694087
0.009422981299162747
-0.025591050361656913
-0.035937110495820705
-0.07533172442420669
-0.07547952457161178
-0.007336430021015594
0.022489022948840948
-0.022762673377288992
-0.01571556721930384
0.002172644261374406
-0.030092185538958247
0.02281942664844368
0.021026633481989056
-0.030698718635456645
0.0376029405633503
0.034789573870048954
0.0436603256295913
0.024869713485888043
0.03969445798351229
0.04123486436607703
0.012625355679693837
-0.005407972510664124
0.0030625656098440253
-0.029255649345664932
-0.00461705651511333
0.025705176597743365
0.0010933848107469
0.044808192403156956
0.0331656288306013
0.0028389546495397773
0.03846513546658532
0.016623387092500212
0.009769255601144233
0.02285992423380924
-0.027166211239103608
0.002789816838086349
0.018026104617706908
-0.004191775208329245
0.04010291495936226
0.016908084549069367
0.001363723232942397
-0.01577159396786497
-0.005358831153427833
-0.0035496522543136243
-0.033166703796272276
-0.047833091910944266
-0.0933401702440248
-0.0810061032228103
0.027257214836053083
-0.03459934249246509
-0.015662765962908207
-0.05060292217268343
-0.047041577749050906
-0.019844998976485992
-0.008142090424758117
0.008470193137777658
0.015588818270806331
0.018469015918965458
-0.005945468783621283
-0.005839027244013358
-0.0035117401478333646
0.006997944118675599
0.02604106439019645
0.01308703479749031
0.037885915507431915
0.00950987844467986
-0.024011715990033673
-0.008676346949264384
0.0188544891551006
-0.014557496210425864
-0.034103171372373654
-0.04520408939192005
-0.036112603084484704
-0.038491408300473345
