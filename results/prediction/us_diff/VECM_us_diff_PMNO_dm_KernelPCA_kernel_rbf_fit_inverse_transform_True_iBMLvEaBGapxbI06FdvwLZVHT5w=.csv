# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; PMNO
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=15, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.3595687715620648
0.3018135558986401
0.18845161770844354
-0.4565479862210069
-0.004265341940349471
-0.0992933548869458
0.021253267122087166
0.06852935505834233
-0.055603419862209874
0.43131814495621995
0.39773123199841487
-0.01630576159057634
0.003599642631376583
-0.15515810328685165
0.050960284226790456
0.13815834404876334
-0.09844729643023212
-0.28955475570117895
-0.07644519096976726
0.030103607630540312
-0.016298333704106258
0.14570771461681184
-0.04125944428155784
-0.058728779924312136
0.04300544314354384
-0.11033739462704913
0.021989463680191564
0.07981063443857658
-0.14282623134241876
0.18251655790501758
0.11770350570820617
0.024657180398978268
-0.11358748151651023
-0.01823606270484741
0.08875328889834302
0.1195683048507168
-0.14728469266323213
0.13016758658296307
-0.11831827737145029
-0.1315021002836017
0.138213154653496
-0.022333831396764946
0.004679135939972941
-0.14228638770013213
-0.01859010981595408
-0.0393866141996075
-0.06738075839763916
0.13123831506050984
0.061020389738355794
0.08535229455706558
-0.1572627097284677
0.022948589879430548
0.020484464166923042
-0.025256929288958667
0.07645517218210653
0.024110228335287302
0.0413204381591558
-0.04504593745416031
0.0889251443328872
-0.04117508059462954
-0.02118391903945644
-0.1144071330444983
0.1331700683036385
-0.05671738611616842
0.08293051604777349
0.03471949772488486
-0.09445871325736713
0.020941096402672305
-0.024602452023443078
-0.0024753085073359604
-0.06282810817387095
0.1827206369830419
0.08435438039469222
0.2136631051267372
0.03916208415007878
0.08300811686306166
-0.05528858455991909
-0.14892932090124988
-0.07209831485558524
0.16915818944683045
0.10131306145488682
-0.048828092767441954
-0.039113450486205355
-0.046939786359126566
-0.1274322735733832
-0.05082575005910765
-0.023269668219792684
0.08948612108070166
-0.08627938240370785
-0.07854461522300049
0.06143909113228607
-0.05822655324844464
-0.04065889513508592
0.0869354900139766
-0.004065860670248729
-0.11279932269820439
0.1635823461031758
-0.08484334366043894
-0.12777865769353036
0.12394769879202022
