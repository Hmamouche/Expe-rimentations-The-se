# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; LHU680
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=13, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.044814840201717075
-0.13383550526043722
-0.079141807233827
-0.03974375845941034
-0.011574914224766697
0.0029781033557064046
-0.07095403357299364
0.031864877616600874
0.08893825773231784
-0.0021791499208120026
-0.03424387396948916
-0.03459961790929804
0.043059732149902025
-0.07866984823868856
0.009676340643172892
-0.014189485531235144
-0.06730638550877731
-0.051097912005012444
0.0018806031447919716
-0.050745288025087314
0.0008693889232228623
-0.014034004830418428
-0.014761863751863146
-0.055022883082307586
0.016618860320345825
-0.01294110811494148
0.002224999393366161
0.001702718723478519
0.05505866800730427
0.03915783185033904
0.08606394802417494
0.07783990173135721
0.02176947282661311
-0.012612778532876748
0.06305416985591893
0.04718275610140091
0.05163424709091252
0.03210431239818301
-0.025865008902717833
0.00976731711691129
-0.037432462229694326
0.024721705755135312
0.028434073691354683
-0.015711636793620344
0.023354229346200964
-0.008076756839219192
-0.09699869715039638
0.03747501299810335
-0.047912779307080856
0.03116867036984324
0.010264727983077303
0.03247187593438027
-0.03305245585621968
-0.026161129838807824
-0.035334192908392736
-0.035641962243037166
-0.0067168003216979575
-0.03890552841741051
-0.012755168778272541
-0.03920651681296426
0.03829034816078747
-0.011285052251559525
-0.022101857076515838
-0.015072170308256168
-0.06516284851110052
-0.02484648640219852
-0.047379097309399563
-0.02695521934946437
0.0030394718209888626
-0.011194215672234827
0.0658960406303192
0.053681249781146426
0.13974236795930697
0.09176211199494003
0.02825382664332361
0.060984015317362555
-0.06677737898486193
0.03786983577236256
0.018777283340116906
0.0898672201635147
0.051939630033782005
0.030684381806142243
0.0441003275583986
-0.029259546558648063
-0.04410560863388416
-0.0012032256645640883
0.0072533851460820364
-0.023596679497788146
-0.01773156637037074
-0.01951647726786512
-0.049960040412509975
-0.028481051767363735
0.00285145769344372
-0.04058459079475121
-0.02779920928801311
0.026179470177885373
0.03803321618176227
0.025281656610391612
0.019959701793066498
0.01993808928407681
