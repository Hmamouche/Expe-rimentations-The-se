# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FSDXP
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=10, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.034093831252374245
0.12934023184892254
0.05439373423893368
0.14985594972490918
-0.05494467253245543
-0.007462274268944774
-0.09347620881933034
0.020593208209196776
-0.1095687928822456
-0.09520475627332031
-0.10696000012798139
-0.06042901252062153
-0.033396652097049936
-0.015387356822230577
0.02635121330857552
-0.036172045776517496
0.04544031756456478
0.06032221530105697
0.028418647153598257
0.020295732820749826
-0.02381763213990157
0.028507335523715146
0.004521551738640453
0.034766835651542524
-0.004905910452360593
-0.02633218677043698
-0.010739946289108807
0.017032713049070603
0.004595091109692164
-0.019522012815141723
-0.08159249459561742
0.08509878086202956
-0.008337113154058579
-0.04111733848249638
-0.0193560321897978
-0.023762945713615348
-0.07411562468529824
0.05430772266932166
0.038163476002241736
-0.020310934465365426
0.007526579577077687
-0.03209085094304819
0.03382114993153538
0.0149593972943278
-0.027616357433810836
0.0024112241142958107
0.00039752977864587584
-0.036721690555820294
-0.10388659791295823
-0.023838634005738064
-0.0061849987491200825
0.06748304423439726
0.002826150100366088
0.007548502485324246
-0.057459551314242616
0.03303146404335409
-0.033055029614897105
-0.043747398599836396
-0.01540447690197951
-0.03871404764167698
0.015282704196866437
-0.06100430208202884
0.02209617230367376
0.04644426024706185
-0.03505556340563046
-0.001337612595267287
-0.03148119940597403
0.01676215953187484
-0.005470196665209944
-0.027403669302760395
-0.01604243462695586
-0.027703454296121116
0.0458012971220056
-0.05760115696049255
0.005991163249524893
0.0023047617831410024
-0.03129650540390827
0.06116292361230053
0.050148379468976566
0.048069541630772816
-0.04045088642956122
0.0018460553622049024
0.023429904905429944
0.035843241276439404
0.04640315569785147
-0.050205815810858395
-0.032183927036683935
0.007805921816224654
0.02450431867015629
0.015020204090715228
-0.007955816250510535
0.009512588335752841
-0.04780484082092604
-0.001298621841186294
0.014320908656724491
-0.03828665541986099
-0.01450812002988086
0.008804856878795668
-0.020021253900814707
0.01764533734944224
