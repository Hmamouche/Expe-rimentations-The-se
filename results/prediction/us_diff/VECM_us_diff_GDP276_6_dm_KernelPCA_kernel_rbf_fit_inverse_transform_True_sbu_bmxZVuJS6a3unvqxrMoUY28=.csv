# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; GDP276_6
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=1, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.007201060496004607
0.007411919616444827
0.007823710931344256
0.007503213186856104
0.006267665464983143
0.005762777910821181
0.0056901651426258144
0.004731959373769683
0.004568415580181213
0.004089601396413076
0.0038670464364600017
0.003647215397334506
0.003374811313551754
0.00410179513447051
0.0031641204162343468
0.005701928679234263
0.006330127917963595
0.007074328727150009
0.008853037205118305
0.009971310165274099
0.01047986264115312
0.010533174242619268
0.011068595475675508
0.009778906075893457
0.010403563236378933
0.009966005829968736
0.010217159482590898
0.010012232716346787
0.010087162871885276
0.009604512391898619
0.00785542463365637
0.008445608495705236
0.007588478103889925
0.008068641400568592
0.00850112721279299
0.008487452481794968
0.00843884638663475
0.007985988047180302
0.00794092967379985
0.0076793853019019856
0.007019591427558572
0.00649791036914205
0.007196090327418258
0.006507370379988289
0.007423044600565034
0.007597102301481264
0.007533721384102989
0.00559957507304025
0.004969408695388101
0.0049524313005054
0.003656761512776253
0.005629024164924623
0.004614393856163447
0.00521103994075691
0.004714574056365128
0.0048068006273721845
0.004575379077466504
0.0036215649641765935
0.004723724323025881
0.005632122823780302
0.004877089279555431
0.004687682728735674
0.004495077129609466
0.004124585623246019
0.005232506544210398
0.005639129088997962
0.005296414909874528
0.005971263027220094
0.006969962281525561
0.006569815307542574
0.009252313397660491
0.007826908687354502
0.006946450215695115
0.005289568588581269
0.0038426689563103907
0.004907143307148451
0.0054659164201477164
0.006997824861679895
0.007976837666544766
0.010055957683275059
0.010092145719576248
0.010676952300862472
0.010741464268291132
0.009396586070792476
0.008337198393576767
0.007039608311997295
0.00867280325303205
0.007697023177431031
0.007949723193881793
0.008471057814601737
0.005844939306040672
0.007397456903635795
0.00740064458343417
0.008192312388249159
0.011701677974823943
0.008976576627365007
0.009939141143278887
0.005211295342999372
0.00728032425841498
0.005471510277837989
