# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FYGM6
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=8, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.1752635554608412
0.059662470178282564
0.09955725290420982
-0.021961334673254063
-0.011116531777328829
-0.0660371447758869
-0.0013708690503648202
-0.03904976406185412
-0.03006785134679588
-0.0393611982371909
-0.030148604499958355
-0.025110261211106245
-0.09057624146757014
-0.027287389055688574
-0.03828758721975506
0.022013672363361252
0.03089187269832272
-0.0479249189795184
0.023217374222853523
0.007885014570890105
0.037087195417365895
0.08917536344729986
0.03395828881765217
-0.020591097252655628
-0.034703527295754835
0.032792465926599484
0.005436487293232501
0.007754454685947798
-0.08418921439131127
-0.0002231737510440179
-0.10701512196661297
-0.06759298401001813
0.02161124453773798
0.014279916754990386
0.02671205087827077
-0.02712101875437169
-0.09784524108448918
-0.0536305933429472
0.037333381447239876
-0.042225567682809086
0.08622339908727732
0.028322659145266764
0.005364370679733299
-0.0056730007833150145
0.0317474208268145
0.07568814316951586
0.01780333950829045
0.02649486478645912
0.04346640622503041
-0.006035200558466245
-0.05938615345917368
-0.013500940911904484
-0.02288090809021017
-0.014120045908267615
0.019036345257520255
0.034115368234904055
0.01244984836087254
0.008126668478009271
-0.021457854249502258
-0.03704518856465971
-0.028344776362882542
-0.06495769833197461
0.016029268614476684
-0.05703815026769414
0.014767097707938326
0.06700159725974977
0.03297069779633445
-0.0022395180095538467
-0.03650421203620975
0.02579276979406098
-0.006215575437885796
0.012945576377490836
-0.06936685272913293
-0.1580717491332806
0.040846968720735455
-0.06349450663566078
-0.009842620400144303
-0.02226928081287975
-0.015016067304130607
0.021812408260115098
0.017439658186548274
0.05368314218693129
0.004061741112102354
0.010901673188148366
0.009786177150689763
-0.004160379581820528
0.016370415619007542
0.011175421467183734
0.03318096423906644
0.03907588667765795
0.04807505761079763
0.02290932948157237
0.018333954137680568
-0.023322169206260644
0.03522416955516475
0.03329476026319951
0.009570885033937299
-0.06999009970637193
-0.06531949445770102
-0.01681422996804456
