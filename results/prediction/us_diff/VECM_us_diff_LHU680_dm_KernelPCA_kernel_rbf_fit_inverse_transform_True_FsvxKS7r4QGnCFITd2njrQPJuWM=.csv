# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; LHU680
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=2, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.03741010107215877
-0.09918872196556044
-0.04927736683085393
-0.06402688156856626
-0.02965609719722294
-0.00881750332922537
-0.013179458573981666
-0.00086382329799839
-0.0028015029830106945
-0.011301125621496703
-0.03641816948352838
-0.011580865064680788
0.013407948455112832
-0.003524803541694834
-0.00600020415791828
-0.02794762066924578
-0.05113205470744481
-0.0435741677893334
-0.009004780785529848
-0.03444576132379906
-0.010085726666126518
-0.033991021554761996
-0.022478468054340447
-0.018090598751661986
0.012857384031140236
0.016337151133097087
0.002359716652856367
0.008309080340556704
0.03192944933820407
0.04746795495251612
0.06406375791746464
0.061589169554855744
0.024594203809572297
0.04368851298744708
0.07116030736625431
0.05148978700874803
0.0506398268424777
0.06401593368709611
-0.005482453669045605
0.00400145206669154
0.0003476653474608583
-0.014581444732066937
-0.0060250810475890705
-0.01749744734372166
-0.0283531038231555
-0.04384739536874691
-0.058670534997528896
0.00016698701410795044
-0.02007420619299973
0.010907489219895105
0.020472716033170267
0.030216639534051598
-0.02621729709573656
-0.03463399883366191
-0.032664278517964294
-0.037953978837776695
-0.003590411849598296
-0.04279825081431714
-0.022650662777038165
-0.026210342644363276
0.00048684696845187857
0.010949060186703835
-0.030905839788235277
-0.013512853800678175
-0.036719992937415506
-0.03134196566784793
-0.050123653746032525
-0.014408754638486659
0.012349765064897496
0.005902907255683869
0.05389846605023373
0.03479098699327146
0.07587726222322352
0.08395811544749217
0.06583823825223917
0.038662213725326355
0.0487566838681848
0.05809089213404165
0.02842921710108675
0.07217892945899536
0.02695189646749224
-0.0038489861646216258
-0.0140897297033395
-0.010287528275038631
-0.047739524615999104
-0.006715644715212533
-0.0027474392801910223
-0.0080525653292722
-0.034808418978126726
-0.029062023332269257
-0.04226307680797641
-0.024404010613684458
-0.007952060934315493
-0.017631001381025514
0.01668722399352035
0.007030989659819131
0.027557246211993004
0.02450284631853615
0.02866218059964798
0.04097400777516371
