# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; HSNE
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=4, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.128105357974454
-0.1080163688766975
-0.16100087030924531
-0.1074926567847968
0.014812267464799567
0.054561913241135386
0.1451974359111645
0.0806513818656638
0.08210061006256529
0.04577457066533449
0.041517474300967736
0.11731332346926773
0.009431190819983448
0.04866968539321076
0.045882982855061774
-0.04277915560767115
-0.02797316859014177
-0.07842004138458206
-0.04017130587056229
-0.012508300284756899
0.030627413915216322
-0.035530132025084114
-0.09338750710084714
-0.03984138494882422
0.00030597104726496835
-0.009615146249459156
-0.07622017655722975
-0.01217374629262089
-0.04618606752133467
-0.05989833491703585
0.046992225686403204
-0.021296025593894884
0.019417588682564768
-0.04296592234858576
-0.007080632895771263
0.02611429360959326
-0.007222980922530254
-0.024504569689067376
-0.005052456077134362
-0.03474288834160494
-0.007506614179529327
0.04087124082308741
-0.011860636187256517
-0.0516818146273538
0.019488864495049905
-0.046209650509019096
0.022038011821126312
0.02278883046960596
0.005979692496613049
-0.003930782281740363
-0.03747770558949935
-0.013092496340644653
0.05536350908784852
0.020759412667765872
-0.0005297302760774135
0.023126486752823635
-0.007242318192595804
-0.03168337269308305
-0.020412227151509664
0.015266033288695083
0.03371869030868386
0.010941317073884873
0.001709997506068834
0.013856294695484859
0.025325100410534505
0.04005526499271617
-0.05162588670450709
-0.026153147595945496
-0.013943417290855464
-0.02254557020430837
0.07523751600084486
0.043282359770854
0.021789556647043927
0.029890157013084024
0.024659225240974553
0.043224521324048165
-0.06287274837498669
0.03078060472726936
-0.05374537871580286
-0.00186763026218921
-0.013824747478769904
0.0460906499881044
0.002282016548147552
-0.06148722936981514
-0.01992589411047986
-0.006229402485612823
-0.010299795265145677
0.0426948479915223
0.024636048482982273
0.03597851673028522
0.014026676507982413
-0.045981546979575606
0.008448793007821477
-0.020942583900849916
-0.027409929809317675
-0.03744552865460898
0.032805424590198806
-0.03541223848183518
-0.013779614022442318
-0.054793888603204555
