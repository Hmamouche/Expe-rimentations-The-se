# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; HSMW
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=8, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.163358348456259
0.09856358755026318
-0.02228662882743701
-0.07811975469563069
-0.047328879359953384
0.14015437467544123
0.04051375995608091
-0.02206235410537663
0.004735714026331224
-0.08823498853933398
0.05454230869904547
0.0421308447872646
-0.023924756261211034
0.040595542968566
-0.041604180819097406
0.053005154731767035
0.0802931905909025
-0.09561646789229072
0.06692390217040622
-0.137950709715694
0.008138804187569156
0.07246266705487331
-0.017328361736978326
-0.011863648626974824
-0.0411175783572285
0.09866238826035124
0.06883572134546453
-0.0844816377397514
-0.09291467894787762
0.015827953391056367
-0.12887477925485818
0.13446341480230606
0.10143556319939324
0.027517326290557656
0.014827368152852072
0.06308793310442512
-0.07303694707307418
-0.0003986592509989152
0.035447393298044624
-0.07499027525975263
0.03908854110036565
0.029781230404705733
0.09186755658026721
-0.15714245236786836
0.10589805288575499
-0.17035315904187753
-0.028488737958410032
-0.09127591978614122
-0.028297470517092995
0.11843155392387272
0.06347378404728932
0.08683871874950086
0.04122476265825997
-0.0464496983959558
0.04800891232079381
-0.06887875432321847
0.0372910234622051
-0.02820169053457615
-0.01837509703998396
0.041958657898581955
0.04182453495826691
0.02204025173804432
0.139964716911513
-0.07413628296062567
-0.02909330567123987
-0.01582401323101558
-0.11114169090492745
-0.028337876887428635
-0.025003913534384
0.06722897728054963
-0.03725480670020136
-0.004610519426273647
0.004813266708862682
0.009881497995426985
0.21378386467702049
0.06231503482150514
-0.17063739071360504
0.0242097021590095
-0.03355374030718655
0.030213567514319854
-0.0639247676002518
0.10545813376095904
0.06441796662227389
-0.08866260876965393
-0.05539158230605053
-0.10135151799250673
-0.03469315888953002
0.06302323911936247
-0.0014605468864361815
-0.00956846068546613
-0.03276614610684765
-0.0878485221142325
-0.10647890240928193
-0.029096051132226484
0.03629139006950894
-0.08109831028902267
-0.06916938015652165
-0.15880768791205388
0.05396221008570041
-0.03123343822754361
