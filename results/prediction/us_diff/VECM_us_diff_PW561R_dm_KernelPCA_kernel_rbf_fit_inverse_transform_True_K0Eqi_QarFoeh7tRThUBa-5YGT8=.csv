# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; PW561R
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=6, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.0041608446183011155
-0.08468127991038256
-0.03712941072536294
0.012600385221942382
-0.028829567471359743
-0.00947311709764599
-0.0067349710442169055
-0.000868826520051073
-0.01841494115273236
0.0013131758974798263
-0.0510062104757316
-0.12210503081275381
0.006451208843789064
-0.03734393754738985
-0.03434935418090855
-0.03377737938533547
-0.004627015085704018
0.007578143797044593
-0.020945041965742484
0.039458399288485595
-0.007313331963386789
-0.037920895205118814
0.02115525638999731
0.019451678836838626
-0.027837751094439422
0.02621129213774251
0.017946863351976913
-0.06195750941432462
0.015290363447920233
0.06462375794880389
-0.03427593311701995
0.06964683176753406
0.020653670272177863
-0.0018061246526084734
-0.08943859212200504
0.05148636176224264
-0.028989199278415846
0.009979768063762846
-0.0013083722394216755
-0.026415671610777375
-0.03704660633203943
0.0058134453531747815
-0.017748608015189283
0.026053551438519795
-0.013930398732391882
0.00450801496953856
0.010355524644436481
0.006515088930322474
-0.01398578749127596
0.01185423907677224
-0.0005420841926810308
0.0072599276123153445
0.013507711380623663
0.032315554254228024
-0.00897573858632734
0.005765034355274382
0.0030433125162762998
-0.009366910373673088
-0.04340571177857431
-0.004965981909683514
-0.036937892545120324
-0.03071059292483801
0.0021562837920072143
0.03819078782570577
-0.0030232140298988757
0.04397814168620883
0.052088974315246374
0.008120394838288969
0.04225432481419264
-0.0005866431340058632
-0.031267594777199884
0.018375186531796397
0.0025630953206767443
-0.059022188623464755
0.020208099094723316
-0.0071527868811436905
-0.023987312669520304
0.0031930964968098392
0.03399861941823631
-0.0187193665492583
0.04358230122108164
0.0029647442081175298
0.03754814048440307
-0.00021266757026710582
0.03556306208054961
0.015032203548406873
0.02793586677528443
0.03967298396869726
0.04856147386483953
0.0032201854734907583
0.07283878027595725
0.028525543802270254
0.011490750901050608
0.002119573601473801
-0.011813972401358553
-0.01876435232018826
0.032728949301628625
0.03746322708747913
0.06479279229473538
0.18817380486085503
