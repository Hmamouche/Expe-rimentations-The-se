# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; sFYBAAC
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=15, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.14763437064202475
-0.07011911704232748
-0.06659680579636242
0.19353523620163732
0.044275687803204775
-0.04294241553626286
-0.007164083877865907
0.009368531096278693
-0.04711500452500603
-0.12347561556064884
-0.08554990480143394
-0.014028491120655049
0.062158032113719855
0.027317390953305876
-0.0006249824492087497
0.04241118896027056
0.035807296934177224
0.0929923106108062
0.06189607506917727
-0.06454276500594609
-0.1510263837615166
-0.053738881313725145
-0.02176147563070982
0.04819918462677576
-0.021667991452764154
0.02174020729353637
-0.02524986263750245
0.006140145625739699
0.1092270398180147
-0.05633871542563384
0.11119137891276933
-0.0634367088942026
-0.10311057799334239
-0.03083663070537236
0.023462044314994573
-0.08252471243873186
0.07869269604435117
0.034579650956895504
-0.015795481261710514
0.022834393314950484
-0.06389311680756579
-0.013339781499549876
0.011925990534581044
0.04984833870823954
-0.08725181680578113
-0.03045740578082077
0.05461812252802255
-0.0761878073827006
0.05150241486901648
-0.04057204062161909
0.04569309640917461
0.029489175015798272
-0.057145119104580214
0.009503116284970064
-0.04497532467375218
0.004504149909736917
0.013889823717072954
-0.015919947874644153
-0.01848008451118436
0.06627197101711764
0.03305262669059196
0.12246406203629101
-0.05306849847555044
-0.05445906230429953
0.0674140687106113
0.014580386462486397
0.025236661547711987
0.012773950069950739
0.029587968336663738
-0.008706174640573043
0.11502718348302192
-0.027209134996526628
0.06290050604644196
0.009727805812231285
-0.017170050759815224
-0.04286636918466942
0.03693764495784011
0.050259255006969795
0.08031258022065699
-0.19259249219407906
0.08139289745975621
-0.1228191426725695
-0.05278523291334698
0.013865611736579029
-0.009703185091874792
-0.009964935287868529
0.00522075045247599
-0.02284462525303841
-0.03725167911798906
-0.010884728205506535
-0.025155183893690237
0.02855883843059566
-0.01587248527547959
-0.014056169204516087
0.022072393105617344
-0.058290212778838335
-0.056196075356675895
0.08982791161070722
0.13022796766527964
0.032844666612572966
