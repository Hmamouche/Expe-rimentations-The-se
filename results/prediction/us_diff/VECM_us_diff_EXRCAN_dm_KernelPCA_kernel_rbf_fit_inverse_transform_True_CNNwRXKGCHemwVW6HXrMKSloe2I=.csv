# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; EXRCAN
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=8, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.013109361793138803
0.0090328568758546
0.03552724615242101
0.04734280014728632
0.03296652638306508
0.02501374238942561
0.009830980665158747
0.021419642368449777
0.005828889796083141
0.016742858498105525
0.02712193141859093
0.0009651458042356006
-0.0161283200240422
0.00545397310502869
-0.0364174556236783
-0.015713726479172692
-0.021700630745527937
-0.004042915722543162
-0.04886783054362919
-0.016527106049424756
-0.03391109786213488
0.005578575431734444
-0.03761449127150696
-0.020862911063581808
-0.030411041605408792
-0.013162305998914442
-0.009551807944446957
-0.021378686677483476
-0.05270879423297853
-0.021496435974260687
-0.0010402126696508456
0.022831383694116428
-0.01560037422563187
0.0015581560670904736
0.029087932608417424
-0.01706044288280701
0.024589665286406465
0.05509391019282455
0.011218264722950413
0.05095998878888603
0.04615431123130975
0.03234711169180935
0.03438626937636289
0.028032724415980897
0.011550940098867648
0.03753736293886031
0.009796046882291394
-0.03202092935441069
0.03986422227775499
-0.03548442555527911
0.0354358902340358
-0.004385386640861702
0.001078551961821668
-0.033288588373546375
-0.005485600610866937
0.00448812613988461
0.015028576651708075
0.037718986559101005
0.011089498723311
0.04081699830721829
0.07389516936381192
0.040958535113548716
0.057889557281981995
-0.03507507433591121
-0.01710015092217978
-0.01198004609538798
-0.018896240532978722
0.009551860393941944
-0.020411579253379285
0.06182739227609737
0.0003421158826930632
0.04182895877021865
0.007495467422291954
0.027108195067506554
0.023737150520069545
-0.005162146231224257
0.006469503998356579
-0.023334979021231637
-0.0027438241199746697
-0.07194840649296946
-0.02587677616589064
-0.14316163315358665
-0.044062984349629325
-0.06403264501236594
-0.04321952246751094
-0.039769096723838146
-0.018249517124842667
-0.03288406466598877
-0.04579606883321709
-0.029990918911727886
-0.050315584864885146
-0.05074722618179344
-0.024704600385888686
-0.02492965930392222
0.01833189718176803
-0.038060643419730436
0.002681292171848834
-0.09297349437167082
-0.013116437982361163
-0.08199619847677839
