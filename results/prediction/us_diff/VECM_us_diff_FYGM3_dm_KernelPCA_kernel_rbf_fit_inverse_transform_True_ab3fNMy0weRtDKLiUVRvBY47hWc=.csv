# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FYGM3
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=9, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.08934804745121028
-0.04234087453636974
0.16727588596020332
-0.0519653783622908
-0.030758110882099423
-0.15805065361597712
-0.02352296592421506
-0.032754375168363374
0.0355414312182033
-0.009780164467990769
-0.020728371989436715
-0.020185791404603366
-0.07232555927253516
0.00010292117527737482
0.0028173229676680296
0.005041180768802121
-0.023046987439281165
-0.07018281950648322
0.07925539847476563
0.003293193983250932
0.05037920766314985
0.09339753899733967
0.06205816259401536
-0.03262888823558216
-0.04905934179811698
0.05359023718796956
0.006722330856693871
-0.008148908350972706
-0.05252347113855011
0.0032782807195328584
-0.14010510965221482
-0.054783605453267575
0.03832888259662759
0.0013325510805372448
0.03266096708366767
-0.029830260257632427
-0.09503969643574434
-0.04594708370908038
0.034591006993301386
-0.035128919608307906
0.07950763363306769
0.028784124170457684
-0.010042467569669013
-0.007971673631182945
0.02107299917582108
0.0884886086587703
0.0007460365411439511
0.04688215539536169
0.04136213365842708
-0.01726796693770112
-0.029846518169023332
-0.038519403193749434
-0.028920561746835792
0.005665256125139101
0.013101531268316605
0.02255146134896755
0.03544924589630794
0.009320946999556424
-0.02495143964807263
-0.029103249758073158
-0.01736721364760717
-0.06061223182315348
0.004125323319036884
-0.04742251483263592
0.014208617670038521
0.06963011667150597
0.04410044288616981
-0.010100910624243953
-0.01710092316490917
0.014687372493405983
-0.018424360893392295
-0.00857484497409092
-0.07944123551099837
-0.13310064197964547
0.041332576759192774
-0.036656070855335586
-0.014314464202294146
-0.02797790293052647
-0.01339893301100734
0.02589949448498011
0.00943436797698281
0.014212815521585107
0.0032698530688662856
0.009358503223566182
0.014473082138085479
0.021388718871243394
0.005547214779522108
-0.013811100155279954
0.023008481927970684
0.031196896525410564
0.030807846796190596
0.031682265928981816
0.02007949248130026
-0.02621781170710132
0.03981422220068948
0.040131290021481004
-0.007278602208515832
-0.0680565241036613
-0.06984988939403788
-0.0035910338238676637
