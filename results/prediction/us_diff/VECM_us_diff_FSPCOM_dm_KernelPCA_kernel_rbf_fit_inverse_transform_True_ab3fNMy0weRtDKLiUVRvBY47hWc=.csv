# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FSPCOM
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=9, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.003166417282455751
-0.007257447331628548
-0.0013008796315028685
-0.008157568970519634
0.00480040019970927
0.003290221170506551
0.007616868895712186
0.007099494273539268
0.005960541975608052
0.006581929309240538
0.011647046585742872
0.011528800633818663
0.0023233932887615416
0.005090094182619287
0.014491608288543902
-0.003488132978410919
0.0033846604518115173
0.014925360597131287
-0.001777905918700506
0.03953803354606519
0.0004975658824147577
-0.031503552468036836
-0.0017786283667168396
0.006395773389900698
0.012139345621714976
0.012403319814474047
0.004729655845153981
0.009372631958692049
0.0032270052364238517
0.001897970775382328
0.013125739253812921
-0.008419943009747195
0.0042546323375064286
0.004419958242192208
0.015113817204759981
0.0011964213639686296
0.0027951068363981503
0.013790284916472335
0.0065526257636953
0.014561034553653603
0.010563754375374993
0.003810505508482675
-0.008460224718486058
-0.007969572915926943
0.00869635153586912
-0.00012090356260306516
-0.0033969652231171
0.015204770500447811
0.02902054578079069
0.02185479635700602
0.022907442368180257
0.009483820079139508
0.008541095115852071
0.021562607403675535
0.02447845319157643
0.027342497721317416
0.04199137579414212
0.043250261527524356
0.052812516890914596
0.03422895112360583
0.058279512315078746
0.03516297132918128
0.007580000694678864
0.030759244958661856
0.03465708452956658
0.050798456296868466
0.04396721134833657
0.03654000565487381
0.03038665535409378
-0.007280056337946664
0.005156334101424852
-0.03521454587117413
-0.055226433551553045
-0.028370849528805683
-0.04057040684959361
-0.026145396202382876
-0.07000372263214111
-0.01062007768253068
-0.0700857048437577
0.0056895977084582255
-0.01940940617552486
0.036666755974843444
0.03558331412356449
0.017052755862868724
0.016188517167067352
0.03416581796866931
0.03451877308035675
0.014862117753318676
0.007815422421702203
0.00875531736105576
0.036071172507072105
-0.006772781452720337
0.014069525745564635
0.046766643039894175
0.011705945769868336
0.05437606505835571
0.017480231202737805
0.03472234164611593
-0.05557407780979959
-0.012733997497945251
