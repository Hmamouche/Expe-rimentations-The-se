# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; GDP273A
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=7, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.005556844128560086
0.005054046539239092
0.006356654003537289
0.007670125076028499
0.005389749342161406
0.003544453972671497
0.007355270856704654
0.004690013190213685
0.0044210116952368055
0.004986511885440741
0.004252282346966098
0.001410848545225725
0.004357631079598941
0.0017656699948777697
0.005354135036174403
0.00712187350414001
0.007793606763265565
0.007156969297948182
0.0065062412499322115
0.006814306553537889
0.008280566554543019
0.008226880034862969
0.008596381930650324
0.009823402442011008
0.005942721869468027
0.00734661899633136
0.007601722600352209
0.007357830664526052
0.01114336358082457
0.010476635461900818
0.006553762353079929
0.006034699556483673
0.0057913291917895396
0.006384072978053946
0.005225076978996264
0.006210332436254973
0.005741993284455179
0.004767110823952088
0.006110319970889665
0.0029267198399803305
0.004282155769994319
0.0035404375639120807
0.0043191743905052395
0.005871417551540297
0.0071741629622795395
0.004214087505370115
0.006202158689929587
0.0038007315408678187
0.0041947731685026005
0.004759386548567581
0.003703646516010899
0.005554301501331313
0.004258907009010586
0.005278609204084073
0.0052634758326759595
0.0043665529337690865
0.002982945100683132
0.001974638345173247
0.0017144489174996812
0.0014474120319360397
0.0019861720427864567
0.0024837714547089893
0.002912577091733424
0.005705103810786994
0.00463361972161028
0.007305769628821118
0.006387498019789728
0.006112572754216057
0.005994781236766433
0.00436555232338724
0.005807354493952338
0.0049344446092707064
0.005677782538444074
0.0021192177962650336
0.0026869230881643377
0.0043285933552148015
0.0030505438697176613
0.005475715472687763
0.0058885743469314295
0.002541555904918733
0.007150922439438103
0.004165990816023211
0.007367425001891233
0.008544260885007936
0.007244662205559879
0.007872171262199393
0.006166213687962003
0.007458108180367012
0.009254963842952687
0.009633658728541702
0.008354032017937617
0.008274542054669248
0.007628963077478022
0.004106529191744434
0.007373643398016782
0.004497215548841459
0.009344969094373747
0.009418035567212133
0.008332328249382744
0.012224894406836722
