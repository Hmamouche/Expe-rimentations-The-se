# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; CES155
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=11, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.027231711446335728
0.03132324058509156
0.041440518734403825
-0.04173102554631842
-0.16621862961529565
-0.02601979808896111
0.14494377091363533
0.040842395753341525
0.010657712202454445
0.01670532123606162
0.09107006218242322
0.07056858726362424
0.040229117879604044
-0.05175618985299452
0.018955628339713232
0.05831658224102346
-0.03886021792002888
0.030179428460437245
-0.011565411929303084
0.04730582684752625
-0.017827431497594706
0.06889782233048297
-0.026274768458711135
-0.0521439701482034
-0.024255907384568818
0.01840779806045075
-0.009052473650925543
0.0406784781386848
0.020170831473724157
-0.03385091734002587
-0.07382072859924131
0.04110404045423075
0.13967070334528847
0.03639063671576591
0.0014202728296435346
0.04289888783076951
-0.011787996881731334
0.033447270554288186
0.022985910657742456
-0.03328357245402908
0.08159898022717518
0.048253686593506095
0.0878951668062683
0.04760044083395331
0.041322922791890135
0.00614827318452167
-0.025120282049637097
0.01336416972872807
0.008042858317869866
-0.03499982024556146
-0.036793677026054905
0.011670274903443011
0.015687542391659055
0.033259289282734875
0.04208999946326501
0.035019388495850186
0.05708817958012405
0.0018784886333915486
-0.007012604194713244
0.020940061788635847
-0.09396391631558684
-0.0027727575067514106
-0.02053198744284497
-0.03218888607847385
0.016502843843573648
0.019922574079339922
-0.03241139528789225
-0.060789925882627285
-0.05276455816200802
-0.027545932820820188
-0.08554245443245305
-0.01210679719323151
-0.024726929016065186
-0.010293538532862564
0.05032936361823312
-0.00822405428937515
-0.043769315953603956
-0.026865605145508006
0.0001314115368257554
0.011155597485821213
0.08195307707141833
0.036551465177977166
0.011373907002599035
0.01154541946595323
-0.03198401188600648
-0.04214309997026741
-0.0023568555571184934
-0.02034435481453641
0.051718154028007536
0.007238643583528084
0.012122400113882936
0.02530294993055008
-0.05533926234950726
-0.007290016651317091
0.021841814884702188
-0.027105454711459243
-0.05485678918661811
-0.02411841907705268
-0.07778933182818859
-0.03419246243335678
