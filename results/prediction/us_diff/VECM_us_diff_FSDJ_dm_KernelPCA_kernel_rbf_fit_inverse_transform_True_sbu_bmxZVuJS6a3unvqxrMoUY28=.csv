# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FSDJ
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=1, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.0002251832295111908
-0.0027075418943131877
-0.002545041196988519
-0.004802820391521731
0.0026862896726025927
0.0006771594394399272
0.002202568169762486
0.0034062765946842387
0.0021717380929169652
0.005096468819897237
0.011179409506869842
0.010650224583999803
0.001673353734752421
0.008314899877933551
0.020781544075828946
0.004798215789661752
0.012109911476398
0.012559514749660999
-0.00023649376610335865
0.029499927391410646
-0.0030471816071629947
-0.02297583600110674
0.004297831550102682
0.008404966719404088
0.010972230898380354
0.010120659553321937
0.007920563327951324
0.012885897192409965
0.00601592994959863
-0.004268603947763204
0.00825351514520611
0.0065834706728225365
0.0007266903248070908
0.004572677818341924
0.009115658213470268
0.005164071191672543
0.0025856010870086545
0.004448339018353544
0.005272914117251893
0.0032399225836929313
0.0012487387954535113
0.005107496836473953
0.008735811089520332
-0.0005808477288993827
0.0026920441816947844
0.002219303090224133
-0.00011677993570796122
0.013150062382923197
0.01692384201280119
0.020184166178037612
0.031802733783720984
0.022877651971094655
0.01599645668862807
0.02790074764329412
0.026961234681787283
0.02447613873293415
0.047210229525115825
0.03197281478982328
0.034546987339648214
0.027435098205997384
0.043761732627140226
0.026473998663514735
-0.00338815305598608
0.03879541565599333
0.004597988685208611
0.04302634027395979
0.04626278822332444
0.03056444693238991
0.009110747062760311
-0.009044004514097028
0.0034493172722864767
0.0020956182615174727
-0.015321531583090998
-0.008513436129215589
-0.007901902163367627
-0.016950986939192548
-0.039021146358407414
0.007112629167525222
-0.046785659330301044
-0.02118986917611325
-0.03499269180563055
0.022404489387108924
0.03508211838334996
0.017306058585182808
0.020452666653461637
0.019347361547969188
0.01626669884914124
-0.007064126566468052
0.012011324163940934
0.0006884161880909741
0.017559593360454935
0.0040698914217580625
0.01470485320704884
0.040331135866373116
0.022686966677742604
0.04657796874698576
0.02901049959317455
0.035124750736386816
-0.020810833693281895
0.012834842331392792
