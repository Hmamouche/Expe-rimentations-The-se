# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; GDP276_7
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=2, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.0025668379069416773
0.006276157429531965
0.006620938561574437
0.0071868914659039725
0.005905883646760299
0.0058116867667834075
0.004699397784899764
0.006242150117810286
0.004604791941609537
0.004386519431102557
0.006707574713712753
0.0055094422441278245
0.0047934930997529076
0.004597528439741177
0.005838109887175594
0.0054326005959142686
0.0054455776499692955
0.005878286502301973
0.005662657905078552
0.00654520928831526
0.00559783309069724
0.00714058539550024
0.00800588466671529
0.006946585293845713
0.006254503198181709
0.006963116828438344
0.00816744624199868
0.0081191397628262
0.010213816689484869
0.008087764609981698
0.008983835909553635
0.009104528527320192
0.009558234556465095
0.006080820655960626
0.0038252736474607657
0.007280292326066423
0.0070502285993775185
0.005078707265355013
0.005086571825280432
0.0066887613231718995
0.005655796636168465
0.005735183517108507
0.0046733349061215314
0.0029333377692838474
0.00252990389025741
0.004058353811651793
0.00523195774260076
0.005236257555043168
0.005046386885837751
0.005586749639895395
0.005621465842967174
0.0065648205717766936
0.007045889927959431
0.006557915275301557
0.006595673242091822
0.006362773791487989
0.006960876104711632
0.00575185356245949
0.005544119569290682
0.006058287339256132
0.005112527567882954
0.00506403906559353
0.006653691306316508
0.007026558069279415
0.006456916897440885
0.007168987215319244
0.008521106228876368
0.008684420576223087
0.009568485158847855
0.008780081487961127
0.0074727612032700775
0.007677659557448073
0.008686969390258243
0.006004769932787759
0.006279797766868364
0.008643210459555769
0.008111724561935147
0.0078418493377991
0.007874011117812007
0.007578151682588793
0.0054291895013946845
0.005289995062032003
0.0065061352656878
0.006819125420865897
0.007193076879474427
0.007382349154030143
0.007540714293396388
0.00606572282674773
0.007583317858777431
0.009917239056196053
0.007481196382897266
0.007863918658707801
0.010313957050113506
0.005141099259643945
0.0021915130264259274
0.005323547630217885
0.004035212984045453
0.006726792253534356
0.007073706688244771
0.009888996320385104
