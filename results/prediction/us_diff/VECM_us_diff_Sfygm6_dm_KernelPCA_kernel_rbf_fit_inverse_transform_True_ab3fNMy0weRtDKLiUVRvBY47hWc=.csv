# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; Sfygm6
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=9, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.10694574995131853
0.15397427530155716
0.13146622259117996
0.12478023874612104
-0.055412061778405165
0.1415673375461845
-0.12102956729973725
-0.012909221225474068
-0.12121245664169605
-0.11275610107158582
-0.03914843425709383
-0.052005069782245006
0.0029698133482499393
0.13027159839595934
-0.04681729530931217
0.055752652394789416
0.11533499197365943
-0.023567315972727626
-0.03794337550929819
0.1451885503746498
-0.136171916945022
-0.002939507631455413
-0.020561372835312425
0.057428203979870175
-0.08344320345304283
-0.13762556287776578
-0.013316218763402221
0.079596907044301
-7.861203188505757e-05
-0.04056290420271734
0.07864114723091553
-0.09934038814945234
0.07282667828429869
0.10380846885296724
-0.03696864149688433
-0.00419141254238618
-0.09985408695095144
0.04293615010084098
0.16613754519142473
-0.011390177454662167
0.02693208606587298
-0.03430133872886654
0.049892644184000995
0.13289954616613375
-0.013226052060700585
0.08950834503058738
0.028250379054743667
-0.10339988465609319
-0.07704890667303421
-0.07792282452481225
-0.03786631130955517
0.09837593722643377
0.12193407300934658
-0.11198076363511447
0.044893554883680575
-0.005046839722962989
-0.0609538019751345
-0.0034587492667889908
-0.012946732180366715
-0.004384875237522212
-0.016266755749150014
-0.04825954921447197
0.09683750684032016
0.07705824646265827
-0.036132448365423564
0.021109212758616738
-0.04595768776413715
-0.04681029071654886
-0.02343415638663201
0.03820527183637759
0.0036400819696693412
-0.018228243304352843
0.01679661486947916
0.022947763000247874
0.02230558082675045
-0.005183943595327645
-0.2516926176927879
0.04859992305773324
-0.02519971058654142
0.09481009332393382
0.042339579089236495
0.019750500576470993
0.06252023961892669
-0.009715661604748318
-0.03976784928163882
-0.060897012556409204
0.05104299901591802
0.08294482291574648
-0.005334304671504446
0.03169507824787823
0.06376710883329803
-0.016819703686390425
-0.03677657732670629
0.012312125954498808
-0.10805812344183638
0.028051461702896657
-0.011607562780115072
-0.027954967031635854
0.012640988768850552
0.00657251665620661
