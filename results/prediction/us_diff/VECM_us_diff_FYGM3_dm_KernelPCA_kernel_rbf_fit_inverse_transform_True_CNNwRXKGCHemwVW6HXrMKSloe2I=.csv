# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FYGM3
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=8, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.12745051122372492
0.00992176225503101
0.08652258325612011
-0.05415705908141966
-0.03225379831910216
-0.1088849438579206
0.005626966846499493
-0.04138113456670265
0.027207202475402762
-0.004571428862916661
-0.007750041912857171
0.009383011288566931
-0.0740587472274779
-0.031273237933945694
-0.01649061249579472
-0.010500527415732487
-0.0028335785740409747
-0.04248854359876148
0.05867045855616897
-0.015166830602725952
0.06010408854787844
0.09005122216143419
0.03469703199027102
-0.02702025627078257
-0.03484677057869136
0.05326817671567874
0.011789864322955064
-0.01304554569283647
-0.07332022474074026
0.0029257530959902467
-0.11413397916955983
-0.06078512846036947
0.030356926848912003
0.0024107696745032693
0.026886664177046892
-0.030510663639358157
-0.0774472167331193
-0.054079436956128546
0.021825131661385135
-0.02190088366583539
0.07191330221735988
0.034515249889992355
-0.007828385270100357
-0.0175397952675451
0.03546451295012749
0.08263876739329873
0.0069727197994989
0.04880050349621665
0.04614537469190757
-0.008902276239021862
-0.04405508596864252
-0.035743208737664234
-0.031119493826688237
0.0055851999566772104
0.01205225895852673
0.029991397687740225
0.032868204378969125
0.010712402989777044
-0.014706480741806098
-0.03249357387555685
-0.012448376446356572
-0.05455704041983099
0.007392464189022186
-0.06611531715215249
0.021184974455867273
0.062293631981520756
0.038027855401085026
-0.0052024731245890075
-0.017578972677997087
0.008353613734793102
-0.0020847828791836012
-0.008618815451036066
-0.07633946953936031
-0.1517165613203856
0.018701542494071438
-0.042825843671458225
0.010300257399402886
-0.005293314705330027
-0.021675050257627785
0.010450794326054193
0.007073738446227891
0.030499521682386815
-0.008028434536279133
0.013310383115887961
0.00715933789766509
0.003059421419850798
0.004163230080595479
-0.008462231738392724
0.032801886966254704
0.03424920503751133
0.04656139583425094
0.023581400975739422
0.007868982155193374
-0.014809304558051094
0.044536004839124045
0.024057318055064333
0.008688319501181256
-0.06832476825203694
-0.09031065073466564
-0.016450755720892666
