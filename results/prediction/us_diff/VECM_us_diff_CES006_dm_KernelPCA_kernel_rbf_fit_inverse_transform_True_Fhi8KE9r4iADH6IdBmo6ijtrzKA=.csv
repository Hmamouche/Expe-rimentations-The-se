# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; CES006
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=12, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.1457210448808915
-0.01054868606808615
-0.0031388756788883615
-0.10853639065047191
0.035347562413307894
-0.0034549650195227766
0.0035766163410179824
-0.05831212185369213
0.03677297678670925
-0.0461774629252367
0.035831116439751606
-0.04768927094957099
-0.07544164613440532
-0.04155608344201875
-0.03752781190059303
-0.03272576246977694
0.03316375043500825
0.02167101513458708
-0.07083817549148352
0.017093852984054546
-0.06296493909456458
0.05135846888073511
0.003963704315021251
-0.020546322672392436
-0.0022919893966300686
-0.028410676572992934
0.021576453126759418
0.02845146706340832
0.018412997351352715
-0.05895631591864559
0.040214848997043556
-0.0658309683625943
-0.009344185555919544
-0.021187425209672962
-0.05683697683836683
0.009527798976192256
-0.014944440793726801
0.03903371266944788
0.03713100240744072
0.018823188119129596
-0.007848221198245258
-0.02765188064964327
-0.016691050427383947
0.051267942767771385
0.02735128313795766
0.006858183502791914
0.03763496083730728
-0.01072333507429047
-0.013572192386363322
-0.009572830410299594
-0.012224246938190483
0.028874659747066465
-0.0022636165741058735
-0.05700905254822461
0.023390703610930232
-0.006492411482576389
-0.00047402146757886433
-0.015263772286942486
-0.006738176117933542
0.03206524601578705
-0.002664446882499262
-0.01899317108721979
0.0168804220560462
-0.006692573933207246
-0.023465076143540833
-0.013459493891343311
-0.025372504244384343
-0.01872094212220917
-0.0008154395371289015
0.013454261470544675
-0.015246577531326147
-0.0029025885643259603
-0.029012264424407352
-0.06040809584854696
0.07035433784420046
0.029891791878390687
-0.023990092101235844
-0.01913833148713122
0.01927182832832356
0.009168301731784245
0.015436014922527335
-0.006141719421149926
0.024406701057966555
0.015672885194218278
0.012562823856175118
-0.026821442039951127
0.03953526680431756
0.02377543161041621
0.027752719100206043
0.00038939294092679797
0.02375439928078616
0.02540755349071879
0.01827182940691118
0.04878635475086346
-0.021623092194216716
0.02061042487980662
8.633610427096709e-05
-0.0066438469564901684
0.0012346903804289662
0.015820877773311384
