# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; LHU15
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=15, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.00018624667505348635
-0.23799465311524354
-0.004082535895201543
0.06527955892660176
-0.01995616476289877
-0.03316830916131832
-0.08817885750230209
0.011110696048134925
0.017391779625707586
-0.022881069241543348
-0.00296428279520115
0.007190658909492609
0.014586157487770977
-0.019646883042602835
0.0014748772710283953
-0.06184380577316552
-0.09800873535350436
-0.012528899617858084
-0.02137479438382878
-0.030310464259672595
-0.0017676132277635748
-0.08284606514010753
0.00976358244410422
-0.01096047330662542
0.009165845279675556
0.012752040218085705
-0.005176302629508371
0.03584012950275003
0.06083214520346133
0.0811673776089921
0.09271926418472043
0.005033023390994401
-0.02549453434622399
0.0344999709884316
0.0673385669489091
0.0615445069033692
0.06806614354174739
-0.02412603827871678
-0.003562674583037953
0.011256267226282726
-0.049751378303863066
0.014838351554777727
-0.027965710732568705
-0.04248532696761519
-0.02876952940066549
-0.022366759193953614
-0.04766737627461461
-0.024691298938586445
-0.023677970802279565
0.0516645036066062
-0.011999863643616854
0.026310600380284155
-0.04752413313769699
-0.0517612634272882
-0.0430242407402052
0.022285878530442532
-0.0260725178449598
-0.02948477873608034
-0.01737108677101812
-0.021133301513556434
0.03000874868284521
-0.012298639953365024
-0.009551616675505858
-0.042748050331681615
-0.02932884071562191
-0.02783941231814895
0.029258419682806104
-0.027824756497099392
0.014734356342024055
0.02101280168560332
0.06109243165000605
0.016452191732465496
0.11025687977429521
0.10260246435173515
0.07350845564826244
0.019815478512764995
-0.032352464259337785
0.022969654242675323
0.04880580500079637
0.06802517561486038
0.004226148779869588
-0.04586646477942005
-0.03239584332122587
-0.012847196592671112
-0.025303810057764078
0.0005136126024875027
-0.0004371041765893072
-0.04431468185530373
-0.04561030423758387
-0.024031528809760734
-0.011784357744273253
-0.023051685150236832
0.01932149336626491
-0.05094747782900902
0.013452061061281839
-0.02259097251141607
0.039923086760809724
0.04014954812608963
0.028520130555835153
0.04138429557041189
