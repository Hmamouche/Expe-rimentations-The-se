# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; EXRUS
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=8, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.01527223843730427
-0.042328677203391854
-0.008086283574818673
0.07924288906826907
0.06503840732529602
0.036036048505033544
0.09144627279503728
-0.0012533822633647024
-0.04247732826466033
-0.09855348337019337
-0.08422206961729828
-0.01280089282981243
-0.07321602407484537
-0.043756694727525004
-0.08977286339759137
-0.06144149882347265
-0.0010533963280694811
-0.036476530556274304
-0.047818735834132896
0.01356606430542821
0.05026407623795791
-0.032480919132622486
0.021014930666805064
0.06397214438289713
0.019506235354602455
-0.005037453916100436
-0.03923945445889992
0.008440208695506497
-0.122882677559152
-0.08311929189083916
0.010900509865646755
0.02397846940877063
-0.03958887723794579
-0.014266694540104409
0.06757845920897118
-0.014955797488619322
-0.07326430192162359
0.07667860658108748
-0.007129847399104258
-0.007919298383832745
0.05912871027554806
-0.017233193191412956
0.011665313486768907
-0.017932934874667582
-0.04797197650650765
0.01924467543739433
-0.006800485242637544
-0.04387024551627024
0.05587150072231497
-0.015979381554932923
0.042581456806047205
-0.010343146239273647
-0.025275420552537023
0.03740878058908787
0.009108525384488794
0.042053704762100894
0.04356257412147783
0.02768093671417528
0.04710134443285518
0.02538924275659369
0.008733546619017998
-0.017581199746019075
0.011619925518653111
-0.027152862946080842
-0.053106782719350695
0.008075539883885714
0.014704768388533448
0.02660606687744412
0.03463218005680621
0.04152504992626606
-0.011069337301876352
0.04286423799469709
-0.026136742535264986
0.012620734177269128
0.015035486393150492
-0.059638213921049746
-0.018913812336324037
-0.0014619988614305787
-0.06674251221566391
-0.032213937029641336
-0.015911229217064064
-0.07478801618053403
-0.0024866660359252938
-0.019346136056278062
-0.026306250400892284
0.011509225914455933
0.006714895958466902
-0.014157779604584322
-0.012263630267955622
0.011175284706283817
0.014389799240433623
-0.01051702244655282
-0.019314098679648763
0.03992963993407281
-0.04416561086210758
-0.02113823812668973
-0.010637168997184483
-0.02361767053399778
-0.04737977955913145
-0.061289796280364725
