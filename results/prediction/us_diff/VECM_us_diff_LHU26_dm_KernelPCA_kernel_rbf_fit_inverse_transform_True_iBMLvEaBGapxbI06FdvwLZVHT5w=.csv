# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; LHU26
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=15, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.03924532467935996
-0.24368858760492296
-0.03050928697687058
0.1621683158283786
-0.024463808934206282
-0.010834634035631039
-0.117201153986567
0.038342362727673
-0.02046915213721899
-0.01760776200938456
0.04975158223238299
-0.012941440740813798
0.017066698808220846
-0.0049500808081223945
0.00687952319765682
-0.05516435536186451
-0.06433006630821837
0.05614300199160661
-0.010259952590925308
-0.08046522941902581
0.02687413553479489
-0.11524188022127448
0.04609499449730841
-0.02132419704493421
0.008462926227899452
0.021358048488320947
-0.028774903318941628
0.08691547144988152
0.04441134777514743
0.10827584513444813
0.07052316479785749
-0.0410758969717354
-0.08030157914272704
0.08253342282189033
0.08016321133516649
0.03858923571825354
0.04772113967826919
-0.06226517236285177
0.016335232101351512
-0.0010518415625878543
-0.0476548659928798
-0.008947414886600953
-0.04727915717310003
-0.04184335055996296
-0.02067112369553816
-0.04208789895792155
-0.016251792188622438
0.003097339951387598
-0.0014395519896615461
0.09678886745439744
-0.041436227080805355
0.00037964744144586976
-0.0387164131539576
-0.06048058371210429
-0.037661828566614236
0.043532867237364055
-0.07275790266336282
-0.015201460453549299
0.002966483150818684
0.008585684084483309
0.024374980450351635
-0.027446499991809117
-0.016690390597922627
-0.04202004241383635
-0.025751886011338168
0.009536884815011882
0.046682063077594024
-0.014430973352161279
0.00680187854011312
0.000965440351250197
0.06360527781721009
0.05061164492059008
0.1104396887563762
0.10534700854811407
0.05952925647737335
-0.046698409482559586
-0.03192544419694564
0.08458843983991143
0.061126334948350985
0.024657714915496405
-0.03878921239745993
-0.04128017470066094
-0.054137759273908806
-0.014952673136680791
0.01858490718935591
-0.008488076053664674
0.0013957306599560286
-0.04820644708795935
-0.029185880893923904
-0.05671003060153084
0.014332598202935878
-0.010752271310409577
0.053377922416764964
-0.08489088359585872
0.04468385684975558
-0.026754805591418695
0.036317679414205786
0.06632947008912375
-0.01403102942259287
0.07740324809939154
