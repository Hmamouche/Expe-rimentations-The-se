# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; LHU15
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=14, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.04136932134307927
-0.21996766118275204
0.0040893413779050924
0.05127339283255159
-0.023441242847807006
-0.024346141680842835
-0.08238666874224906
0.008796434396110973
0.015998885351997748
-0.023622601942244924
-0.008333271775502577
0.005377665925378902
0.006519058381761124
-0.021923873012468103
0.002781139645040705
-0.05996150409790568
-0.09380600133660778
-0.015548635789805547
-0.020432468323084575
-0.01711813774018687
0.004823552100899407
-0.08647217881616173
0.009932568099699
-0.006749202544403756
0.007635565889251395
0.015322128807963529
-0.008752378130346829
0.028393688192963715
0.05766432804567068
0.0749802564367844
0.0918020276992226
0.021485795022037176
-0.028685193196593153
0.029459618995068042
0.06454382094449065
0.06407122459006737
0.056182908873420395
-0.025428729677147204
-0.009809286858805309
0.009526957669571899
-0.043192578795015785
0.013715657007941933
-0.028656484253320666
-0.04437231669185544
-0.020567295739005426
-0.022032652446786073
-0.05016434548700119
-0.026047041252109104
-0.027248134968859875
0.0508230061132286
-0.00907592587716842
0.03434502694639612
-0.05597472450924902
-0.04843255647800468
-0.03649298346838066
0.014295199122125523
-0.03342471284396559
-0.021659663473081726
-0.014939158096964344
-0.027296867028519328
0.028219126296421523
-0.014103298421745687
-0.0019142845931865052
-0.028612440809395373
-0.0401714579970226
-0.03789682002998874
0.016623613853668433
-0.010417899416014145
0.014387460451477487
0.01888805117088469
0.05561461671936365
0.025350275530066965
0.11496894166456097
0.09085095235432396
0.07162096786614366
0.022099289291236925
-0.030811821110667586
0.02471985934051401
0.04771796142689378
0.06364848618807294
0.0022265448276639586
-0.047480774089331934
-0.029388511665941655
-0.013856366336082011
-0.027822643307895085
0.0046507789559639655
-0.004177216076181499
-0.0463463963302927
-0.04335971197436731
-0.019835921683674193
-0.015904070722649565
-0.027816763397100285
0.029363848801114277
-0.061574527751629464
0.01739082357263538
-0.024710064310132736
0.03735842875329801
0.04931586126088459
0.021081455627536643
0.04740122578746513
