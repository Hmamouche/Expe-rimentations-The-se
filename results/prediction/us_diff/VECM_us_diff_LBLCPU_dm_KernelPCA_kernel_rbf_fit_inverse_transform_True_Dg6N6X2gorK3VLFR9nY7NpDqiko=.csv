# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; LBLCPU
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=3, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.004368634126807871
0.004604827012234077
0.006408372525037965
0.0059711233692595015
0.007586110843360652
0.0039859484262769265
0.0013148667784734268
0.0042041040618527505
0.0035960485922662397
0.003908590328786865
0.003224284656998107
0.002438696406642792
0.005205927735836585
0.00946769242276866
0.0060632023538995695
0.009429804652611737
0.010330389930095022
0.009476440814905826
0.004546820068140098
0.004234308848451734
0.005806268716199164
0.004880709563057788
0.006475374797640068
0.0055593422818043265
0.0013336910149297362
0.002029545091497828
0.00585806714287329
0.010345454408358932
0.011518322384590534
0.011039292246984682
0.008617111256460716
0.004786659406377812
0.004475570599057784
0.007152644221892262
0.003868488407043901
0.0035049014571851347
0.005265450846821423
0.0012352624812388706
0.00034155972465112325
0.0061217254728732785
0.0033804907103286838
0.0013124977872039925
0.0051772982200910134
0.005380035874175859
0.0037899447206340992
0.0020638672574362863
0.0009495783481768074
0.0016987317573193008
-0.00038142933241354024
0.003304233257197065
0.004838604181605507
0.0033090986416776935
0.002477498180813111
0.002455548872271382
0.002914697705550904
0.004154445842622837
0.0011712457424855514
0.0045482733964819395
0.004314448170143673
0.009353564241730393
0.009922114925849176
0.00687449883754156
0.00685702326199569
0.007552615931288997
0.005118507667654593
0.004914700947307651
0.012495987547489092
0.014040631269452153
0.003845900009433075
0.02310332071842048
-0.003049143432724273
0.016327188112276365
0.0003943320173709695
0.007313658907762429
-0.006767414568105258
-0.004823040153352663
0.002830122327075039
-0.0029745692349735188
0.002634746548957582
-0.00027495308901895267
0.0023723996462006952
-0.004153276133876329
0.0056771122152336935
0.002112256736002362
0.003972039846937692
0.004133751208049149
0.006558352001595188
0.005443780218868344
0.007204635932740796
0.003281067301274756
0.00661385456098473
0.013507900091398437
0.00832347086112413
0.005210112180628187
0.007091938754512111
0.01204083025474282
0.015301317357964745
0.01144466103215301
2.9623852921538407e-05
-0.0026235491942908137
