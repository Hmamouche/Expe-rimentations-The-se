# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; HSWST
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=6, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.008160472708139667
-0.07716636825631303
0.04054544481046954
-0.040285575228086346
0.05770076149269204
0.04500184791314375
0.02575970835520623
0.08683785702669702
0.027056746124806436
-0.002379429007386854
0.05331682290264096
0.03629188523999596
0.00789491200461549
-0.09871328581903509
-0.05816002899957248
-0.07740285285907939
-0.07952519330254458
0.00372319601482244
-0.06663191948856326
-0.05408449604035079
-0.011352094180393171
0.03445080767372856
0.09619028280550536
0.0060307055371595
0.018980713396379682
0.04757911952928217
-0.037613359238708205
-0.031013159894379957
-0.07912404289564363
-0.14427766712211904
-0.035382978806728006
0.0005233302895384841
0.03450497124403697
0.10420146151608445
-0.04059396025580572
-0.026009570452729676
-0.045756535701787206
0.03585266579390744
-0.02386609813167052
-0.1379279600162663
0.00025371494516006603
0.07522863350733285
-0.025534817720463045
0.015030429925552612
0.02366460047824604
-0.10356361022912414
0.038126334104007675
0.026838145115612434
0.09127288494416047
0.06372804331038776
-0.00932069600228557
0.016083135622662385
0.005017307649408859
-0.004838558170411376
-0.030878300672770517
-0.014863565181576811
0.051698499121894184
-0.031768679788826686
0.08046360570355805
-0.0025115160315772184
-0.00920159910726498
0.09521790762035236
-0.034509915085009696
-0.035598926468781565
-0.07461259710093994
-0.0011190065173371962
-0.04392729476581901
0.007328773724120321
0.02033909516337605
0.02088937016295831
0.055737348922469385
0.04711724696721123
0.07418251305683327
0.03799646320355718
-0.047225931150168134
-0.02077885962257949
-0.08164008885130138
0.011632979517784872
-0.026016385240499665
0.062225299251756794
0.13824361156517245
-0.031909436165213086
0.0305907185962101
-0.004909541999196857
-0.030725245415391657
0.048033815409736454
-0.006195547672462335
0.037445707283504936
-0.01533588381609561
-0.035212685817314425
0.02166697533011414
-0.027692446128963898
-0.06739735435884064
-0.03819609805652756
-0.04572452795214025
-0.05043285746697715
-0.039784480434370134
-0.07852456748376568
-0.07906529469256003
-0.10102353079255522
