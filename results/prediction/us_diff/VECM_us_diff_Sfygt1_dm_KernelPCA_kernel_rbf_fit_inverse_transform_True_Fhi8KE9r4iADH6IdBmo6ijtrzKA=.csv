# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; Sfygt1
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=12, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.018677239794280653
0.13331554744862112
0.38897992477078136
-0.20078552012085915
-0.10631737956869697
0.18963889876347878
-0.0941610564986834
0.0371480130425005
-0.27968782411670196
-0.03852217482035977
0.12367577180252559
-0.07979097301191566
-0.2534947633460064
0.0374652525673959
0.029539495869158154
0.10847067158207363
0.1807455874586658
0.1672538202845619
0.04072952002428418
-0.06022940602163043
-0.17538416281784933
0.18122112246846347
-0.07517016253791109
-0.020151526679068617
-0.1969069608701806
-0.1015492992922849
-0.07827261090400472
0.22339238400381917
-0.03862049671067598
-0.09999341834804172
0.0808636061298628
-0.12387411009721422
0.1637735710983419
0.08718069718824875
-0.1362068718181199
-0.07655874914060418
8.530748774202013e-06
0.1233531776546096
0.12849120472603526
-0.08005367575884662
0.04228120110184663
-0.0174504904347383
0.06439110844836683
0.10378845002052939
0.004975734553895703
0.19527985887830224
-0.011318200628917918
-0.0442941606951871
-0.0945587048207386
-0.1364813626454569
-0.09502812835599289
0.08697632638452665
0.05981485221725337
-0.01852831427081879
0.04100548848668749
-0.11392757460423757
-0.1134947392941798
0.014295928579244834
-0.02471622828521453
-0.023773362407840976
0.03212642723326408
-0.033788425361581866
0.028651388039004966
0.08554263371780942
-0.037598465797750294
0.04387529376815032
-0.0563904296054519
-0.06411811170781966
-0.010757280719070592
-0.040671587513473335
-0.06403828248640345
0.0423770905402414
0.09989330329729496
0.15287896281768082
0.06183996873444969
0.07559608897851552
-0.16180275985919024
0.09621396858354098
-0.164436056397907
0.10351961823881278
0.05423807082620605
0.003158984832510349
-0.008427119280900161
-0.1792385079701095
-0.11717315175887577
-0.04879604139355799
-0.008016674341190265
0.012147562701757413
0.061663279188360995
-0.03531433835134867
0.031120888789649727
-0.04104715226864369
-0.06887193538189249
-0.04650831306006761
-0.06518161559768594
-0.03301685510297393
-0.0761536388617208
-0.008988387404948547
-0.12076708999197906
0.03471648255365839
