# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; PW561R
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=12, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.012224631842740821
-0.0810804021035051
-0.008292198823806343
0.0023316938871417482
-0.016478545419781315
-0.03525561300997665
0.014405943122972083
-0.005326021332425686
-0.006188693640000537
-0.012664783106471015
-0.050110706403331096
-0.11467820179093237
0.036863361966892265
-0.06645051548562057
-0.011611889827507382
-0.020888919724981546
-0.02013783349192725
0.028161908460810372
0.01562767687427282
0.045554675777627826
-0.02102438185892953
-0.04179414799096708
0.0626743525852436
-0.027233203924553792
0.03768592613988679
0.017307266812365333
0.0017707509539715161
-0.05504706522788587
0.014203383861938854
0.027560884309602886
-0.03032933040952273
0.07849064781491012
0.014186839379055632
0.008821433849928888
-0.12279631866942699
0.07863854881048087
-0.03258263198133451
-0.026755410949196735
0.003984138543215625
-0.02310494780126884
-0.08855512671140053
0.028467398187840966
0.0021599060238504716
0.03613665509474909
-0.046286931311712236
-0.03865825274912241
0.012949752335711419
0.011995369725043723
-0.024249150494146304
-0.0007134593179079548
0.011338141034593761
0.0240407908635298
0.052258985217385104
-0.0002942791349927285
-0.018499021578530162
-0.004610909771555445
0.0020820474719377667
-0.03778768611495069
-0.0399889723827947
0.01662338734428566
-0.03320993915006318
-0.0064450642693431055
0.011236950161389979
0.030051729419159772
0.005453895887453323
0.0024898969014754604
0.05528361951968969
-0.023376592071751974
0.04619049455644171
-0.00410954036476978
-0.04418612755060304
0.04500570105097576
0.03337785564915305
-0.05784705210436156
0.01476347509684027
0.008456751566108355
-0.028571953227159882
-0.032518619649010955
0.06095705130074661
-0.0014880404147961709
0.03874414582776797
-0.034585025228377815
0.03495206243555598
-0.00919583864227214
0.03164539412943229
0.0067894396727422
0.007062240950058606
0.035078781537430166
0.06586848986512306
0.003977798020257841
0.07652999875185885
0.029632756088470727
-0.0053088780984745735
0.0045689509971141695
0.007457329568726261
-0.05323874723054761
0.040937443436468615
0.031387423956422
0.06115130257905052
0.18727128010966013
