# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; EXRCAN
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=2, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.016662780203249464
0.009981287705411832
0.02627158060899608
0.05751664820302302
0.02670065380026291
0.020771144340858735
0.040369212957568235
0.023954515327620024
0.02276233545454711
0.0076216653319243
0.013792270682782732
0.0010026752872901484
0.019137251761084627
-0.010137014243059829
-0.035105881449731126
-0.01787268052901871
-0.03560555699162738
0.004356392747924936
-0.04494383703317963
-0.03369253945410096
-0.044800653434536174
-0.030754107522269425
-0.02719776978950889
-0.015755577881151655
-0.018976643896307804
-0.012823879809378602
0.002138450168613709
-0.021806191066909068
-0.008338994238939572
-0.010982928987422159
-0.01964433727678551
0.0017063245958058005
-0.01641665995296709
-0.010724658997773516
0.04237819457236814
-0.005166203856240607
0.03368466928251213
0.054562481235059426
0.0034112162286096094
0.05183830562464979
0.011557291228606135
0.03298995291815859
0.04010963322050239
0.05547535497968079
0.016409311996985095
0.03760810302233529
0.011154795803412134
-0.009261384960095213
0.02176621394022664
-0.04898884940577643
0.019633840874064203
-0.007933824013650538
0.004210379472572462
-0.009214015131340859
0.010163284777326993
0.0008805688667780447
0.016626988525881556
0.0326566993508901
0.01334131827034767
0.03718593697443212
0.05699009485757154
0.03704864997783407
0.0361718739018065
-0.01888224126891782
0.013380188758807981
-0.02491019083606833
-0.004996493813435045
-0.00011469520501359957
-0.009027072066872959
0.0453889792731864
-0.01354151524182351
0.04690016824855837
0.0022297414667753427
0.04067214264708324
0.01895193197546658
-0.015756854304359226
0.012747841698033897
-0.02373965779765094
-0.027007062705979065
-0.08068143616799361
-0.04535553014684257
-0.12392731321523454
-0.017178968454206377
-0.02518027101912391
-0.028419480483483278
-0.04858934043851923
-0.0373870733264928
-0.049217931889839986
-0.037627197430446166
-0.02631998634068994
-0.03435460596548653
-0.03505744387054932
-0.026705595682848162
-0.025390281528294936
0.011833228457726285
-0.04328832193126821
0.001550889136454266
-0.07812500535343397
-0.020207052674334976
-0.07414450907671088
