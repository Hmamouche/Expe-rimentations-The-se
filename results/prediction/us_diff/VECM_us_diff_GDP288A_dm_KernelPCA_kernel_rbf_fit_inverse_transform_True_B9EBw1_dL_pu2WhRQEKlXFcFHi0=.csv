# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; GDP288A
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=10, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.004787801406156639
0.004002610007061258
0.007024040202130115
0.0054214597916663184
0.005039635786234723
0.004881738212231259
0.0067564643231351595
0.004396027821770777
0.003367673945669736
0.0036665883733089455
0.0018119869550895945
0.0004307654176320937
0.00337792788195727
0.007082379015448678
0.0065735528672278415
0.005053773394316753
0.004870006703227623
0.004853413490616755
0.003401419919979093
0.005405253234961921
0.0030483969762183077
0.004759292199341795
0.004707128075905686
0.0069840949652566296
0.0027495796362190074
0.006645345631492185
0.006882222174437411
0.005142929512756105
0.008074250195788338
0.006512902026153971
0.0046772273177438555
0.005750902255222305
0.005437280983884975
0.002365716495562069
8.787677003628059e-05
0.003921152153348692
0.003054168070171839
0.003151993781786486
0.007330312884422585
0.002471761208051594
0.0026704933706697635
0.0022025896120798256
0.005890371411337785
0.0036446298117337726
0.00667633900072892
0.003949819172075004
0.005049956875647067
0.006051177263756797
0.004820872318254644
0.0017790806158837433
0.005059616640514994
0.0015814625557340168
0.004706021824851703
0.005547339469435063
0.0030156063704980026
0.004066285591549867
0.002423386389736176
0.0035503226863895132
0.0008554575290644268
0.0035583335889735456
0.0030844317317336957
0.004492874103150097
0.0033314150844675158
0.006146811930834143
0.006436804340927587
0.008085438707005873
0.009738807115851197
0.0073814785976369915
0.008637624488557528
0.007658679863342199
0.005883656918495222
0.006919464528215474
0.004793086622102937
0.0022804827113341317
0.0034793338808257446
0.005150675374131379
0.006346136409244174
0.00760327053025951
0.009919519047688237
0.0066734902081814805
0.008917398537704027
0.01003993557851938
0.0034962831070549137
0.008947751753693186
0.011621466245355407
0.013515260893467473
0.010866924266505897
0.014001304135932032
0.016753525793965385
0.016514837462615957
0.014146079333112613
0.015627968428577894
0.008524757312308673
0.009344670854956324
0.012888100184612243
0.009836406978559895
0.014484007988515797
0.016754700355150216
0.014996523952725303
0.018774169248360482
