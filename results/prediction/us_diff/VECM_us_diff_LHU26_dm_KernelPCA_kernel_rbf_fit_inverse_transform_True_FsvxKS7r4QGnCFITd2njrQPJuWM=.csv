# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; LHU26
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=2, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.0677537970552827
-0.07731190650596337
-0.07127941593809774
-0.03525656898496791
0.01346365011172837
0.0015102295871874775
-0.012609060719740313
0.006322109278606874
-0.003893082378628338
-0.036589576433695106
0.006478589623137392
-0.026900739783090558
0.02880235495627562
-0.006208536272739507
-0.004902101421858141
-0.03350933206479663
-0.0678037997747414
-0.017952227887334787
0.019010814294711208
-0.03327185824016772
0.0049551059171581095
-0.043078113191629144
-0.010000324361940137
0.022915310511277986
0.059006552750030694
-0.0010700085119131966
-0.015809126838682894
-0.0025222154216200613
0.05308861293793571
0.05898243026614019
0.08615981726859544
0.04670847121824559
-0.033927838748656666
0.0561537439920158
0.02505817627649031
0.040955225285719404
0.02834375152731109
0.01757020194026484
-0.02812470648527198
0.030206563087047895
-0.04320945666989684
-0.04536906324092322
-0.027128223097613088
-0.09314600739055091
0.002241651447847078
-0.0525286638363534
0.01720145971088706
0.044333629956692176
0.008369392142753168
0.03958005433129358
-0.026173177849555182
-0.030771502997256943
-0.03075095151033063
-0.025265896400380336
-0.005207337307196429
-0.01708295859653311
-0.040737386552611055
-0.008250372694083612
-0.005009901066537302
-0.010548231821983278
0.024962589656463918
-0.025793501465515997
-0.0082955405184744
-0.05789975133969947
-0.005133985432733919
-0.04281719304913864
-0.01245947335970149
0.018630720107358435
0.019826583991291495
0.03954203919367685
0.05796925771650772
0.04304973688582472
0.055169188187858716
0.09085202052356486
0.019135343803049494
0.0010454389109093775
0.04129640680144551
0.055984829774728004
0.04120751252133893
0.01769168389914961
-0.002221214020840257
-0.07326901743206957
-0.032508181438275705
-0.040834743778088226
0.023397301716996513
-0.014525200415363471
-0.015003385669139944
-0.02470873665911152
-0.022983949242740897
-0.021918958132051206
-0.017265007528642726
-0.042007385545537027
0.02674870439487285
0.015070570610933127
0.01795219798170358
-0.012706293977089608
0.01384929116344311
0.04203931400952651
0.038114587130415094
0.07512836042266123
