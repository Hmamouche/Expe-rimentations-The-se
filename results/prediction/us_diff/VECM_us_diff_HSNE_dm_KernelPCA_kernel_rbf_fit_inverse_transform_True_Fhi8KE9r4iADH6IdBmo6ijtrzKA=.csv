# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; HSNE
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=12, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.18416777016416236
0.03449044048339536
0.03011727854568237
-0.17460361246270698
0.20602938008411031
-0.027813614210920757
0.010462345763517927
-0.00953939216345287
-0.03819628826008998
-0.035136871163757286
0.2056281010642389
0.20772304800922797
-0.07780932280907034
-0.004953968535260374
-0.08577131218818289
0.015887272160327072
0.16564330695747448
0.009722106119307697
-0.11969455474226073
0.1433897841213281
0.3113080616719501
-0.017807140147363988
0.035497728646015705
0.006226504814294837
-0.06177157152582052
-0.1089036385761821
-0.06965342518142083
-0.03165021659366621
-0.158268235469209
-0.12398717961885894
0.07051006295471049
0.049932283450825926
0.0864524606749692
-0.06648766920177618
6.540667217289423e-05
0.003119185689285764
-0.012212154843654623
-0.01073464933849749
-0.10064366168856477
-0.09493737554616653
-0.08648260227967525
0.048247957818735386
0.021210617892608554
-0.015025092889946842
-0.045022612702876344
-0.0945680219433192
-0.034724846674221196
0.0005156609112513899
-0.053653166991990575
0.02239606334723903
0.01726569890130754
0.08219903899306204
0.09600518609139909
0.0221240636767199
-0.015643349480957094
-0.018336321450284195
0.022354137518119495
0.017836982810669526
-0.028698341287075966
-0.03562838330610107
0.12222760774631486
0.03330228198041037
-0.00935783898129857
0.026881346249262177
0.005628005253196968
0.014117926316859243
-0.08175338763649669
0.037679841494805286
-0.01819604031706281
-0.05444018644545292
0.09177505775036598
-0.018464141217910318
0.033965852591028065
0.056707774474993984
0.050785433219845966
0.07746606085002639
-0.054901638035605854
-0.03972738363118878
-0.17222446174472525
0.15171682187325414
0.04755180239369518
0.0466276816022038
0.023622517292768482
-0.027258035459300965
-0.05961387773062446
-0.01773668780829403
0.0861943489270487
0.020187242920259668
0.041901350263003616
0.012297156010836197
-0.020316719889489673
-0.012134576901686874
-0.015492145108863755
-0.04132300707817701
-0.05980808910101197
0.004459521785140121
0.006872021145332053
-0.10782303366823254
-0.03050140049163744
-0.10286175819147587
