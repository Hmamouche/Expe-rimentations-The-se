# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; PMNV
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=12, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.288045798761312
0.24130681753179953
-0.2568662491985381
-0.3723608067761449
-0.21941977938008933
0.059478913064534886
0.10354535545716904
0.09428811854945893
-0.08477404446433576
-0.11238654946218721
0.08453535407362807
0.06825908394381361
0.14964214811969412
-0.09305497969356366
0.05377706520028878
-0.03843291311827482
0.058937503045945944
0.10424748194514076
-0.09700273432104889
0.09211168605016543
-0.12156577538401747
0.11131378838648157
-0.22063393809060988
-0.08001543586446343
-0.0021397810234250123
-0.09412631160530718
-0.013854978634038721
0.14311732585850173
0.02393001616755995
-0.08064199263692853
0.014944953830574224
-0.07879807841011074
0.05673080432353881
0.13912527524307042
0.14790335083264397
-0.024249518893920227
0.009686945431284164
-0.06031392429714055
0.006553654198499834
-0.013925393684144173
0.13809281572064186
-0.018931946519311165
0.07915545962954955
0.04022116436247508
0.05820653970214388
-0.011296045845293784
-0.08436705414096748
-0.06671162797101364
-0.11300668109170947
-0.06006744982003394
-0.054383858189529156
0.0011315463897538028
0.09969025657961655
0.07224621535664166
0.07959607089735343
0.023758801022398683
-0.016006158035708602
-0.026054144152705586
-0.0034446651207963135
-0.11512485942928687
-0.026419108851954566
-0.10330807288077624
0.056959747300748736
0.03234007743404356
0.16666794886311245
0.1342993404214281
-0.07394283852679082
-0.02246413355320566
-0.1038323644976835
-0.013215504995202666
-0.026212328356138216
-0.0739178371226728
-0.045949520960412965
-0.10083096495122733
0.1389299735492445
0.024205985469840777
0.041021212737981774
0.0433430028510316
-0.03419614384286747
-0.08084468820546163
-0.035918348681027765
0.12630456600203338
0.12555690499619004
-0.05841078766407706
-0.05293548289648108
-0.152996554357952
-0.060103122590901045
-0.0467554918041588
0.09969099030093428
0.07068675138623877
0.05601632038420602
-0.1192454802508965
-0.054227325195096626
0.0035023944844951305
-0.026349602037755923
0.02206340136574235
0.052171261059578596
-0.042621864178274786
-0.02607105106573058
0.06389704001310523
