# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; GDP265
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=11, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.002102605234455617
0.006805693729847358
-7.303199449602837e-06
0.00016672768903701666
0.0072176183417666274
0.005302647067894544
0.031087486446711575
0.0038957382966524996
0.01964836104398525
0.0047459992196287985
0.016552060639951873
0.012434118338447098
0.01697853295496756
0.00954364663900521
-0.0039036497030316853
0.013442232301532873
0.010816216543073326
0.009100181556518368
0.0138273237280225
-0.007255975181060912
0.01229684798411768
0.0026041375331298934
0.008116861963139854
-0.0012770972471616068
0.02020589097674701
-0.0029845997376841535
0.014335762051846529
0.009261602488967038
-0.0008172695843483849
0.006778544327454774
0.014949252322003954
0.003078394114870312
-0.002472189104604664
0.001322107487698473
0.0018058147237499187
0.004224223329219471
0.00254826719353771
-0.002861697033374406
0.008223373241007786
-0.0018772630327381845
-0.002982195551801862
0.005845774244761351
-0.003838347209464345
0.0018323225801374337
0.004368228871613931
-0.00694848779240393
0.02011837962275761
-0.002655203934013809
0.0019769651120320423
-0.0016184879960169747
0.004660798243088198
0.004271952070400346
0.0013389014708547063
-0.004211463683492334
0.01657344404182076
0.0028617256752538167
0.005925737925494643
0.006422256568834516
0.006385985580284482
0.0005012316507112066
0.005186278379342438
0.0026828724758395147
0.013110148046399737
0.013397563077996812
0.011638709377449917
0.0023021771290326135
0.016582668455109795
0.012006747758150778
0.009395882056709721
0.001985959745353317
0.006469126251246295
0.0035972550128531124
0.009473163252616906
0.015784892600068386
0.017756510900916393
0.015964254574184964
0.017537196813398895
0.0022079455552594203
0.02246271032793254
0.0020330150229159556
0.0037324400567295543
0.006251155503460356
0.015570764546533588
0.011460830849008669
-0.0006736213219839292
-0.003894007097422698
0.009995862485374566
0.0023957767835383844
0.004062081532320086
-0.0035570982300299663
0.008910582818197609
0.0008588740786893017
0.0051857849228181645
0.001342474494999803
0.009785255452755237
0.004886382229017169
0.009427422255874202
0.0144953895996512
0.01849534315305536
0.012405968582144499
