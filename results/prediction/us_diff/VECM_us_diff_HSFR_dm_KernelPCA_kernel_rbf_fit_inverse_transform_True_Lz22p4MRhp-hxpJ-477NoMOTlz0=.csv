# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; HSFR
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=11, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.106484108175389
0.052323365323852
-0.07234829996632378
-0.10945732790181188
-0.06696371934521919
0.07444676305253917
-0.00662236380128337
0.01960545854127586
0.000895453762069124
0.014360339683881194
0.033398776436648944
0.08764283550790261
-0.09835427717235384
-0.007339316540855711
-0.06958195064573804
-0.07086425888703687
0.10328284144906553
-0.06429091155647848
0.006840899604819747
-0.08732254181712135
0.027159147094399507
0.023125163060381662
-0.05260350513390685
0.016129569032718657
-0.04710754839979263
0.05155997084083999
0.0235545414661648
-0.07658244532483183
-0.004963672139930934
-0.06860374878947133
-0.07544515840772484
0.07632924129412061
0.06087638896592042
-0.04569010495178841
-0.0073591425380022685
0.03684874904230499
-0.010446466973951324
-0.035643293431499495
0.049110874131229144
-0.13864156217401163
0.05417087860958729
0.061796902143988676
0.03021869148798167
-0.08613808743130225
0.08964766388109761
-0.13615238017039183
0.014636548799021959
-0.04083129105801525
0.01918118150167915
0.028637620648553748
0.04427848920304446
0.06012307312397945
0.02515239618257954
-0.060374135964765326
0.08338510631796014
-0.060873277564711926
0.04703785490767513
0.0061391376680324065
0.002260980607849298
-0.021734171334990005
0.038396396563746854
0.027286594359102107
0.02224198199386328
0.0006465807430734985
-0.015269244425673397
-0.007891903912569491
-0.05527467465542778
0.025058987623644433
-0.020658759875106507
0.05527990930057472
0.021856708849264044
0.0018211434490690952
0.02075306682073463
-0.013775289679107867
0.10660659740007078
0.043499290144348036
-0.0857028126395805
0.009394721605754316
-0.04484350687448967
-0.007934895724529618
0.0771551831895329
0.01745415621966811
0.015696352671238124
-0.0029668286106493864
0.002082848100965911
-0.0340479653890502
-0.0015325971460452142
0.007154915196503606
0.018700490989446653
-0.008617632277272273
0.007835274318307547
-0.024700926149963227
-0.03862407169378892
-0.029117031315337792
-0.042516785061351564
-0.013767753062216177
-0.08082027721623256
-0.17514318256988035
-0.009184809301681467
-0.08053030067910785
