# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; EXRUK
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=11, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.03051337746756666
-0.003732242612013964
-0.11063415210945493
-0.01721663615204508
-0.12498497568017601
0.07320216389890041
0.06559510362988741
-0.03563034889192964
0.027123794607056297
0.05912031111148132
0.09042004010610086
0.008994330190138055
-0.013066362241645802
0.021475738703531644
0.0554043520669224
0.051805071182271505
0.005002401674205424
0.12927572904614615
-0.018373078552307406
-0.09396966150071295
-0.07359943076366184
-0.06836027768205152
-0.0597234247997642
-0.07462639744534882
-0.0028625607590966797
-0.0131470612215865
0.034310237116500775
-0.026280860212080145
0.0943712350009282
0.07429071177789996
0.03731389383855225
0.006450481623205887
0.019149113628479635
-0.013437922700766693
-0.08912903208974929
-0.011836311566678328
0.10131424110560504
-0.07888268442764078
-0.015291299675936953
-0.018521555386726903
-0.05200801220104776
0.006082220186185889
-0.04913644929703409
0.004422784534797303
0.025860206293660756
-0.04094076300544365
-0.015743417065678392
-0.0019796522893233953
-0.012045432519469362
0.012748435459847895
-0.026587942486404947
0.07209946178338023
0.03133257067228966
-0.007182777687375498
0.009315294875677352
0.007316871812825795
-0.01836496203189549
-0.01204446007501968
-0.020265304405996973
0.007738160103202317
0.04326345365109678
0.04448492488748501
-0.01665171966614213
0.007862386258542764
0.018861659312271635
-0.008830891548470783
-0.03756642203346229
-0.05260373007692735
-0.022266278282267196
0.0029840890007083384
0.006851863347719813
-0.004204828102393908
0.021893120487976755
-0.02837555408172952
-0.027674809631912316
0.008075878977985347
0.0071970994735605295
0.04556594025411888
0.05000595112046601
-0.06285613603210886
-0.0027954212803482613
0.054392419146061986
0.028834840537051282
0.04953597336812619
-0.003576474573822628
0.006185129596705287
-0.02567519753937265
0.01383309105787785
0.03180290131319678
-0.005983226316046211
-0.01454289956930353
-0.031623916796833046
0.011276981580991871
-0.014116956308439645
0.08380064002151058
0.025024015402501244
0.008045985081918137
-0.002989879460320243
-0.008464018169007607
0.019700635372244563
