# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; sFYBAAC
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=14, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.09568117738516421
-0.07342809644694398
-0.08029689058481701
0.21672637657435967
0.042128469454842395
-0.026357699070431152
0.008597308039487148
0.017082851785658833
-0.04546680871720041
-0.14263403657846213
-0.04759706141071567
0.01621885294469061
0.056698810982311804
0.0060720500660588865
0.00885135551828975
-0.005151125920801677
0.01414240936548803
0.10590065951159502
0.06401317692459477
-0.009507417082155724
-0.15990098109840306
-0.08914391499911746
-0.019376700925349336
0.07495677320641225
-0.035772712900695454
0.042302359787632554
-0.03656482834435909
-0.01684138951414211
0.09222494319282529
-0.03743874158622208
0.131547952189558
-0.004211364263740074
-0.11653877683545891
-0.0872761152630486
0.014560120292355462
-0.0553820572420233
0.06713180311326751
0.03886087206437052
-0.015463018839527803
0.0012121942789872497
-0.03467884242107692
-0.015694414594582873
0.019306657305162633
0.03792585766289868
-0.07351950701681177
-0.027969247020425567
0.04968135381748301
-0.08182092030618734
0.04882672675308766
-0.03715648066918571
0.03401672788336588
0.07516685165295388
-0.07999591032580505
0.020864789744230505
-0.022637881835879392
-0.023991589053307352
-0.01787421606530215
0.01577177654836602
-0.009532598902916878
0.05159087416809933
0.015472311723102629
0.12262466603407632
-0.025701600000747286
0.010872471027528474
0.025367327050196512
-0.009632580523582969
-0.025498060893083974
0.06085656012137838
0.04164085232273879
-0.015194318496365038
0.08191457247008538
-0.004806982570946926
0.0893695821063414
-0.00019701122048734407
-0.06760991521363964
-0.052711019454164725
0.042399757192753844
0.07706887223286302
0.10752874142423668
-0.17094984094008828
0.044605755600757996
-0.10129980357641054
-0.08600369034658019
0.048703133874756104
-0.019457100814174413
-0.010145016836485547
0.009572225328590311
-0.05806090894674384
-0.004056170168497006
-0.027279893309428302
-1.6356620469845998e-05
-0.003597928183069695
0.012957086841827455
-0.006480385004463371
-0.0059646349606706575
-0.021019857291860646
-0.06710275718495086
0.11544686323079384
0.08953092480676311
0.06498242204705318
