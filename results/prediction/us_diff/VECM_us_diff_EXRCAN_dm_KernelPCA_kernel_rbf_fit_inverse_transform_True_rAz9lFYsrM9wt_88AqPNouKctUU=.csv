# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; EXRCAN
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=5, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.0043836699038962805
0.008031132274610435
0.03254804662835823
0.05039875050437839
0.027255250119314477
0.02337479247256711
0.026715279364318154
0.019802448712389606
0.009525430546234762
0.00869576274935555
0.03153122247860779
-0.007283630523603649
-0.0040935215955505075
-0.0009339471780459765
-0.02273864023696936
-0.020997923997204418
-0.030591064701380035
0.003955346680036509
-0.03788603027498054
-0.034695816973893706
-0.04146906279331615
-0.021566556783808907
-0.03570335957148712
-0.020159177460302845
-0.021088144535559634
-0.015140122406780619
-0.004402910886011654
-0.02395465625292191
-0.019320799823779045
-0.01700950488755507
-0.001160870628244765
0.01600329293294656
-0.018663634851243054
-0.015556142119177245
0.04838267436430134
-0.009673522951088033
0.030135998718812836
0.05974394941193491
0.0067377686953793425
0.04954925892263363
0.015541107379902884
0.042410654040079536
0.03855381655254428
0.042074656022392944
0.02047166681418817
0.042376592853629505
0.009818510050107705
-0.008114586198230057
0.021339693089304737
-0.058256161888219866
0.019725856329505683
-0.00883922000738214
0.0032022876684101058
-0.007477455554580504
0.008374096952192376
-0.0002038410899982781
0.024902162049461128
0.036142814902537575
0.013718615373481603
0.03539962256867672
0.055799325603565604
0.03841806335744957
0.03895990859965022
-0.020037872633679923
-0.0008874879454436636
-0.017526326533046917
-0.004664408341788889
0.0077361372432886225
-0.013635503159472062
0.04828882645415002
-0.01762023102399525
0.04586916354106283
0.008686046666766603
0.02853495729346625
0.010046216054669113
-0.004873932797266186
0.015115793023422242
-0.02664084977073737
-0.015217601702271905
-0.0875941199803845
-0.026856470148602898
-0.12739861198479632
-0.02369957447408303
-0.03833860604516988
-0.03848105314866562
-0.04218164389587453
-0.03603296852289723
-0.05450112356547465
-0.039690172721902554
-0.030862700659804627
-0.03693066596231073
-0.04427372152351868
-0.024514105194719613
-0.019311175595091365
0.016363862436256323
-0.03556663716786611
0.003318349973806983
-0.09010302730209237
-0.02247555136192219
-0.0735305448887929
