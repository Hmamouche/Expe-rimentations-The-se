# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FSPCOM
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=15, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.010277102442886625
-0.019268743901424343
-0.0057968150328319805
-0.016855005107051936
0.004476858180962263
0.00019601892610385772
0.0032280191328003843
-0.0030257492594908102
0.0036390646201558434
0.014357410474146695
0.022074526453379047
0.01845636456687831
0.0051806185240726235
0.004555210006264015
0.013113915572752804
-0.001971687187835849
0.009516844220621358
0.003961863929726974
0.0014266544481048513
0.058880897149283656
0.025472671777358638
-0.036510742248470035
-0.006188387456595026
-0.0031663435616331188
0.014218879693363752
0.016690942919489367
0.008136858953216086
0.007493034778358817
0.0023867469163317834
0.00037776005458175035
0.018015975406027646
-0.009656554370095998
0.004493922506333718
0.003514177917029398
0.015516079889071803
-0.003621203706691771
0.013524064717640297
0.010012834572686358
-0.0031821786695188734
0.010398817891293825
0.012116461943470217
0.00033527200055794037
-0.004024670799081682
-0.006817520076514436
0.00853628193024834
0.0009351832962229602
-0.002027063195887852
0.010615303461438226
0.02935807372909571
0.022296987618966584
0.018947849516080476
0.004231252958652438
0.020866724444916385
0.013162341443875973
0.026654423287024373
0.0186513757493391
0.04087917966675852
0.046170729444081654
0.05645727013414217
0.03376162232355319
0.05898473273670812
0.02904522556082248
-0.008439478535694139
0.0294300615838761
0.04172797312391923
0.05531946779085803
0.04859274157515579
0.04204545566952689
0.026910503249045672
-0.0027475277202665474
-0.007764249386788245
-0.04746019676788429
-0.05034126432590659
-0.02226467069641291
-0.04068040057170569
-0.01581274419900593
-0.06016888716913767
-0.005628669858316122
-0.0708228565457127
0.0033639460970750594
-0.03566828251239405
0.009996755912233167
0.03413688074071999
0.02198742814969475
0.0033705847809100287
0.03018702875396869
0.05506203053634925
0.010763025418833026
-0.0035396362052901725
0.00430102332942592
0.04205154902743744
-0.011658993236270574
0.015853360127414333
0.05696419475626131
0.004274120986151767
0.04400446516145031
0.013605412655611326
0.047332696122528314
-0.06308968502812073
-0.017845713255842444
