# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; LHU26
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=7, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.03745923600813023
-0.10399793052968757
-0.06191762799518288
0.02541746598960595
-0.014365728833116861
0.010923412531283143
-0.03803627805340245
0.012337069163253441
-0.018568101610826268
0.0074891916975956575
-0.020499379909880697
-0.013124060550186705
0.051584549141598324
0.003919319978581548
-0.0030153930388734304
-0.015527159483423085
-0.06373684615669581
-0.03369623742925862
0.02794651475016476
-0.08131403923022155
-0.005759979634441756
-0.1142068135973176
0.03714297559754502
-0.0032166087216668558
0.05403529988142827
0.0068056051855743946
0.011467210103876694
0.026661249274180697
0.07972114442091852
0.11028151453098181
0.04822480601001998
0.03946386232309298
-0.0945234473793191
0.06912634647609617
0.02701931788371282
0.053828690610512396
0.013272342994590956
0.022867340514410785
-0.047502227138790806
0.016215929411186436
-0.017784507772918377
-0.04220808496187573
-0.034126915443244456
-0.08651344361076319
0.026217029353183346
-0.06787163848689197
0.018801955817433214
-0.0021432572134968894
-0.028418162982346756
0.03400114802753897
-0.015594681434176708
0.016095865275067827
-0.02267249095127855
-0.0357570019657719
-0.021648303756023876
-0.01658891210498358
-0.06328834661229063
-0.030155894445274715
-0.015474292938630405
-0.00809348069812436
0.01923269541356851
-0.03411069520763026
0.026601239061693673
-0.033532086552289474
0.0020165295358825254
-0.04432507566163242
-0.008905918289582239
0.012069021944167665
0.008368306419214369
0.02028611247737372
0.057266476773499886
0.06929283905630507
0.062476692224040516
0.09992863825895856
0.012917168195797716
0.017895149080345073
0.00956637965877262
0.057791709859826464
0.041221724607090546
-0.007795389708829977
-0.009271131048608924
-0.048100590892484446
-0.03375224819112093
-0.012466000741418156
0.0220256017104944
-0.03129399200352559
-8.047607567075038e-05
-0.004271249402538469
-0.019964058949034157
-0.003913871467121469
-0.03679937044926825
-0.017733001115204428
0.043076706685829906
-0.04725194802630317
0.01950303183934349
-0.013155819575901605
0.0061374703433793
0.05956767057656829
0.05362475561266645
0.10451679282275902
