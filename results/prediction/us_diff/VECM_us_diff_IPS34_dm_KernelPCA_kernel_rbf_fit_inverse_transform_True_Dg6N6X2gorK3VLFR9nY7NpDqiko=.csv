# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; IPS34
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=3, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.011695070024545139
0.006514810205092621
0.011769889415975725
0.008212328229259153
-0.0045080801060632765
0.0034294749569421723
0.00695551275770415
0.0014572794000646799
0.0014216525953600905
0.00309708497322471
0.0036257696768207184
0.004219272115447995
-0.004464437048506253
-0.007686834292571541
0.005421533744801172
0.004403641059144009
0.011341772671293208
0.005637246794730332
0.005716361285298684
0.01030395530876031
0.006420180688130622
0.011622270485542225
0.004582389129962474
-0.005084868397669555
-0.006747973388960569
-0.002912081528030119
7.675139628536386e-05
-0.0024251231302644073
-0.004156993689325697
-0.007590430904694161
-0.008534972520376043
0.003510392379509124
0.008736299586991745
-0.00016471570161487184
0.00429697088817669
0.001839019598371979
0.0024684733093500596
0.006155589640270329
0.005002585683291547
0.0025571049454726227
0.0071309782949823975
0.010725953052540437
0.015299697007810231
0.015351601786514507
0.010650310318395582
0.015329639742400545
0.007167129753077363
0.003754704698481694
0.009272228463234502
0.006817008656553462
0.012619604848459844
0.019487267574869287
0.018181503311079022
0.02227002376467889
0.015563992145890004
0.01768971282754146
0.023760518974964846
0.023620016935374872
0.02327308674916151
0.0178806403697677
0.013462556092048921
0.015395659200837158
0.019763854433152123
0.01989733566782181
0.026907162230645522
0.028847170660788374
0.024064521836794757
0.02135322840064132
0.018475832714293716
0.007888018863307486
-0.0072127230388791454
-0.008040105384073688
-0.013982033697055024
-0.020489907009054155
0.0012950767017560385
0.009666145772763535
0.015182337266171388
-0.00023153764495522505
0.004239593426894288
0.003227913850479198
0.015366441589725822
0.021384468989185366
0.014381704087946924
0.010201354702962762
0.007724408610598099
0.012013501316203944
0.01378821498734846
0.005816432282510213
0.015675648008503185
0.019943004326495914
0.01690188430343074
0.007558082214414204
0.001339340584388996
-0.003143669574925839
0.005787463732328912
0.011878474090493902
0.010208342919858488
0.007254689809633147
-0.001639542569678165
-0.002005087084105579
