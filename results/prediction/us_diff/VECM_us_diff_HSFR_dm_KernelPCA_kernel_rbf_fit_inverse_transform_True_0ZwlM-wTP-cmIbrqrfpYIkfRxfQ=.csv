# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; HSFR
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=13, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.08817361572387722
-0.04785309224253413
-0.16076328013231495
-0.10279250229519418
-0.06406912641471642
0.05776175692074686
0.0026821242733451947
0.009919635424651255
-0.016121860193583623
0.01881592419913798
0.12265880081760897
0.14922149559314138
-0.08035457406942748
0.03580777888248404
-0.10929537615666726
-0.08763904867585465
0.09785289423194815
-0.056537921321422706
-0.03467447408470032
-0.10847591049327233
0.06567697241390466
-0.0029477568887331003
-0.04405304720592633
0.04147718532859427
-0.059626286231992016
0.0573361280900758
0.02922718044234605
-0.09871884760654548
-0.056019683000977136
-0.03286019038854302
-0.101255160083552
0.12573787404167014
0.012726989416863274
-0.008643771473300844
0.02455507241620567
0.044369379484872906
-0.015019747624294023
-0.05222956029388155
0.024698659659295095
-0.12005137747172147
0.08547607765524659
0.0525050687917966
0.05376008095249501
-0.11811000131288835
0.1089096286043866
-0.09679216430474538
0.03838480653382005
-0.04158781348660632
0.049109486918684844
0.025889220722874542
-0.0016871014349310598
0.025460168625832273
0.02795087847795928
-0.07115458171199315
0.07803538469125143
-0.054249892548598175
0.03957435142796323
0.012097519348214753
0.017027960271478317
-0.040739798118460686
0.06062752002220564
0.023390193931491388
0.01082063595935448
0.0016282080279377026
-0.01605527231782773
-0.02446260812349792
-0.07531631541121558
0.05638932750916434
-0.037124161992166324
0.05558330476895215
0.025348210577663288
-0.015374425180043772
0.030259002258014193
0.006424212077553804
0.08853077075289337
0.05945333537041617
-0.04790681915542702
0.001302259422991555
-0.0734227230930694
0.0045844007771607895
0.10672418392587521
0.011736468591464058
0.04483185040195052
-0.007422697000517536
-0.014113778395967842
-0.022511954637068027
0.004614464336855514
0.01992133625528817
0.02555448043496508
-0.008032832545394525
-0.006343681477943543
-0.03972938330858475
-0.03895052311252935
-0.018578205915116764
-0.04119039396381957
-0.03371586389071153
-0.06151223170653289
-0.19121506498231922
0.007685654621309938
-0.10050171336397445
