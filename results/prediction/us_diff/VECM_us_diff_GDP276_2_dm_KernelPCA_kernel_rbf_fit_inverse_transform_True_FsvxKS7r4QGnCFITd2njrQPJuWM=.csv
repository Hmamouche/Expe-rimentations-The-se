# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; GDP276_2
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=2, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.006973490862201962
0.0059106682998974515
0.009612863664713636
0.005286744185350585
0.011090610939281152
0.007552930457060626
0.00487805101248266
0.005906446207088147
0.003758229892378117
0.0038761475110994977
0.005605963048148398
0.00297943361009249
0.0013569748573061021
-0.003940688880993818
-0.002762072770556299
0.001052793791518231
0.0011550433605644899
-0.000391200567994769
-0.0002173600681370264
0.004408508379917492
0.00395845765051949
0.005160332871572973
0.004308798500906105
0.0043863025056493305
0.0048613215300621
0.00264320118778724
0.0047021605380684174
0.0032381864229215998
0.0006660514402497331
0.0047623244867920685
0.011866701790411925
0.0037100697991951986
0.006029148222504473
0.00812111086158429
0.0034205826273896937
0.005108325337792657
0.003986674015681071
0.005289226100151195
0.004305937602738602
0.006758166637526104
0.006264859037032902
0.005009487350708711
0.004918028031063077
0.001473146515925927
0.002416659568449894
0.001450512318380591
0.004843857223594538
0.0022593167805529915
0.0023887568852630006
0.0033275222176046694
0.001878607920601861
0.0038402504433838512
0.005090547648498852
0.0038986471741508494
0.006952850964397392
0.0031655495498634917
0.0014684187795732206
0.0017377712684070333
-0.0030112654177337356
-0.0006912319723931825
-0.002463553568206657
-0.0011431213397284238
-0.0013713785441255206
-0.0008700974106231019
-0.00035006846655257715
0.001350846694117804
0.0026049081088847523
0.0033720367495779376
0.005597553829070324
0.009041835173617549
0.021869533590907844
0.013261310270652697
0.007147688617625787
-0.00084512209821796
0.0031593985906180917
0.0004859634904908498
0.0035267985916737164
0.002168106037662333
0.013124660843813049
0.01102838748310954
0.0025993724551125865
0.0005359363595483474
0.007891256881645346
0.005911624071676714
0.005780357278166694
0.006416629180362939
0.008542078185265022
0.012426403828321787
0.013461990986599223
0.030462151010938073
0.021719599466490724
0.0020299472011391434
0.01546577633287102
0.014394492670232663
0.009844811893325627
0.005392158312555098
0.004111604231121699
0.0066260413329121
0.012130077745316062
0.026143149921564034
