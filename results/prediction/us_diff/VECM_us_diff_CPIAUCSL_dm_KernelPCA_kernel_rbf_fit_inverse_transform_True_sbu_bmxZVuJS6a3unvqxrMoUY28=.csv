# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; CPIAUCSL
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=1, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.003258213181628191
0.007625350819668494
0.008764890492560067
0.005067949805010827
0.005685848411200817
0.004373367353350791
0.004512014942872113
0.004633068096049678
0.0037871417303975466
0.00637057423276453
0.0021041460476629685
-0.0010270788046021007
0.00444550889913805
-0.0004779475754316852
0.008095134871371595
0.0056773802885274425
0.007353841777109505
0.006015874445316522
0.005035663040139696
0.007734232216948802
0.006438516697514255
0.007307131671495253
0.007271686871589526
0.009311260576566269
0.005263968289534094
0.008299044905847548
0.009315206292219207
0.006159072927195276
0.01204027342151859
0.00900936846374901
0.008540799351791202
0.0069763636855476735
0.004881110725924041
0.00551499621652678
0.006027408968790518
0.006237654317502949
0.005006297771872038
0.006912562154005421
0.005706421619971048
0.005721239270411164
0.0041633472228161305
0.0069982336467532755
0.0036476184482452066
0.006142229098932103
0.006003715672590188
0.005363023324634963
0.0063402821882909645
0.004855521699371753
0.005254526303243529
0.004583999509448665
0.0056713107203833005
0.006949313487525354
0.005702288077117623
0.0074370662472472565
0.004961565700199116
0.0046390914385116035
0.004464017094328067
0.0035036323839474784
0.002942258858663402
0.0032846923759847395
0.0029096940991553888
0.0038050727627463466
0.003964601300191762
0.00575240289102947
0.0054054177781108735
0.007539591764760336
0.008142655214213704
0.006548666153632281
0.008881381172318625
0.005903994462921719
0.008383982183415075
0.006059453061049511
0.00482176675476527
0.00019367741629093842
0.0034663983526533465
0.00442537646335135
0.004370885947794396
0.006645789617154926
0.00810664376221237
0.0005286254109757081
0.009053021190255077
0.001937973443107833
0.007782278754088893
0.006886245200095998
0.007593825959325665
0.008998867209340792
0.006999075413466741
0.007404173017010779
0.010240282291221117
0.010937537374002002
0.009997186846440322
0.00932308269025588
0.007858311824755538
0.0029035472178608602
0.007824220369947111
0.004043939797783669
0.00893915363923774
0.011977560004995428
0.009434596978327785
0.014729701478752324
