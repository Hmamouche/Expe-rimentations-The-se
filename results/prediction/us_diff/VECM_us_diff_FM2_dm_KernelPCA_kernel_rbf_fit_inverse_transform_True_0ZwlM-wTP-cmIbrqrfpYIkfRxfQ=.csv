# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FM2
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=13, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.013123206221315237
0.0029878691645196464
0.005143756221754276
0.0035831610330227834
0.006957351433948878
0.006150629378107441
0.005279490548565312
0.0071393488788253955
0.005965565634299938
0.008017761997160805
0.006343006598206975
0.008378674820204919
0.006122716824894325
0.006522113280779786
0.0026786777658904365
0.004579467886009976
0.006436490691588543
0.004863301897134204
0.005997419719896413
0.004069587911915392
0.005448315839374685
0.006609530201253272
0.0038178719185747553
0.0058876046754267495
0.006589215797568831
0.004816706367508916
0.003639574614304659
0.003960015787472407
0.004641441599248805
0.0036452699801335763
0.007127744930197711
0.005762547358395426
0.004455903662987235
0.004769469171595335
0.003329997520663662
0.0015469349218337975
0.0032509306972431156
0.0010481229995151996
-0.0005866146391069358
0.001987748697925978
-0.00012110905125605262
0.0022006435060250773
0.0015798277318613211
0.00018182334717576596
0.0016825222726551182
-0.0003258199226460317
0.0019383626699992032
0.004344377370927822
0.00484725260465427
0.0043654119484346785
0.004673621597314437
0.0025470569015341063
0.006383793101399739
0.005659039417372202
0.004382860347744108
0.007015446087308469
0.007625021157880367
0.007284997857960248
0.009706744974477552
0.008129778414981454
0.010166105498244052
0.01224392343753048
0.009044442545379538
0.008487159324388302
0.00780653268597024
0.008613263461661174
0.00822023486102638
0.012692687431419354
0.007245444108725572
0.009216075901761671
0.014682987562839318
0.011912675827533789
0.01485761365691481
0.01257966605869857
0.012023852210206866
0.010341388897838262
0.015230421077227154
0.01409246236345852
0.012471108983925366
0.013575913636609448
0.010588025874130444
0.004890095579521861
0.014276508239607629
0.00947106076153423
0.009570855933187966
0.012062705762594536
0.002239228434090515
0.012738624082536947
0.009227084503416222
0.01165905361049014
0.010208095542716348
0.005191633403182796
0.011481986997175831
0.012539224536350448
0.010273416561723994
0.013618274067079762
0.012998383787609657
0.013831033520224547
0.020587487436911855
0.011702416128962893
