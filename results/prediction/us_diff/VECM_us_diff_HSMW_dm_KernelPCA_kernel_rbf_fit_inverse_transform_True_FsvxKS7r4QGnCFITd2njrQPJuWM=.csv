# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; HSMW
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=2, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.07803015185046963
0.054756612726163174
-0.06329469446953925
0.011049284238362806
-0.025338855315860692
-0.023111277882007262
0.1286276060569518
-0.009086956040636512
-0.002693580756876493
-0.02033504928007148
0.001934148007324013
0.06369132125915847
0.03809554373230322
0.006199675462824191
0.004461884525651744
0.09784714351533623
0.006490066679000104
-0.012163667219159237
-0.032938096523099376
-0.13681410314556972
0.022344187892275415
-0.03608625069330429
0.0007565266496819944
0.017733011085457436
-0.014416882574377041
0.04206370305090663
0.04860936345374124
0.019824645466636058
-0.03218498690435928
-0.03525359568595875
0.003552926484140433
0.04139758103934166
0.05075267485040248
-0.03395078922968178
-0.03328944550712583
0.08016904729902119
-0.03099860824845835
0.004783380500353703
0.0050558191698116955
-0.07019586249154036
-0.01907069955977434
0.033758238708892344
0.09333393155510386
0.002772715573559773
0.0245267693375437
-0.030774807187741556
-0.030883255076118714
-0.057114557930746
-0.014001727307569658
0.05021240224130113
-0.000867441334831116
0.08168292404373663
0.037372837623039876
-0.005577791908220553
3.973471223925589e-05
-0.018720026661731518
-0.0006179082846390557
-0.03898572874683927
-0.008053833168037499
0.0280131186298394
0.018797870007296935
0.010810616500094766
0.06550261364142973
0.002462742643433406
0.008338596767602409
-0.0011010227468631513
-0.002654585701079175
-0.013617523701682073
-0.04568510660496242
-0.020107521583099808
-0.029725881614019976
0.08538365859592842
0.09277489573959935
0.0253984381958832
0.029127426587272128
0.02375522587019576
-0.12281757028502754
-0.055946298560873575
0.011575795768435697
-0.02176719052042953
0.06420276064191366
0.06842744806966788
0.04156447429025149
-0.03008951858344465
-0.027210046868626728
-0.07285552390990069
-0.021414599474366434
-0.0012918195703349013
-0.01678516410666112
0.0411800187781995
-0.016315999837113966
-0.02398801666694323
-0.05121250899866138
-0.038409497620042615
-0.04363946391164888
-0.04885256992027369
-0.0026064178925206682
-0.05099672099139198
-0.01718197534942893
-0.009882066489531383
