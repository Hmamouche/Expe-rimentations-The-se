# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FSPXE
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=7, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.020342272739406253
-0.05828208247322271
-0.0419485513713949
-0.035342539027904685
0.010344854118093133
0.006256169507554299
0.043594453660789456
0.025442109620532123
0.028995393199876526
0.029276323496454495
0.05220389609200491
0.02520883788417635
0.020503292931623524
0.008903145929911071
0.0632671085893293
0.006635229685390718
0.013464242615301018
-0.03207786137200234
-0.042048097398132867
0.024251904746005547
-0.07284151353234926
-0.04099169373067987
-0.001493692373251772
-0.005902651280281678
0.003025866597123601
0.029307646318415523
0.01472580200365958
0.029887428248014054
0.012034118545840519
0.020143568354965488
0.0735016927528248
0.003919875897369592
0.037988481763938
0.014941115819450953
0.04310155952120007
0.013176986900127823
0.017136526097246335
0.04128293106924534
-0.00621406530666941
0.00019370422111129968
0.0013760280833084466
-0.006459810210782508
-0.05205857354975875
-0.04422112341344141
-0.03526448498248944
-0.048935260447701225
-0.03785837232886007
0.0024004983243714542
0.007722320262173672
0.01112882567283037
0.02694048049771429
0.018254564959948386
-0.010973851507139264
0.011888920891885117
0.00814760446591076
-0.018579630538202482
0.02765155554578649
0.01265249568049136
0.025876800862738008
0.06106515013082644
0.012749171922393263
0.06638591550592284
0.0703117269257031
0.026488503618454444
0.009722501923603771
-0.005944238392528851
-0.02053694574702926
-0.03569494174828918
-0.01304532956853316
-0.038152475816401646
-0.0065279110650558
0.010450939134845216
-0.004628159148856767
0.09826238794406815
-0.025509553951005
-0.09096271705123993
0.38555190998748584
0.2156011015201498
-0.032349548937743035
-0.08155782709003802
-0.01852363785947351
-0.052833392582551944
-0.02780941587624886
-0.06563215810183455
-0.09343537934409106
-0.05854124639354142
-0.03705889554556842
-0.0007931465654298989
-0.004336540518121521
-0.018496223440982524
-0.005005976652185787
-0.0669861755211981
-0.0006420213930808409
0.01641655012000007
-0.0025303244577027546
0.0013136395971648628
0.011687917895052394
-0.045840304930268486
0.04289230159112249
0.0319927121295873
