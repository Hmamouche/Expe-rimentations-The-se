# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; HSMW
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=3, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.10035265384717038
-0.07627999569992369
-0.040289743293044376
-0.027539908773930626
-0.004201103159264093
0.05201050557147079
0.14553760588928985
0.028148140582068965
-0.052959264496730926
-0.04332331558692826
0.020884438401198562
0.07055360050869554
-0.013082280672385881
-0.05496580827526179
-0.04625947698350455
0.03362795409853684
0.05137903512551967
-0.016325811121021376
0.046084999668118645
-0.11029534904814736
0.05768901987479089
-0.004200633353308718
-0.02130603272659886
0.028518809993095032
-0.011477146332310239
0.008627433402965215
0.0007461173370839139
-0.05525535538903134
-0.07712044655932716
-0.09099673103402528
0.01925642169984495
0.07884579161227426
0.08895035525935961
0.0021015703566745723
-0.04020357710427761
0.05931565373414294
-0.015180737233707803
-0.003612766322351236
-0.01802680787782137
-0.06679505571022143
-0.04103692882836035
0.06310279875449018
0.11474203736839465
-0.04479359979019655
0.044561005789613084
-0.05387512496597535
0.032532790370244986
0.04044051826521126
0.019022980799079502
0.041014383330487375
-0.03655844583143273
-0.0028804734147765652
0.006802926038737306
0.023560478183193276
0.03186381122987016
0.017915631183649722
0.05222916579979811
-0.008585131461523463
0.014769664872062377
0.010035306075766555
0.021901939020162648
-0.012029139194440795
0.0515000675869172
-0.041563635186933415
-0.007168606311868116
-0.034034695360284466
0.032640526175982704
0.015538896783084094
-0.0036257767753948016
-0.0013270085970551806
-0.05273355531798451
0.03604222907503697
0.07662498895461006
-0.016459565360085235
-0.010422593195664222
0.039066774229650894
-0.12267174930632013
-0.029229269833385273
-0.012282885494380752
0.0059036215029938666
0.02714075763333883
0.0896370245296176
0.08900868734933576
-0.042221872202935995
-0.002060049154633972
-0.06442707719139931
-0.013506650231180023
0.00707813964266666
-0.03392944317446811
0.01768672982741147
0.029722383207293236
-0.07238784591298915
-0.05383733338476543
0.004896410862236231
-0.03961057688378851
-0.042420108647550595
-0.0275404511985785
-0.09335810505623576
-0.0016391480698318053
-0.045221967677252464
