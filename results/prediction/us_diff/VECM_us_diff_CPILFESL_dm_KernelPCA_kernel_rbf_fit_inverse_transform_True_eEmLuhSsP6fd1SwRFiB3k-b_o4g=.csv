# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; CPILFESL
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=4, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.006994186933497306
0.006016595081984277
0.008695502199882821
0.008384782246717968
0.006627964362544942
0.005279598072477759
0.0065356080205712114
0.005528943122511924
0.005920905800106374
0.006173424167373223
0.006010670036120135
0.005650973733166414
0.006171553649330753
0.005528229119034349
0.005688038213965449
0.005838881579874122
0.006311796559687563
0.007693367068720335
0.006947727053821065
0.009107904141086703
0.007028065681089075
0.009098854664535541
0.0077181021553610705
0.00709544681808502
0.0061781906558610634
0.007597322594173094
0.00830132302217538
0.009012886256318723
0.01054448128979648
0.009080317420197496
0.009399989519033753
0.007432862553860472
0.008569889467348267
0.006999228152206808
0.0071618276409133865
0.0065723353387405065
0.005587004732484041
0.008208612033885083
0.0075470428207595995
0.006515222274970728
0.005938815339656199
0.0045995525595884145
0.005234405992624394
0.006969662638912304
0.0061154253107385025
0.0065995632676481225
0.006590601138804635
0.007269957876683861
0.006891532866688858
0.0051642787553718246
0.00498008168459263
0.005592492846154695
0.004632158065248308
0.0065782134598550345
0.005934366136673378
0.005613950327267062
0.004673414791633658
0.00481629068619025
0.005098812604871649
0.005251396264125648
0.005020600314778044
0.005106234268501716
0.005057380013755434
0.004037231023414767
0.004936969203787821
0.005895045446311175
0.005836402940038616
0.00638420503427918
0.007093920771171696
0.005948446526791781
0.005306732428148497
0.0051118893792384155
0.005872349729257717
0.006248659787061986
0.00835534320895788
0.004645188081103097
0.0062739847924615354
0.004967370610700435
0.0041412448290976895
0.0010960216891027301
0.0031524306028433025
0.003164760300231036
0.004938892264807304
0.006348358238348301
0.005715062943307108
0.006124434086725726
0.0064796965738124195
0.005723192946882662
0.00444553914154479
0.005718587753996692
0.006229513041392136
0.009527770765032202
0.008372620974469693
0.006281879338341851
0.006234959092342829
0.005317533511783427
0.005056551288759044
0.007409682053291558
0.006732744029079503
0.005972435904419594
