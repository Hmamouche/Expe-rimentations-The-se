# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; PMP
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=13, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.4447072396384352
0.0463034820486993
-0.0917825002659439
-0.49937537376853813
-0.0972825428333566
-0.1684957744683723
-0.04663930410589188
0.402765272071535
-0.0005876679256129821
0.26462684569684125
0.33992015614212956
-0.022275173805603746
-0.07456511189385448
-0.041401735424686184
-0.0036033903744814716
-0.0010576705452727958
-0.07078643890847433
-0.11449953426468394
-0.01982364887203085
-0.024543632289354855
0.15885476118833924
0.062154411603994736
-0.17559980107244144
-0.09661459657077799
-0.02482604719316743
-0.029860598347910206
0.06267774694124925
0.00960006151891505
-0.01366526190173785
0.13677357715666424
0.017200371165400345
0.028608169237296893
0.04376319530207491
0.005513801692770104
0.1132308497032656
0.0653570548838552
-0.03031020687042555
0.03803296212020096
-0.16509217627502046
-0.06474894102549178
0.025444334560189324
0.00997120846702429
0.08860317142302775
-0.1066685464920141
0.09960681781488762
-0.05505983700360221
-0.1841798349080495
0.1272518402868194
0.005487645776558086
-0.0248184840338603
-0.05790743279749541
2.8505034831469156e-05
0.0037034593702412284
0.029106033628543776
0.04616664448149985
0.06749535801512357
0.018723412969917388
-0.036919843022514755
0.02427767988856433
-0.08778233832274593
-0.0138375642092562
-0.12057077585240525
0.05403338747700828
0.0012976847969102673
0.09198102715581381
0.07820527581341115
0.0019282479277675825
-0.04009130450156144
-0.051393312865759974
-0.03800246043332425
0.005964639791093447
0.12042203464484919
0.0021016711588539294
0.08584877178155487
0.1574347163609426
0.06274075564380382
-0.11539242172043684
-0.04866257196117103
-0.12873061459518736
0.12790024944527612
0.0640887766272603
-0.014943310340381388
-0.059156079671063753
-0.13078896942824128
-0.007735843282562868
0.07115263238309373
-0.12216897505028476
0.0037544744559735443
-0.04145842050289367
-0.12842746530715196
0.07561838638604412
0.002996410513737109
-0.10688093823867821
0.020589606901580692
0.011645737207708777
-0.0073073525909235285
0.08799912502011752
-0.11453717624751913
-0.07810388474449932
0.017492559832308952
