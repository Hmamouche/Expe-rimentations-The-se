# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FYGM3
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=11, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.06945938389689668
-0.06449966104832816
0.07685724218751966
-0.031177739761751116
-0.03739933149994635
-0.13567544906687162
0.0073740546643822485
-0.036410090406941056
0.049992756261043575
0.05777438583742094
-0.06541357796158664
-0.06297984889984416
-0.04304512010619081
-0.004970918953371501
-0.03164148571045085
0.042356975240359196
-0.02502795725137867
-0.12928133224742816
0.10673050545344973
-0.017055717707469324
0.051685729063167034
0.09658784404837031
0.04059434606233325
-0.05000398768817473
-0.046431561145206385
0.07120990900621368
0.024866256886736833
-0.05057801560633733
-0.051084748542309696
0.0745083764490228
-0.16230537628437017
-0.03677287700681044
0.04885916936518074
-0.0028716038763870293
0.019284854658628295
-0.01543628470420294
-0.0959626228788283
-0.05305480229824858
0.026281256729549597
-0.04611055876706677
0.10483839433008271
0.011418714918939553
-0.012416163049664666
-0.012335586653391226
0.019606178979397995
0.08104597679683712
-0.007142483717513828
0.04172428561857437
0.0430912062771398
-0.004765212359897481
-0.03086343386879149
-0.03990939279114446
-0.0026806189646486935
0.0013831900099729108
0.010022418953075377
0.019101523975405904
0.03386437801559384
0.008841829264863377
-0.022724524721135515
-0.025038273417150213
-0.008702382672297953
-0.05123587980473254
0.008602730438478022
-0.04681873969744075
0.015149493047221952
0.07328327386473565
0.018180411901507834
-0.013245650785868753
-0.024624048087702285
0.025355519369658255
-0.03140910225338826
-0.010094639958922234
-0.08259578402945321
-0.10786258459865114
0.06743430684515837
-0.01884584973976753
-0.016876859097082822
-0.028155331851905767
-0.04254428883229364
0.029270341336859903
0.020640552562394103
0.023460743511785093
-0.016113232149949742
-0.01198150951405488
0.012267319643537687
0.021049660771308473
0.012118265137983919
-0.021353891620774182
0.017954653063514833
0.037328301337366476
0.030049751298789527
0.0445027124077942
0.024953743603927436
-0.012321378477969703
0.029499800288110747
0.04104730809406273
-0.01003800223714283
-0.06627970479587765
-0.0768570877740466
-0.021324976948799583
