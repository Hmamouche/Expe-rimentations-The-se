# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; HSMW
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=1, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.02257243201383119
0.043439261903142325
0.014137256702187795
0.06367589965696258
0.007949347975537757
-0.012835875625547617
0.06585631429870767
-0.030930591481772164
-0.0010727676108382295
0.008991933397597714
0.0037078416136041657
0.04145791731828108
0.02374856137880234
0.04613358847252357
0.05933381135419755
0.06881111744968292
-0.05528128105574833
0.001136758224808212
0.0002035074714431956
-0.08821063014430491
0.03633621526295586
-0.039269162961842796
0.005444162123556993
0.021476379345660095
-0.03984012682806333
0.04317866370346621
0.014243275460500892
-0.029492191589618744
-0.027743975812807256
-0.003526891385743875
-0.00318694304487558
-0.022994261637544124
0.012395064811372997
-0.052303865944946945
-0.02505248597332023
0.1179531276066189
-0.020050444109903757
0.023960518667502213
0.06770807045660196
-0.037379769776736405
0.005653978016413738
0.032953051190707475
0.036463338491027696
-0.018830851395009576
0.04862520368060852
0.01819038684691329
-0.0029993253356496302
-0.03128351737241866
-0.008747582562076753
0.02663664438691572
-0.04811912028635612
0.031060312345793567
0.024960458388511304
0.004717536831170358
-0.0005118623508002015
0.0024697048290468473
0.000803771804703951
-0.027308118383210893
-0.0013179656407154893
0.02973810081129348
0.0002642769591008039
0.016091931782710303
0.06932686367444542
-0.04068445058725235
0.016223980975516097
0.023857460887616436
0.008751848991868854
-0.014300807249658296
-0.0023890364654369626
-0.0012121105933645474
-0.06277250916493916
0.015718002624555083
0.0019248988498937192
-0.02587348140956224
0.05588926085031104
0.03835267479739886
-0.05995919606906084
0.0227834267761284
0.04410156982654592
-0.0134918768908615
0.016417821377177778
0.0492310530113532
0.0033908849438869203
-0.02408879012885043
0.02601198823186256
-0.01105045972288898
-0.017449086396926997
0.0057101327746085975
-0.005736916682030837
0.03327046994834251
-0.02757406684084488
-0.030950267898652573
-0.019371565938561066
-0.052978494376793986
-0.05634069004783043
-0.07443728611749624
-0.04539151532821279
-0.05868814179611219
-0.022442496266543134
-0.006304489167746792
