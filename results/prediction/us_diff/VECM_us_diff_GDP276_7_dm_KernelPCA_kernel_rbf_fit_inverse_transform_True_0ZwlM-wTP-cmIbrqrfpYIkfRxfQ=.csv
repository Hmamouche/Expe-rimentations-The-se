# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; GDP276_7
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=13, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.005443198360723853
0.00710907266361552
0.0013214381528494903
0.011129270907766806
0.006241137379472178
0.005476066903405474
0.005740752554930303
0.007325840746110047
0.0011614494072619327
0.0017378868448514823
0.0075404131145122184
0.004833854706485325
0.004308556630418605
0.006011445990834833
0.00892327699554422
0.006646979304355575
0.006305736044371747
0.0061244472854639215
0.0020851075053886655
0.009502251281135968
0.003837216096426067
0.00522217789624593
0.010649398261318352
0.006620769631203414
0.005561269729521068
0.007558108734201609
0.009015404881389834
0.00796888945115295
0.011720216700256877
0.00693495481466363
0.006139451070140818
0.009567584772307937
0.010287777919019782
0.007090031658307395
0.005441479106684889
0.007919594438641058
0.007175911593429261
0.004618206708118922
0.004402155556127161
0.0042592348190410275
0.007758788317638918
0.005376888602996762
0.003985010387302984
0.004472672718785434
0.003890904851970349
0.004371429074393116
0.0058695748112962045
0.0029941862915410004
0.005447402812717894
0.004845595313790687
0.005867711003668006
0.007762486726404902
0.006415698482168905
0.005425556431295199
0.007663619294005645
0.006217587025278428
0.0059632285913464225
0.005528066732590514
0.006290877274051963
0.006413547332769821
0.005622033627409806
0.005782480390328089
0.005710238938466535
0.008296274252875897
0.006186103302938492
0.007334151471544741
0.009097404084966756
0.008283085930131369
0.008342241224939683
0.00914711581901029
0.008143572797080358
0.009562736847065862
0.009437469565801198
0.0034422632133587665
0.006770603042342857
0.007886359583627262
0.008341528875358543
0.009274207692229634
0.005299108217690376
0.004993940964382821
0.007614565380673652
0.006492876151250089
0.005843847039676082
0.008010706466421713
0.006995530568994667
0.006244077238211153
0.008621282322257921
0.0071209511324689685
0.007738070731917933
0.010779122534970088
0.007835285464337635
0.005937042536680836
0.011465582284779934
0.0052340271579866034
0.0015863463447491502
0.004336708185697611
0.0059327318744241495
0.006803742460706351
0.008956686153682464
0.010604929780091943
