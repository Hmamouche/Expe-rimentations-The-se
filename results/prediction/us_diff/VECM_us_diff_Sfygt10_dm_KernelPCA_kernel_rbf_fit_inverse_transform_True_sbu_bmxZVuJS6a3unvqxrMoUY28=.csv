# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; Sfygt10
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=1, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.017769363180124426
-0.1427940675947787
-0.02250338759982308
0.10245729854588664
0.06576187064240452
0.1687060183827423
-0.026382783501055513
0.06400160533574875
-0.06479142063837454
-0.008764458381163777
0.0030195426003740006
-0.0372960899568413
-0.05470930826524145
-0.09689919657802643
-0.14326733745281173
0.05080356271643461
-0.07461123271102996
0.15166087329073313
0.005312769417905289
0.12165662381867473
-0.020155739091947768
-0.05327731428758914
-0.060103680304329166
-0.03630316827398714
-0.07186429611082401
-0.1043244250382935
-0.07791353641308728
-0.02285436494711218
0.07408512448793995
0.10192439832777747
0.13417406094129225
-0.015305760401204384
0.0072613260686518605
0.08921151600467045
0.047739872157292404
0.105118724772025
0.031091239634111693
0.045790534394707635
0.02216313717616371
0.03209058044170603
-0.08085815505070262
-0.038438472206825176
-0.087418320794614
-0.07687874135140702
-0.00557851280931804
-0.03522966389554168
0.03801290373068152
0.014436839081267652
-0.07261997141186652
-0.06597062454306243
-0.08452204847315647
-0.08029228160811352
0.014281875217042557
-0.03424588990159136
0.06386739921665696
0.004016973423495933
-0.04447175570124655
-0.027680009344476997
-0.017452639203372512
0.009347167631530635
-0.05403674961143312
-0.008845285854021955
-0.06710351442085329
0.03219105206104013
-0.04025221548446161
0.01172754474408003
0.021100858698594453
0.025086179150445788
0.000807114609210487
-0.011059778003127366
0.05302358746397258
0.030884470388587815
0.024684730945267623
0.19807784641840692
-0.022345058177427088
0.2043447771237819
-0.018287809949860324
0.13563609683458638
-0.007071308250754738
-0.03519869183169753
-0.0328403186955606
-0.09000279818412202
0.007281248682228115
0.0052031972346775306
-0.015740106660172132
-0.006641422200293425
-0.0868901880084012
-0.0050938419202440875
-0.12986900158082804
-0.07311780038877427
-0.10395518857783789
0.011096065289050393
-0.07162807254452652
0.021298247968892334
-0.047612503925306304
0.010028882138407531
0.046153303500160794
0.05381319692670712
0.12113136637098042
0.09177510888695285
