# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; Sfygm6
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=10, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.1329069519348757
0.1635012129454037
0.19890518354964365
0.06563227994823809
-0.031008726472503953
0.11709492360856906
-0.11603437997833105
0.030519816816410084
-0.14466958940658087
-0.17263230746503577
0.0270088962617722
-0.04239395931921518
-0.02937861926450553
0.11045284283401624
0.03658444982808057
-0.009665127814271042
0.1145696781105603
0.07086504711811212
-0.07697782574661817
0.13052512563724544
-0.1433812422563651
0.025115278750465757
-0.02526495198007359
0.02011272857845669
-0.0565371563207953
-0.15962564805662183
-0.011797326930302887
0.13516725276372088
-0.04452847937522388
-0.05241039867457808
0.08241004487118928
-0.09132100384151973
0.0827381508911196
0.08551940372306718
-0.011905303260304631
-0.015248719037521612
-0.12191382588318875
0.05750757451103039
0.12616971724307907
-0.024503704506314165
0.0030547648094532698
-0.004551048349086026
0.05557351461009991
0.10969642074276711
0.0125058896684235
0.08043774027437613
-0.015627570994530195
-0.052283245540590606
-0.12796425913103068
-0.06082132428771802
-0.04627229771641781
0.14079845956341838
0.0702270327005474
-0.0846802826109911
0.05679622213774477
-0.003906732234439405
-0.0650429848951112
-0.028203691976826314
0.01481990020503025
-0.010673185133653446
0.0048025386432635694
-0.05707788149971035
0.0899117720959115
0.10505489034148469
-0.07495678973262007
0.042925190002085514
-0.04813149648739959
-0.0739844447162588
0.0032862424555336243
-0.00761390710474929
0.03303513981026217
-0.024066145613841503
0.010380611486488492
0.042367341570925914
0.022313245688557054
0.023271969925942528
-0.2977093462288823
0.10869299929629822
-0.03578606070092227
0.06582040020351457
-0.005011281903335371
0.05956606051491857
0.07954260808938998
-0.0727594040199587
-0.02462051786798757
-0.07166649199835949
0.007232915319787942
0.10646909312156369
0.02272721667619613
0.0024132814500810486
0.05912690266985143
-0.01322250147111845
-0.03173686801432521
-0.00013378460056910884
-0.07785335591471851
0.023811479744980962
-0.030409370357468872
-0.03311156581877759
0.02746661135773565
0.03910703309466082
