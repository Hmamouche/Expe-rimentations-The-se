# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; GDP252
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=4, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.0022632638602024663
0.002082606806213863
0.008435246443622011
0.004578708645488787
0.0015848751952837181
0.007654183941564101
0.010272006335760827
0.007195674549866056
0.007810934983950406
0.008932387294679255
0.006769368772222159
0.004765422970367623
0.004150051214137778
0.005371420167506941
0.005502211016337309
0.0037714689904330147
0.004669672978901231
0.005041177677343553
0.0045036331345467105
0.008272411152872586
0.00496680219985796
0.007283393331187865
0.005459971561200762
0.0031148538762125117
0.004758895051650587
0.005793463745063709
0.004829090823154761
0.002825423020907456
0.0027687342281363624
0.0014999073392552899
0.0005981365126891064
0.0013830350198996016
0.003246027298773957
0.002493970805010093
0.0036425347387883758
0.0032070772270007173
0.005099164730103587
0.006666070559104452
0.0058852248496211144
0.005334531997490444
0.006362309719404687
0.009001266389646073
0.008609166398958049
0.004452728837233493
0.006033618050309239
0.006691117451724477
0.0041491658215065275
0.006769503373597993
0.006401341218076077
0.007151189710538959
0.006476767933677692
0.005373088058802299
0.00673690980008093
0.00803440983111071
0.006348696605729815
0.007811294684842565
0.009167482961028752
0.00888989753785327
0.011526101900643045
0.009411029717226333
0.010137367085750543
0.01322734403144766
0.011908221311741008
0.010036372603700347
0.010379265774512945
0.012952849853438727
0.011953815985039236
0.012066120467861942
0.010790661716454285
0.008857805144284518
0.010066975863787192
0.007743916757108352
0.0036997219544442966
0.005707688283375077
0.007842814163464232
0.01092463578316367
0.00509710233018985
0.005631514640133405
0.002754617847724948
0.0063851294262685125
0.00961229664035215
0.012144083110178668
0.011659820549020511
0.006665773429300795
0.008006560976166947
0.009668274658036874
0.008998761940143736
0.008473911918537184
0.00845735487170485
0.008536090926089323
0.009215315918579875
0.0060711477429384285
0.007282659739859485
0.008552414202396677
0.010096302000416073
0.011312889801196985
0.009687307398169642
0.00464289525348415
0.004310299780234608
0.0037395625449593273
