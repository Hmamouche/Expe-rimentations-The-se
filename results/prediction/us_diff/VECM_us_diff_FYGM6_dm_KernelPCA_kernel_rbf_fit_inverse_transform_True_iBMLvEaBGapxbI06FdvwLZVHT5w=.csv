# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FYGM6
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=15, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.11433566108166444
0.15558573554313365
0.15994923868618827
-0.029451156196049655
-0.04963018148666271
-0.1251670343835565
0.06469257776282826
0.004018257131108059
0.07113712245848719
0.09894013971937278
-0.07032497867889005
-0.17664761689670067
-0.11206072616797425
-0.051078205209030926
0.0851866872150199
0.03488913389606965
-0.02822616809132255
-0.09736285582020662
0.08355171900146811
0.007952064300180223
0.007559294889957824
0.1411971903249202
0.033095776382024704
-0.04564068006777262
-0.03200552106975233
0.08039296886530901
0.0021724910284295768
-0.05271259653472089
-0.03654780293159867
0.06525941077934896
-0.1676824666717466
-0.08857959287274847
0.05111810320741716
-0.04508897500077914
-0.001887489433975575
-0.02960024859005775
-0.10670102076541885
-0.10301256768288387
0.009510495722570648
-0.056438889070756226
0.10173300225449894
0.0026251521398148493
-0.010375853702948464
0.016211022979912042
0.012610607639477607
0.09220723945906964
0.01943192036252485
0.04823640409821525
-0.024562523152846995
0.024588135778040206
-0.07125337348377911
0.02733360249980135
0.0039097546950135655
0.029196039951566027
0.016402528142916508
-0.019843909113114545
-0.01900506167434569
0.024843847906629368
-0.013723309636220987
-0.029906885533241813
-0.05328968340654349
-0.05766054562264981
0.04141079307651514
0.0533662146466157
-0.011955047772435057
0.0982667770300608
-0.0788299293589616
-0.03262357697422544
-0.02067116812078163
-0.008848712050343294
-0.062481860841423674
0.032266048614046805
-0.053231976429760915
-0.12407144610802491
0.04500228555413548
-0.03650273826849862
-0.039130776431759134
-0.031965656090115514
-0.07617246439444919
0.06614720478975367
0.039631210747409905
0.08168550384551279
-0.02997698174087917
-0.030680094221555772
-0.008284221598201434
0.01422188755586115
0.02823947740271081
-0.035657195013018686
0.05611140707126907
-0.004930766531171067
0.07376961698414253
0.025279759361774692
0.044978417578294336
0.016012857992812428
-0.007066739223719539
0.05283714134793574
-0.017578640116233
-0.011851133055704594
-0.11257140043431096
0.007142334371023082
