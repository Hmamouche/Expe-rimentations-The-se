# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; Sfygt1
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=14, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.021653753022554068
0.3224887413439982
0.5020132027051151
-0.23001515668642603
0.062152298191909
0.17261351256783136
-0.20912917340466086
0.2567407995985157
-0.45520473630355446
-0.07154634106998858
0.052807986421657516
-0.18220483008508745
-0.21621754627878678
0.12073812523902282
0.06986351704518859
0.11440218959223798
0.19396296622740913
0.1010158085595103
-0.031016888474203638
0.03463333488947301
-0.15828293213931702
0.27666848372911995
0.07949835040462663
0.04598732762655518
-0.19932716036082873
-0.052935012431413755
-0.06973341857616538
0.1624858323723813
-0.021096557151563056
-0.10010958140258333
-0.018182224952687343
-0.004398304867393184
0.20914360217796452
0.10849772328915655
-0.09655603460790041
0.016625422000784394
-0.16288746665073706
0.0762244715327041
0.2775105158449589
-0.13049763272431492
-0.023810574577335347
0.02257645393714703
0.07726490441147664
0.09656798552195078
-0.01956797798124992
0.1808304569062074
-0.01250002345754582
-0.09663788568924037
-0.13090991304420505
-0.0955492146341421
-0.1024650953835223
0.18162679622274952
0.10022368057698343
-0.007407296139658128
0.10734469896118776
-0.0641855789866087
-0.08800751597240433
-0.022368171482579115
-0.05660810879996066
-0.01838451916055355
-0.006805322043522882
-0.0680494407088291
0.09514048679378342
0.11439402329041998
0.024944067836528576
0.07874027769955042
-0.03886482507702594
-0.07097845516060647
-0.004799079322227862
-0.0925031777466546
-0.08230575345010252
0.053438071888463994
0.05432759903259278
0.12009756366891408
-0.000767363697260659
0.08760200188136513
-0.2652285598481287
0.1002662273443748
-0.0944997849477486
0.06972787076343451
-0.012489517665521005
0.11209323205366381
0.15737400145861458
-0.07664170879318413
-0.09529755127339273
-0.022408434279294717
-0.00690546895876332
0.07305768683405636
0.07235084901450656
0.03349780810138528
-0.04223776663538268
-0.02482127425950391
-0.0815578013699272
-0.13197385842046147
-0.07169623604470529
0.02653298961026785
-0.008510033724087478
-0.09241870450952648
-0.07582863531832487
0.040733878337460644
