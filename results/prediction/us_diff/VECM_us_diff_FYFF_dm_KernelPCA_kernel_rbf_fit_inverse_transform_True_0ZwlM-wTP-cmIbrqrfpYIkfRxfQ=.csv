# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FYFF
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=13, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.0657850069413182
0.02981084629969668
0.1066655515486636
-0.1092693059544313
-0.022516334866208146
-0.05322235678678635
0.01869557945304537
-0.06795160548789952
0.052225622229870226
-0.03605223611583323
-0.0626082413663184
-0.07234886067183595
-0.031259736544666364
-0.025037195203121615
0.08649791337867668
0.077129406666152
-0.03970472958155439
-0.1008767318418572
0.10499278057333791
0.03341725685296122
-0.015142518967847022
0.13784080288054987
0.014536651042223403
-0.05704873006709238
-0.018919119308462434
0.02573806690079834
0.051392626000853196
-0.0009948245414359309
-0.07050939466543124
-0.009785163309581565
-0.13385575100274139
-0.12299474349756831
0.03167095064746747
-0.005557980187193053
0.006372830321305787
-0.047092581627693364
-0.03109977666464625
-0.02958132367179156
-0.02548479344937253
0.01691058394746056
0.06381477059561219
-0.01280732505734782
0.05305488757705566
-0.001554499372130088
-0.02008988711086724
0.07515664673312297
0.04695330212287052
-0.011535532081948305
0.0017957983960893626
-0.037537265109096954
-0.009325818110648056
0.017036814688083737
0.016589179151532622
0.005287076703656293
0.01136644705040718
0.0003037098187956581
0.025953306124424584
0.022325114138735714
-0.013453768813239307
-0.0052151041808717255
-0.08115530384643158
-0.04380298352053527
0.07955084828453077
-0.07956739535372402
0.03477433308525231
0.03489805648646663
0.012224067233078006
-0.04207015503394829
-0.023274721721734455
-0.022478417305158822
-0.049834632431077955
-0.00797722760085117
-0.08651057571927416
-0.1591721404747624
0.07483460073732559
-0.03296079649999629
0.05408264226491401
-0.07817980451110147
0.01937908635312147
0.028658390106911923
0.016811202197642573
0.012865672898118862
0.027935815102794956
0.016423342260920494
0.017069652361490703
0.018263360017396823
0.017744312896836152
-0.05426381638889793
0.031745057377949304
0.030364986876649527
0.046894015619419624
0.014060424862888426
0.02882885751857992
0.008101681042339553
0.019039064960846556
-0.003192661456978111
-0.014606141922755746
-0.055931101797454946
-0.101303604611106
0.03714957478841257
