# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; CES017
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=8, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.04080647882700111
0.06741980349302838
0.04401833374563684
-0.01534449350750118
-0.0277540399836758
-0.009571143000827828
0.024267424782971336
0.0017224531617623176
0.0022796909497692953
-0.02107677792157845
0.012608252862852106
-0.03273901477167344
-0.04434505841786482
-0.02441163038087705
0.019222557104085502
0.02578540961535105
0.01733431757022224
0.0028781232582510644
0.017493336255951016
0.01867972262953754
0.027079683450724392
0.0441125820946504
-0.0261018630203644
-0.03150366782044196
-0.032650364172248994
-0.006526464679340633
-0.01799253048979005
-0.024314808824193294
-0.022437827449935367
-0.07768904828367476
-0.07217657190759963
-0.005154485193928051
0.042608654411557596
-0.01787769756004695
-0.008269036892089372
-0.023119390556286867
-0.027476041929418014
0.0160092312526604
0.015636300803922677
-0.019992511290000254
0.015411724985239187
0.01859786755618675
0.020952608326881422
0.02054383084104879
0.01393466196549649
0.02709130118413417
0.0008908684935578469
-0.006207246260043582
-0.011222144939581097
-0.029057504918026843
-0.001551975199002546
0.030917824055507668
0.028853611100309855
0.03979039812036325
0.016936570306320644
0.007949044954906812
0.028907057832509197
0.012622865237692554
0.011406744892956483
0.00608302217199119
-0.03288939911938829
-0.004471810415256995
0.021183156562729198
-0.019920319322951067
0.03364389189371699
0.01151124094695434
-0.006517201232343698
-0.020518266209076916
-0.011794749304112225
-0.018955215919725515
-0.03709086647621725
-0.04607501970255626
-0.07831847861959997
-0.0966531378421381
-0.0003407803929932063
-0.0295693800315068
-0.0037391364923741177
-0.04389263030782096
-0.0559797294142313
-0.02445201055756322
-0.011138456159651044
0.006177844460577947
-0.000861170585710266
0.0003993921912227756
-0.0057556411480449154
-0.006078199738445276
-0.013505285953186133
-0.008407556305837312
0.012100483722442419
-0.0029640471966953853
0.02745452113053129
-0.011834099384779462
-0.030008044045675274
-0.005113603003735144
0.011224605174610963
-0.01975219806575776
-0.02128458443803054
-0.03829131086716766
-0.038868038396678115
-0.014944964288438082
