# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FM2
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=6, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.009966635878708092
0.003714112902806823
0.007672699714634482
0.0037525026725399742
0.005888539189591445
0.006532365664428461
0.006560853348164975
0.008384432278306543
0.00516806966093955
0.006364190546583201
0.005848596781755854
0.008288700001347309
0.00653656613770176
0.005559922138032828
0.004755022063045725
0.004709643465646906
0.0061518780215162085
0.003982950916149372
0.006222952281812799
0.005646327361154382
0.003960226710622426
0.005267783013522254
0.0036279648250369897
0.005210820754506686
0.0066394059913017
0.005836487306067778
0.003873633186425235
0.003479425052343449
0.00543262922843387
0.003634222341458249
0.006902900399505931
0.0042982383339633876
0.003807490144133202
0.004222049600678087
0.004000290327930284
0.001335242119059043
0.0013135555716380966
0.0012120976172261654
0.0006590582215084898
0.0018789439383112684
0.0010200327702018736
0.0030807801022393125
0.0003330571943082249
0.0005783694127681025
0.0012791193576086788
-0.0006460354988251717
0.0021531622972372874
0.004568517027754335
0.0055936559334095306
0.0041073527508042615
0.004790640094811619
0.0032614591423726857
0.005911028991532761
0.005538940054727722
0.004942167111307892
0.006193432893502205
0.0077355601007887175
0.006652290538226977
0.009530212081204022
0.008417400123502502
0.00984600121390567
0.012900704712893497
0.009303064065379351
0.008804446743235952
0.007982228843921826
0.008933462714804248
0.008879958858446668
0.010740642912242956
0.008518809374093437
0.00911463232016538
0.015613122771675838
0.013050330417627945
0.014090184120545624
0.013820476280038659
0.012311781489722617
0.009230305872997654
0.014943282559399095
0.013652708604445753
0.012412416409573853
0.01352405078584816
0.012725620769291148
0.005699176467124363
0.011343544013021173
0.007562048707253261
0.00911345552975233
0.013019528136837388
0.0033139290980394847
0.013290638822114158
0.00805479309623792
0.010388348138592116
0.010903822090279727
0.006184528291980702
0.012633950670845273
0.011563643839924408
0.010501057009363867
0.013874287148706265
0.013358420569507735
0.013784296973149115
0.018777496376181847
0.011751650194721922
