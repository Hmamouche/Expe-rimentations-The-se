# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; HSSOU
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=1, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.013980259406406962
-0.0063825449484672565
0.010273833493411245
-0.034519390981909176
-0.0272468489356798
-0.06936678503079807
-0.01329222613687474
-0.02294926413189222
-0.04249159931065833
0.0034771419185457893
0.014866104398254571
-0.01822353177288638
-0.04104504788569873
-0.07329514282922092
-0.12146257938559964
-0.10093785760989761
-0.0854261401071412
-0.06824237712112229
-0.06447107688410539
-0.038064882152492485
0.0007778183989162989
-0.024249062471502218
-0.001972094883733219
0.02249183815160999
0.027746287119640334
0.01922910055111652
0.007649215304033585
-0.019722733333560193
-0.02224981420936156
0.0029718704553126744
0.02634570291474141
-0.030486302911642393
-0.032247561014370166
-0.0032999143578873544
-0.0034236704472061785
0.016881365416355147
0.024109928171544497
0.02648011615008457
0.01725665953867532
0.02787578925776577
0.025859658826240188
0.04001203486397774
-0.001528037982441415
-0.020645572664454166
-0.0165500336560822
-0.020715527745041012
-0.023985612465718606
0.012927780402048443
0.059674503831042384
0.09219503403091359
0.02719784635528116
0.002053292101817629
0.02216037374395743
-0.04584566176513145
-0.051858490820553056
-0.0022982820785829184
-0.014844027998908365
-0.012667297708067703
0.009581327244697232
0.05456727644051605
0.07237679593676578
0.08585322123338532
0.05736601321949971
-0.049280732575847294
-0.057371262257884775
-0.03338641994891761
-0.016564515310403812
-0.044670346637335795
-0.00028553916912793173
0.029149417530641777
0.0924337026236236
0.05570536222322424
0.04360191104585749
0.055058534435398085
0.02870917830079913
0.015375914933927594
-0.01886991933563168
-0.006064502229948954
0.05103584341484499
0.010264613568005313
0.010448086469655785
0.02363168852639209
-0.010830626804991538
-0.02194329700375385
0.03006538062454465
0.022583844834761002
0.030945170076054636
0.06837039058400624
0.015484577539149941
0.036544597004405766
0.05242947610865881
-0.045560370691205984
-0.054161982390410826
-0.0502855884835761
-0.08655157106040555
-0.12463426524280347
-0.08131305194549694
-0.09294654584716723
-0.053678557158621326
-0.052973872102391226
