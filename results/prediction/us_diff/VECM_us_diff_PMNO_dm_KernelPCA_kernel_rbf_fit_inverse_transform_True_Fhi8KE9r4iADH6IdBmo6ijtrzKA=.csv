# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; PMNO
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=12, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.3457063010661078
0.2099869953437109
-0.031675737534257814
-0.4049921329550563
-0.10441789937159783
0.04086701822339986
0.0477034125985956
0.0384437414289797
-0.04920343773120152
0.21153319527433218
0.27706914828865753
0.017402391191344006
-0.06565477584504265
-0.08925863094125638
0.019540783549987305
0.06300899162697929
-0.0008079589188092345
-0.1790487480744503
-0.027652764213572358
-0.03424510699769196
0.0583880193320908
0.12701456894844054
-0.16961935686386978
-0.06881707105051665
0.03334001183563272
0.0043500452992404105
0.016817918851034522
-0.06025220461082527
-0.10220306365122786
0.22838712078256562
0.04970143249186967
0.09041814508357067
-0.05363580415011292
0.00014325168617387862
0.10712794273254703
0.07731671879507328
-0.014940107796623275
0.0768603240411169
-0.17397680351466482
-0.05793954796889493
0.12011218579991355
0.009177605916605543
0.10339443770598222
-0.21407068761894865
0.07515111687794371
0.04704031599987171
-0.10200393863367098
0.15150107498439977
0.035589255648549
-0.014072828425005141
-0.07541156504964239
-0.07777279347420207
-0.011139638273366089
0.042440654262027616
0.0197206634542502
0.03552090217577409
0.08805145578913812
-0.059332138063519
0.04480839011710658
-0.061923313713340276
-0.010855997650292847
-0.02597119124665162
0.005307455036208438
-0.043843763297777866
0.06387650843073225
0.05175482848003511
-0.01718088450381714
-0.06663266000632546
-0.09565734394238938
0.04762967838029854
0.005168452025218359
0.11104533155149846
0.046349975739528565
0.1829895882182652
0.15331974530847323
0.04060507441285045
0.03267079882629406
-0.09398465235420028
-0.16709031019579518
0.18010121522632858
0.01551150740218087
-0.03381369625287494
-0.06173737680173883
-0.12281510351772365
-0.07978851959232633
-0.07589369758593811
-0.048705806644517985
-0.005299860514373072
-0.011665276955778587
-0.07388883513335157
0.09542161260843715
-0.08135538328913693
-0.12037627009028105
0.06146898510556339
-0.01729694389432694
-0.10935799101070454
0.04478044122418583
-0.05170677998252438
-0.10545068892258602
-0.0011941034375463205
