# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; GDP276_7
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=8, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.004585602459883326
0.0057779388068007476
0.00535357024844014
0.008314604563522092
0.005146077129103872
0.007105806481117113
0.0029529867999860404
0.007191814060892616
0.0033356118894444205
0.004137640161342092
0.007659155459261023
0.005671541370261712
0.004953794932021533
0.0048364971739459015
0.0065429510577047975
0.005798862594470265
0.006244796014776695
0.00723941017421509
0.0031899132201299845
0.006802874620923041
0.004682142495073587
0.006509914351825978
0.010540089655523242
0.006178867776942871
0.005450985988244351
0.007423903255934502
0.00814720925854294
0.009229295769250705
0.010386593856730008
0.006483708972279978
0.007687886085096667
0.010173818659045066
0.009917560003920786
0.006650749989893165
0.005048278247380243
0.00767308978723075
0.007332879959238734
0.0035216418291934715
0.004157648742613009
0.006103573849303783
0.006902588493148204
0.005976337001452504
0.00485287326837229
0.0039335512171502374
0.003092904658713479
0.004536242796580604
0.005099729760789946
0.0035441724224780057
0.004883648209528201
0.005811960606666914
0.005950371944743616
0.00738178447179895
0.007305036646431288
0.005769660917107008
0.005766781821064225
0.006831875487665053
0.006871341019674064
0.005312914779212714
0.006104458964680399
0.005680861924418528
0.005730447664570394
0.005631547287917532
0.006466511284425732
0.007785343499304001
0.005690130080419795
0.00759669023068563
0.00847337431451911
0.009117140122314593
0.008637273825577232
0.008911059882775074
0.0081300811773904
0.008423181485236278
0.010297574363862734
0.004680042451044813
0.0063522262374798
0.008369716686389094
0.00638937861378705
0.007869996375376597
0.007715876489951327
0.007796540107618288
0.005754267235873391
0.004759811335186831
0.006830267322436957
0.0070642875983050754
0.007071513318339077
0.007859065412467413
0.007958335660887796
0.006419065889533358
0.007623112559469609
0.010541092569538935
0.0080153448175937
0.006774909118684128
0.010657104519967538
0.005543118979589513
0.0013721159311185839
0.0038249840802089736
0.005087784685552669
0.007305036255050401
0.006850596913625031
0.010708131239453297
