# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; GDP273A
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=3, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.005353086689555664
0.004811490971693467
0.006925260701666091
0.007749892349021893
0.006555891492949433
0.003582482565942397
0.006101257751582958
0.005004853647930296
0.004349466850382176
0.005061852424877623
0.003693451905232797
0.0017773259910669093
0.0032563367646935345
0.0031502378014396353
0.005795147091795662
0.007840505449078591
0.007085056990571914
0.006855023460924351
0.00651303119086921
0.007479560862110173
0.008824557131533739
0.007587039540931852
0.008746206274647466
0.0091479612774963
0.006559381582140743
0.006703573393704614
0.008297169859398518
0.0071492952079017175
0.010083607439314813
0.01044579161721088
0.007703847346492751
0.006241799265139288
0.004975177229641133
0.004625206168582089
0.006362578609243802
0.0066713217129676355
0.005883214891843772
0.00516104705589239
0.005008288260412708
0.004766595243205607
0.003367150975443164
0.004526847577667638
0.0038446593455181886
0.004832394032613123
0.00670978929222668
0.005356323132510272
0.005644789646959949
0.0037450418050252172
0.003969847295979054
0.0044690254123775424
0.004478089981032492
0.005531109621880662
0.004674345169185427
0.0054538368965707604
0.0048555793216728035
0.0035729045824626528
0.0027373529112300686
0.0022274772528613398
0.0015651876942166692
0.001870087306344192
0.0019900269526887874
0.0024239392015512977
0.003507197051660873
0.005201918842250893
0.004621628665975454
0.006173014333892615
0.007965488861669843
0.0056750821785659855
0.005699276218552357
0.003989834208787475
0.0054381244970015145
0.00641536668726647
0.004586727094998692
0.002422507877929897
0.0016234952106828644
0.004397218880122462
0.004069919054569091
0.004805712645238994
0.00721520195959503
0.0029269693617604937
0.005539938571772085
0.004159128973573003
0.00654878725934035
0.008937433086312391
0.007015267239464416
0.007943016034975877
0.006827820003538003
0.0068394652354712825
0.009487747805430205
0.009477246430022353
0.007681983120856205
0.00805761033690862
0.008351610410555547
0.0034151623957452285
0.006813696729999697
0.004813843368272088
0.008738131948144498
0.010325700358591827
0.009079144168579384
0.0123373567926584
