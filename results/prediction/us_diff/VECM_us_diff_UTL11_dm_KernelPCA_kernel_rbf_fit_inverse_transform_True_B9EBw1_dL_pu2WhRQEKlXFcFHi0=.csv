# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; UTL11
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=10, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.11716842169795831
0.04120623844204223
0.05973442685905399
-0.06810865078685552
-0.045622560617208666
-0.038752555591551725
0.1008061135631688
0.02642198118323638
0.019495223980107706
-0.016328681360865016
0.10446964484728746
-0.025397739108532578
-0.0315776701609706
-0.005508652408614686
0.0489093856332605
0.0717545739131668
0.06391053757092433
-0.002027652009572138
0.027140550204018656
0.03862050283462397
0.008101939910264571
0.10126733736972587
-0.0696821021241236
-0.029977905309200026
-0.0672107805934302
-0.007402217382841109
0.0063179495156188556
-0.037415010639570215
-0.02611350951222989
-0.13997743855827072
-0.050413581339467585
0.030793811943421134
0.05388660955877238
0.002511735045147248
0.0365817183318245
0.05165313861765252
-0.039822399879368954
0.07389512284703445
-0.009548981913877354
-0.044987109881179134
0.05954951970505769
0.07469391274148424
0.059566261965187633
0.07118650163276333
0.028609729442317468
0.00964415201315083
-0.01352545462070446
-0.05405445767331293
-0.07210939101275166
-0.03199128559024945
-0.02145920071493368
0.08917258397069813
0.036186861258389234
0.07146295023413518
0.014608286215602687
-0.01307175157258253
0.03786497613026004
-0.017455269725518058
-0.0027343609018070526
-6.942071881339482e-05
-0.0644459792634169
-0.005231472714370664
0.0416185902073219
-0.022433891128663355
0.024418800997782018
0.03907249843984731
-0.0452407171826133
-0.06502899951978862
-0.055819115521612644
-0.07625892572882358
-0.080677140186582
-0.05130161144798494
-0.09486795348839759
-0.09367382294713504
0.08386194284769122
0.003220695275873535
-0.02103206485580249
-0.001483809206897932
-0.07352426762055868
0.03977623071616322
0.042823114658539566
0.08253362211397378
0.04238598658635449
0.021286796560941627
-0.017688556398968255
-0.03750997879848576
-0.019434855469908873
0.015073909868185872
0.048311110359946785
0.0210134762587563
0.0631479891712641
-0.0218107769418732
-0.06153150268459154
-0.002709875535530438
0.04374392212915295
-0.04042001177348605
-0.0020905841390680153
-0.04370203619156743
-0.062404861226478744
-0.004083299701691211
