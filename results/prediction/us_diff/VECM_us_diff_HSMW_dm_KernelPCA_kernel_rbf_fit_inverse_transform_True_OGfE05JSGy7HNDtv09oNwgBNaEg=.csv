# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; HSMW
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=7, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.1688368224806998
-0.009367899941277187
-0.08259323447953457
-0.0003482212594534684
-0.13610234705673513
0.07037037295097982
0.11344011965205095
-0.026071540900998974
0.01336095099987368
-0.019719623255860432
0.02196851213937872
0.06165613241240435
-0.05208957348491248
0.01597054312688654
-0.02432167424574848
0.07669314389173784
0.04913738467559671
-0.06575622549712079
0.039861735414730884
-0.13228539598927516
0.11057665479765023
0.05086588061700355
-0.03138053767607579
0.043863014866795474
-0.07614457252907346
0.09860220279477913
-0.014087202000306824
-0.06478537731607603
-0.0971383102318884
-0.0292674640126957
-0.07629509245684865
0.11631324704628045
0.16187817962426362
0.06515576413067756
-0.06054769180482221
-0.026169828844242335
-0.008305289690421408
0.0133075764705081
0.12652878442167004
-0.01934291041376294
0.10667889886974673
-0.01724950940676062
-0.0014893985175209595
-0.11155518494625563
0.0908431587303774
-0.16903362529501584
0.07351599652325548
-0.02997536319332703
0.03494587091165135
0.08230216833908457
-0.038090560390715615
0.0021014714149854563
0.011528268400747776
-0.06897239483832035
0.05194659787062825
0.006508962943550682
-0.015182130024841678
-0.03330815067015605
0.007741053031859138
0.012328402966961978
0.058193622678591156
-0.011526471020284916
0.07362243375910005
-0.11702173730560657
0.0017580346532227852
0.015175705286590555
-0.07324996433485498
0.035280443191483615
-0.0019238866667758286
0.05937797539881699
-0.010155972586306378
-0.040418271532180916
0.04784099521141175
-0.036207422103454276
0.14104125027959385
0.040534743354703945
-0.12459227287410171
0.0621603020013122
-0.03219292654439324
0.04817204919584336
-0.016683041848052527
0.08330431321644213
0.009180404141168733
-0.0789004240310723
-0.010465799756586736
-0.05550149065883056
0.0005976644055763411
0.06166433094965715
-0.03497075294079536
-0.011367724193045516
-0.027787512384997555
-0.11441237336722879
-0.07612352961291662
-0.04141897394931979
0.026093469723436666
-0.07453074623648515
-0.04681906373465125
-0.17614590819188153
0.04688752712657342
0.03008075266734878
