# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; Sfygt1
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=15, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.1408838737381657
0.04972375087513799
0.41332347154264537
-0.02859556974864602
0.1173681322194662
0.20376102027312049
-0.1355449701326186
0.28877725310365315
-0.48061153115995164
-0.001357419155129394
0.06834423167695629
-0.16743845644061311
-0.21363803515200416
0.142723638950471
0.12507583712944542
-0.07109563222304893
0.08821922241882531
0.12102119811030174
-0.035730365021915886
0.15088338744574042
-0.1883529977111223
0.26408126363182977
0.07205746278514477
0.13096692608732194
-0.21682454537992535
-0.054615885403885796
-0.12228790318819002
0.12320102366199609
-0.058551590345686993
-0.15044957962469566
-0.014890677637405952
0.11573592508255845
0.24501919306630077
0.07517955265026834
-0.08569043661092948
-0.06113182736032472
-0.15434786174634474
0.0326562961794393
0.25548503350164115
-0.13714819069530668
-0.02280163665470937
0.02504537229453581
0.07934310811502625
0.08503079491313087
0.020555093502819706
0.15519573243177998
0.01631824918300903
-0.10003545683764192
-0.16868780302448932
-0.044647454081885654
-0.12979005284610684
0.22707871940428823
0.09710692167705051
0.05550933887642382
0.0974507611970882
-0.11981822461863047
-0.09878085041769137
-0.0017336373300144634
-0.053236002179702854
-0.036510466792677644
-0.07545668730312514
-0.03382437994832704
0.09567712253597557
0.254635207689741
-0.04182120701205769
0.11268790765840894
-0.07304532259644447
-0.06232572415483742
0.019081750292343786
-0.10698340942392204
-0.12884295255802639
0.09827738023849483
0.021412284477580243
0.16135029752204416
-0.0075393209070668266
0.04649008180525162
-0.2577947941472938
0.10414736317388378
-0.07109517815692792
0.15997151443283603
-0.03797124650557608
0.12122089367540834
0.13504741113956176
-0.11101494413512526
-0.07020488993326869
-0.04297617237761747
0.006815972085154696
0.06234103304175087
0.0911289564741454
-0.018846268577213082
-0.00901999037847774
0.024366432991415575
-0.12015647128116402
-0.0579212802747998
-0.1162828895983593
0.0681908544808143
0.01634018712446461
-0.11938602698071502
-0.09310573330579744
0.04079288959371194
