# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; HSWST
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=7, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.02071149042725015
0.02476852641829177
-0.02546519542591598
-0.007146769696986771
0.021055599140350753
0.05949249590263747
-0.04343774213538178
0.09757042067839848
0.03748089270928513
0.031112159516512465
0.08112384054511902
0.020283347902923642
0.005506239689718855
-0.10137287797927974
-0.059798872267285134
-0.04626695878669072
-0.025780177841810116
-0.03196853046544857
-0.10605173732998571
-0.05415571660401251
0.02616044526646931
0.10098495848133567
0.026591322472321762
0.027328598091764918
-0.023783791733870723
0.06364255886455904
-0.03517291635562439
-0.019174035261158254
-0.0951509156211087
-0.13680208717201675
-0.04233133174513493
-0.028135206713949423
0.045647611773223074
0.08915666188294491
-0.015572681223856126
-0.052454988667729886
-0.042630215580130894
0.03164725222298516
0.026534930164237424
-0.1361351971293742
0.07669798685033509
0.04547576700290451
-0.005828567748004873
0.008870770702345883
0.003524764467474531
-0.09585773642795181
0.05764154367392281
0.009649225647382832
0.10167912948866065
0.06833104042581074
-0.02435632857126044
0.004576547402777777
-0.005677735736918099
-0.029169836656649614
0.013500578632108094
-0.019097609850552418
0.0521372325106825
-0.027566799231897907
0.07891035131458266
-0.0030681006382093402
0.012843738807698984
0.07031918031358392
-0.04274656802417725
-0.043623075990203264
-0.07043472727844949
0.006953241338791762
-0.029444177516862263
0.004433004631972208
-0.025410077412275592
0.04089770099504002
0.09545300616783137
0.025965865563531757
0.04533309946253825
-0.007933467624537574
0.04448812009647662
-0.020622438500800498
-0.10876000056077129
-0.0009642331878677024
-0.027770935554114445
0.10017397633025824
0.14960740597647496
-0.045369531072830066
0.004131736179430273
-0.013777714621095703
-0.01415248844432632
0.044459463681507036
0.011161414882942717
0.05615332794169441
-0.04200458999816505
-0.030367495560546187
0.025600760645120294
-0.044151456373258215
-0.07772204733585944
-0.03179709758362087
-0.04077633627940917
-0.06613263792252008
-0.021645514209708446
-0.13381663239784064
-0.07676626986050054
-0.10172490941879186
