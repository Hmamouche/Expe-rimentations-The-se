# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; LHNAG
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=15, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.009703796193418412
0.0150544319569202
0.006409860675492747
-0.0031580848746403925
0.0026448940294666427
0.006413654463656701
0.02087741629003336
0.003535367980343815
0.01008056633833437
0.014940609343086875
0.021377977951707032
-0.003681649474986251
-0.001483724026845586
0.0061699496882482024
0.014757766936445177
0.013317899424832042
0.008375484587448141
0.006467570497703498
0.006867817990542127
0.010855882876006086
0.008156465742597062
0.012307492274640825
0.0044386772508633425
0.000944801057674392
0.0034709796944866513
0.004518394478695786
0.008400563107892299
0.006544098292038617
0.0008126446858893882
-0.004641548700920464
0.0011000249643333317
0.003945570950393275
0.006828029026718686
0.0010405163582969616
-0.008266712876829379
0.000318465349772716
0.0046211405782456035
0.005724680771068762
0.00780076526122266
0.0014984051744044422
0.013043802154954775
0.008477666142326507
0.009857518097094179
0.002355541267216368
0.0073073064339600384
0.01027021900244875
0.006325085486861033
0.004194506359568539
0.004862184704772249
0.001312403765776324
-0.002961209596160717
0.00468891455752277
0.009067646167863217
0.009475113059820801
0.011379400787187188
0.007618837387988193
0.008565521390927758
0.01041023553579848
0.005234467704273277
0.004063962194150011
0.0006236339315416543
0.0022731532553517475
0.010261558142163405
0.004321424671600236
0.00915941833204418
0.007973079709073856
0.008580880603908466
0.00604386970207212
0.007959931195715053
0.015244948824291123
0.006682418262270903
-0.0019212612664149752
-0.005094952910779226
-0.001051475287742347
0.005120745814181225
0.005234912208367502
0.0007076617866624609
0.0013813633208005819
0.0005636746859462585
0.003843326439351419
0.0009321519630736573
0.014197898996094045
0.006963984842923604
0.0001897982691431079
0.005477432490708922
0.005220115511121632
0.0015039208230367852
0.0034075636936609574
0.007248327691297676
0.009921307654759515
0.013167234683268619
0.00213251803502605
0.004085714329998691
0.009843637092819126
0.007203808715657549
0.008725466509195035
0.004369287345987054
-0.006206395308818925
-0.00344232168797809
0.0007100687458773869
