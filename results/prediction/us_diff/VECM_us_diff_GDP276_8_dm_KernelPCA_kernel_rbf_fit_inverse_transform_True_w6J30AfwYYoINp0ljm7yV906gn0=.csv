# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; GDP276_8
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=14, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.008683297676674941
0.0005526082147530281
0.0025297095144504907
0.006274075885808513
0.005009874101294536
0.0030489318497524532
0.012997825197089141
0.005290353578201988
0.004863130470641614
0.0071654853275454474
0.009936672744624214
0.007145942400816158
0.0117245777660166
0.010765009801890938
0.0010854536410773432
0.0037848039226822993
0.0035216353339166777
0.002115595033101548
0.013286855204424613
0.0026552058619828942
0.00747774150433373
0.007833047195745527
0.009183473352269688
0.0005239555223429599
0.0046948659842534915
0.0057738110969061
0.0019376951916822436
0.008363163834889855
0.005994777827451154
0.009244282486167153
0.0023744147310843667
0.0014260790733218952
0.011939114592319952
0.008877508890438243
0.012183261252638942
0.005364042086311439
0.012714352088377925
0.006873985993984965
0.005240719692356564
0.0070679173107857365
0.002174870747836141
0.0052650672209047155
0.006310420265056075
0.005579931076768663
0.005252168605807551
0.0020784597803407473
0.003019783445908814
0.0025286818147717527
0.005941699596356586
0.005908130005517516
0.002275067581355257
0.01120777034463169
0.00580707639062852
0.005661773072498504
0.010133351060483284
0.0056969514106069725
0.006606079446761776
0.00751988061232842
0.0018606470821967023
0.002948506440795658
0.002724911506888464
0.007347335388894268
0.0060551700149540566
0.004951095985891571
0.004027190421148374
0.007206983485896782
0.0037392514007780465
0.002362700699261063
0.0026026848493045907
0.004917414206945883
0.0014513120857339902
-0.0019630782511630878
0.0025625349318461788
0.01268075938830078
0.011119195559142193
0.010521287392729155
0.005263120263722294
0.008348728972588473
0.006341929796754039
0.0022981844342855476
0.006362744523376171
0.011582213744290377
0.009894220679072691
0.01081460597781916
0.002311914249275463
0.008973838284409404
0.008304184810363218
0.008283439748708583
0.007290534018815939
0.013541513814714565
0.008317245440035302
0.010684169111837095
0.0068664255374956615
0.007819437144188859
0.007181082473772962
0.013835260590649615
0.012610584003002653
0.007540589016578253
0.008706181268631097
0.011626384863358336
