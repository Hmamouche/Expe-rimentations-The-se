# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; HSFR
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=15, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.10821668063056449
-0.03299391408020236
-0.19505477809753685
-0.1077977262729452
-0.03239154125198931
0.015133581439442434
0.009628100754219493
0.02985808961017465
0.05970105537259514
-0.050017535799855906
0.1780681620913145
0.1332202272045211
-0.03029454807852706
0.06993417309091295
-0.13037136266827076
-0.012737073562768153
0.1492059896300984
-0.106352222203689
-0.10825789959820734
-0.18820255192948146
0.03325623712287264
0.009851012156209149
-0.062193254466678775
0.004093869430204081
-0.04623777219457887
0.022980628781816552
0.0695990943786592
-0.09241080684567811
-0.06775846542743001
-0.033142743390618214
-0.053056516348639546
0.0180735484441406
0.00031183217348143555
0.05396136493612755
0.07408324244193476
0.07012915889572552
0.01897397268429577
-0.06619334714936918
-0.011118508544167443
-0.12869573394043474
0.04187081032698198
0.06265921831158021
0.09415390301958079
-0.08020410194317994
0.08335381375645132
-0.110469384691567
0.04094242026116301
-0.02230544153443071
0.019850084030930387
0.0335880487223779
-0.017683262625464335
-0.03759035799415704
0.044090110240765756
-0.08944288941794912
0.04025007203761778
-0.03254414897064241
0.08407059503829782
0.00760711484580958
0.02619307966086007
-0.05779528931554184
0.06326509623598545
0.004223812335048168
0.024609791026174218
-0.06485761461811723
-0.027142638660831324
-0.014759564049346294
-0.015237960398646663
-6.198048021645655e-05
-0.049098999993697524
0.05351177040957207
0.07264102414208572
-0.03846227431479147
-0.023889285807095543
0.013002664042468571
0.0875902738575029
0.09451554663365919
-0.04532383833825663
-0.029907533599413498
-0.07911685947555663
-0.019297539090997157
0.12733228787190873
-0.016374190653891355
0.046401184218239556
0.008827034956026054
0.00406741813541088
-0.035374676111050884
-0.00271975242592643
0.04991021207980942
-0.01846868686029399
-0.009482793181994565
-0.028004558354391786
-0.0602509736566352
-0.04429086228916081
-0.024032610619790215
-0.005535406433782322
-0.078538072752179
-0.045343700228010014
-0.17711153586011955
-0.015641450246155486
-0.12173727255667433
