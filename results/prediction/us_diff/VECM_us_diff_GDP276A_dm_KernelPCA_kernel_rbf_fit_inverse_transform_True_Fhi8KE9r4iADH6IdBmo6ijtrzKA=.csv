# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; GDP276A
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=12, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.00641535435241919
0.004357983742169364
0.005312140702405226
0.008384550938928623
0.006085469011464717
0.004762827292281972
0.008530320457656039
0.004089825572214474
0.0052160101277241785
0.005478224440553106
0.008252604831364358
0.0076702847662211086
0.005973459074526314
0.00688920017337742
0.004627866283904941
0.0033911524254967053
0.006005656348499384
0.0049812176613278384
0.007706827371541649
0.005766651362361521
0.01008062784726916
0.007112502683492507
0.007009405385954532
0.006827121231137158
0.006166300918459813
0.006312277111589606
0.00662184191880732
0.010208590527485401
0.008632400681720604
0.009313126454740132
0.007019827063987726
0.006037177489310445
0.007004889675716569
0.0061411370859925886
0.009345890379349192
0.006240185735485663
0.005998185453384893
0.005485426210724311
0.0075713046448762375
0.005227417283457578
0.00812924726413856
0.004521271307882589
0.0046737577429357725
0.004884563759542394
0.006854833016383507
0.004531829413569282
0.006189410171056238
0.004284388299412221
0.006299889920875574
0.006920189012559245
0.004237218975677755
0.0060976545442803206
0.006155199555955852
0.005690991048657103
0.006324077443372556
0.005637623851170239
0.004533882104923914
0.004817534106654495
0.004453680623618806
0.0036274114078122584
0.003988608625559478
0.0047659564171303745
0.004827969494012789
0.005383778889643003
0.0039846198644280665
0.006444531124644383
0.006028331560785989
0.005742037413821431
0.004463748076098887
0.0047077783057132465
0.008119090449425303
0.005619600434288082
0.006095917630780525
0.006984916254416178
0.008591474153500988
0.006479126713092829
0.004589308511600446
0.007013370996078246
0.006810799845062441
0.005933417737872017
0.008040822509953895
0.008475506118487363
0.008353391748589971
0.0076380622795741045
0.008302138454147328
0.007167371356632731
0.008408588051154643
0.008744882744991522
0.007903134555259
0.010655308580399946
0.0081263855501848
0.009370406170378254
0.0084106500973657
0.007794291890815742
0.009884158319376578
0.008632595524929771
0.008430229446385346
0.008617967364348436
0.008623929507115904
0.009835048317330133
