# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FSPIN
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=4, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.0038897497672146816
-0.0025898850803734346
-0.001331388363032226
-0.004796920690484999
0.0007011903229026087
0.003397148067831695
0.005555828601977969
0.006445439870817758
0.00429778219162495
0.006013334490547274
0.009812490229521896
0.010288420697699818
0.001415911754752043
0.004887098304654834
0.016269948320612956
0.001680976750555768
0.009758396984919121
0.011145323901394506
0.0027066894657409614
0.04032109366929906
0.001961016581100708
-0.024097260544740363
0.004815687795608881
0.007507583944406151
0.012209842522743469
0.0129442147760435
0.008635887395369005
0.008898149067820806
0.004203014389143821
0.002623139399943422
0.010178466375024847
0.0010725080502655736
0.00020450200109353397
0.005659485934591842
0.014205299949281628
0.0036932262770529058
0.0012512882894315795
0.008063045493294837
0.006801040959134085
0.006240578075198127
0.008482697137606958
0.006379707959528349
-0.004142550882678499
-0.00936343092013417
0.00430646423324857
0.0012858230331957386
0.0016177961227684216
0.017917303050816905
0.023331291538390624
0.023008502205916728
0.023463042048027048
0.017702109619515953
0.01169020029225323
0.02007679813884727
0.020461074757994007
0.027754316193221856
0.04175629249763696
0.03630504476237123
0.04903311240702759
0.031967165132910924
0.05426875413909819
0.040657117001143855
0.014905748834149482
0.038400236987350865
0.057151770388972845
0.04822345135406436
0.05335207413641585
0.028283655606044667
0.03957666613900936
-0.03627400578731089
-0.044844806558763446
-0.06818301225913836
-0.06778024613052212
-0.01981681326189608
-0.02537707302475674
-0.024080249088081228
-0.07711644126350345
-0.019874924436787816
-0.058509360587450626
0.02008691527865336
-0.007315171583846661
0.030718838900178822
0.031212266395715976
0.015550863082027421
0.0070483165761967125
0.02664415963485576
0.025218334528625593
0.01678425352092724
0.016006292959184
0.0004709247434456221
0.029223186621019706
-0.0017043509095413158
0.003617458514359636
0.037055180794429717
0.026886026156102985
0.04738822186795167
0.012761887705288084
0.045409269938661685
-0.03552384240438135
0.009202423365319127
