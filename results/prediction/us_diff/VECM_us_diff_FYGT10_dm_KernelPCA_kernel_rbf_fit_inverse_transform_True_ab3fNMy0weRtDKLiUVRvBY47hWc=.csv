# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FYGT10
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=9, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.10569385906519549
0.13182435252097008
0.09956070674737227
-0.024377618764649677
0.0032682838501880956
-0.07408956911820788
-0.09167465647644646
-0.05492279584630833
-0.029249160447880215
-0.061695105843912114
-0.06734656848586999
-0.04652924292965418
-0.0792011330141
-0.00976931088803907
-0.03225335592207891
-1.595929679045968e-05
0.07949054866395938
0.03970519567206987
0.05645109561541682
-0.06743814965548964
0.020459471999231732
0.08588218906433029
-0.005315115718809771
-0.04334248354274613
-0.042922340232907155
-0.008695758859178521
0.007465308390157615
0.007160764286955831
-0.033606112238135685
0.005694678017696176
-0.03274602087880614
-0.05664432339087963
0.054739082106931236
0.02498739092828712
-0.019100890777025203
-0.019665357408367307
-0.06190440182450537
0.0098675441698548
-0.015456399973248248
-0.04655728436584996
0.00026956088480819575
-0.011367533000182608
0.03354854341453953
0.031865750560195985
0.043521041945995195
0.08840545298727215
-0.04801459894154548
-0.017685960167295952
-0.04210811671679951
-0.025377811267106136
-0.004173277841684156
0.03939035134196395
0.053827395128613814
0.0005728071432083738
-0.001900394550285386
-0.00046103370732425084
0.017258206962028226
-0.0200846373225881
-0.018859263943539588
-0.021485992735485397
-0.002556893217575556
-0.05602954872370928
0.07228007227812319
-0.025165806825264852
0.008148262641770685
0.05225706716853537
-0.009934113882994235
-0.059642398527337256
-0.020225657793930582
-0.015921892208568048
-0.029937132866390666
0.0132241693288457
-0.028330436032285758
-0.04861496829232952
0.040416282076858545
-0.0005790009367801288
-0.05256123506628659
0.017003319149408662
-0.06881753639982205
-0.006904015334352441
-0.008969885440033475
0.04776365032586154
0.03324576989359328
-0.024643323184695344
-0.054667563433297144
-0.009716818593638703
-0.014761007129047465
-0.005893345052519847
0.031382004745794265
0.027748211812657013
0.05452775880847916
0.008148888326481266
-0.004306380642893865
0.0017571968323739293
0.010543614040201436
-0.010801290632773825
0.021008368904135197
-0.05587335863564152
-0.06906341433861105
-0.0010599495172181356
