# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; PMCP
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=5, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.06448619180129381
0.0409470524581316
-0.039816331852939445
-0.04637396563827075
-0.0657176443136859
-0.03374162049663242
0.050687098670699246
-0.05278263424974734
-0.064852931630637
-0.014271560251224674
0.018290474110430045
0.07022406811340802
0.02295556086444167
-0.09382636260925807
0.041810716235534506
0.04716973868273931
0.06785493357184193
0.10040270300965075
0.03379658543534472
0.05731551107535929
0.08840058032150956
0.002073351963967414
-0.03529398692351299
-0.12775086155868998
-0.12386805886296996
-0.08983053022085362
-0.03769247344547802
-0.04033719902806957
0.00559039612884631
0.014996196377334549
-0.004442343960413608
0.11133982408832631
0.08762924042202722
-0.004402365190157315
-0.02970927281865968
0.060965515819451116
-0.03148654939475831
-0.010912572990532408
-0.013312816190486864
-0.12083327956540782
-0.026675336867986642
0.07161161302569674
0.12253974325369205
0.109290870265391
0.06349989668903945
0.050682680815237136
0.02371001175716584
-0.034252384363105834
-0.08536003252842111
-0.15449056897135566
-0.13065214987514231
0.06986303683870293
-0.03723963629929655
0.04994512916415971
0.05559666417273906
-0.027125415592579402
0.06584210607863465
-0.028038093310708438
-0.041753079401271355
-0.08930183134838268
-0.0820418887181589
-0.059670296941769375
0.04394341195985306
0.07510507492181073
0.15209451112776873
0.12084616847349389
0.08002418398954786
-0.048651551924845965
-0.022259075541459793
-0.10359076067607996
-0.04111299534177035
0.0024034713104880284
-0.052654632081798894
-0.06475836793159864
0.16717898391076225
0.09318761419746109
-0.03106455822850775
-0.018271169151846955
-0.02658230455214164
0.02922170415393731
0.0853598137617038
0.11543245864831869
0.08263363774664104
0.004032084162220718
-0.05035381909316353
-0.013275849222202873
-0.06763168688834056
-0.0940615100279546
0.02259389470011628
0.027597556220029785
-0.031116604366403476
0.04004544704082004
-0.029576554332473196
-0.12385585786634665
0.06654995212011713
0.03031121291587912
-0.03377677687590446
-0.029189922965062235
0.002219888185228596
0.07676104941745142
