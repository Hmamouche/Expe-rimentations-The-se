# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FYFF
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=6, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.08577022177980789
-0.07400209056869267
0.047081087149878145
-0.014115025541319642
-0.019721376376402817
-0.06793958092353684
0.04797200448000839
-0.04300213395804387
-0.04151166100039763
-0.06698665167564416
-0.007238331436023761
-0.040232158737825714
-0.037182663455741886
-0.004160618708089463
0.006792805522872241
-0.032148129714541396
0.0007244089490328204
-0.01503321699742434
0.052257207011411594
0.008851788614763413
-0.011468330652973835
0.031704810784995464
-0.0052466867129153626
0.001696214271210271
0.002953686704619686
-0.002985287279114044
0.045075068280921296
0.0023188027967811393
-0.0745210829719177
-0.0031521449179873684
-0.07492018735237159
-0.08828412305949371
-0.006583336395873015
-0.004950213298227545
0.04780574548610639
-0.032561694307184384
-0.07120867737868006
0.03010582108612032
-0.0536447496591261
0.02427198732295595
0.04719229473109354
-3.3561514315281647e-06
0.027113527599124058
0.007639271407357725
0.015598192719482583
0.07524553899740619
0.005432982137974319
-0.006533140990622067
0.01832414160880414
-0.025129470892386516
-0.007077529549611424
0.02578200010671349
-0.017802790396955002
0.028826628252908926
-0.004551728841252852
0.0108700752545735
0.016235576787977792
0.01354259998170027
0.008353479472318813
-0.02538038215418606
-0.05939145125438085
-0.024438987326695874
0.04525535261460689
-0.04906657945055938
0.03069593002907756
0.016739316825814973
0.028093667296097587
-0.012424448805631053
-0.007377338477559361
-0.017629486076528034
-0.03819153419565426
-0.014480430096105586
-0.08462243583453095
-0.11168919883935922
0.03365223372828861
-0.07748178204166911
0.052977995444131565
-0.044255946835322205
0.005733019561596499
-0.015988789402568322
0.007994693370137064
0.0384282698342129
0.0420869345938932
0.037414111608447376
0.0019804945204582244
-0.005706026855428925
0.005046851898506798
-0.01014872835116645
0.040157104374457825
0.037448066350922043
0.04822975622335472
0.01728097800512413
0.010603217791917504
0.000520072928467152
0.019931755732438904
0.010715597032518587
-0.009969477348599395
-0.04376270934591155
-0.04986289927398764
0.004314209333723562
