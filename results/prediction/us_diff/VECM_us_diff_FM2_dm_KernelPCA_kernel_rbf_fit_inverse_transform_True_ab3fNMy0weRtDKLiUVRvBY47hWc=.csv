# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FM2
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=9, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.012675307549620589
0.005328232161644575
0.006528463984077947
0.00355560042341681
0.006316653818682046
0.005435179515638601
0.0056536179886988855
0.008369535467792807
0.005655202724849228
0.007152793239938908
0.006244356384754799
0.007168004411516491
0.006315209438325892
0.005783395911374426
0.004638028758336555
0.005257943586108356
0.006434139493605688
0.003980917589417405
0.006094873012876416
0.005090489542121195
0.0038599865254206524
0.005599389664140861
0.0037102030706407554
0.005820877272727185
0.006327555595248586
0.005655856567690614
0.003514770891384697
0.003248467953043007
0.006395328805336976
0.0034624106458350963
0.007146965823042456
0.004597258805004132
0.003726332774171657
0.004620907049203557
0.0038717777367023474
0.001795672533701067
0.0014368638669272358
0.0014486804270849552
0.0006155908748661432
0.0010540390289566983
0.0007372257381551257
0.002793721863559722
0.0002828841380098344
0.000969555926192074
0.0018382969961715516
-0.0008884825871149839
0.002379407945255502
0.00365885751438953
0.005270487312300884
0.003862464183193007
0.0047696109564561735
0.003321442695718359
0.006352040337783851
0.0058616926411765675
0.004646929694483143
0.006086387992108906
0.0078288103909738
0.00672933949096337
0.009620832622081984
0.00833877884645237
0.009687772096810352
0.013034022663119418
0.009401734732889305
0.008846896908761873
0.007870326382696958
0.009079572586064087
0.008675671497875227
0.010935600087244018
0.008527719244043587
0.008823224425694907
0.015237759957924278
0.013071513005997104
0.01375431856748917
0.013819886515230582
0.012480258431646931
0.008979847204318872
0.015549164795723892
0.014083452187023076
0.012385250553612245
0.012615893617398722
0.012698853014519397
0.0058643618712363845
0.013557983394772075
0.008348800122226767
0.007809752073122853
0.01249538266262489
0.0032007639057225943
0.012965863546458286
0.008643116940293509
0.010946916863280151
0.010976999476337611
0.0057371954341555575
0.012018301067980193
0.011764994861183212
0.011040522800897078
0.013791336286044158
0.012805614660010895
0.014835747567634274
0.018934592100269345
0.011077633645297116
