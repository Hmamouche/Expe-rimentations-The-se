# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; Sfygt10
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=15, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.27405333333569715
-0.03495300245656878
0.11860744085163256
-0.10179374668470778
-0.04960948045459743
0.2118955464721224
0.26879075064240554
0.26495652735437647
-0.3720648649827188
-0.22256277830253154
-0.06248844311794737
0.07616861493185503
0.002734287956677725
0.07684508478038805
-0.10985195698202088
-0.23757679478193533
0.2510196544861134
0.21386863640221915
0.17020214067466544
0.05763026847278478
-0.118815632559749
0.04241047548638749
0.1626284274300517
-0.09958767192641463
0.021243584421231324
-0.08955734970974176
-0.1248147961207745
0.11322566102720874
0.1322390639344964
-0.10471136377549761
0.0640170862123037
0.12937841629963456
0.17132798540708036
0.05319972916009723
-0.1045439847048973
0.07126181490179301
-0.02599205977458858
0.26172318030667646
0.22304826220375215
0.02675004803467159
-0.11024559023041305
-0.09122475404127087
0.0299944139578793
0.007725595087623712
-0.11318918976453934
-0.0037631142898536113
-0.038320922584034356
-0.1344329678330675
-0.1833883154125106
-0.09616216375466818
0.0446761650126726
0.17575574305814923
0.024505649679541053
0.008144605108659102
0.01876238424579814
0.027844929188335824
-0.0825582606571403
-0.1886404935249336
0.00575091347018708
-0.052893027601028
0.07999412562807179
-0.039224049723768216
0.04206228187251167
0.13781069411530858
0.015967676223572865
0.047949084448902876
-0.018915431930561256
-0.016011508813235417
0.08437551781232219
-0.14487024498418405
-0.03666168769435389
0.004744975470474012
0.15877395254663781
0.2745815739185867
0.0919267264787479
0.14800258791518733
-0.25663550608566116
0.2486622038642748
-0.060049414710526404
0.006931314073909928
-0.08762175511913275
-0.07812498316621841
0.16976254804544372
-0.06635875695654354
-0.0778317484613488
-0.033856539169774144
-0.10602386191283641
0.08597629261047114
-0.08500341476958317
0.03217879148137166
-0.1101936162443287
0.0034755992148197967
-0.17188684522341133
-0.13205067059374467
-0.0696802087874757
-0.0930820753522032
0.030556661927136075
-0.0435079504935826
0.15298382269477928
-0.017240492314488237
