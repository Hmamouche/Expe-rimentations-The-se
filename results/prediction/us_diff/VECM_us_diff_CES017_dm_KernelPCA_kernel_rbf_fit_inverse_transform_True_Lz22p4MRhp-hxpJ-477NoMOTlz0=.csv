# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; CES017
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=11, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.01488326133890907
0.10925450298884685
0.001730969651153564
-0.008553918597602395
-0.06653042620674213
-0.01599633818137066
0.0007825531168912233
0.025051616279147322
0.022646594949913817
-0.007027672822992426
0.02357439107636271
-0.029107224152226517
-0.028894951090499346
-0.03911839026201697
0.026788069562067567
0.03515822386404016
-0.0017087652434284034
-0.019280189735946782
0.02857175577293293
0.015390422220448743
-0.0036090569734176844
0.06594034518624584
-0.042274885916271585
-0.019593914084583677
-0.04756776779354233
0.0011852038233739343
-0.014400300794177097
-0.04079133857437575
-0.028283803815706075
-0.060000115907476226
-0.07806262932623524
0.0010478526430414358
0.03481350632198044
-0.03336215667240877
0.006816202652379413
-0.01583987292997087
-0.024963503036611472
0.016121744503155035
0.014738762660447033
-0.03800729702907051
0.027346833950964308
0.007961490446927564
0.0333036779864645
0.020724460517330866
0.03068069657764836
0.04137676653089051
0.0038006903177479207
-0.006605526779778392
-0.008807956778941903
-0.03959137272671778
-0.015172306760522457
0.03051737179191934
0.012923782430306337
0.05391626975302377
0.01710595569345895
0.00328148010666913
0.0356746373661802
0.013415434723663104
0.005526049269532812
0.00265677854966297
-0.04165831793129058
-0.0059891435159889084
0.013392784737301287
-0.01896792574680064
0.033816418885788305
0.017516914432880303
-0.008633745583434617
-0.017423919593144772
-0.014515869439504572
-0.011925625599814324
-0.041203547779175224
-0.046286286228556045
-0.09057194960275865
-0.09130105071025417
0.006387982553351471
-0.027251849420411253
-0.0070376553657798616
-0.05363046717462269
-0.06414825464904052
-0.012449526210842755
-0.014412982534489936
0.003402492377427303
0.004818036248310237
0.004498609490132506
-0.007357996770082274
-0.012346085825995202
-0.01068469874174559
0.0017226643462793989
0.029958149809868877
-0.003859032875007686
0.026177926409753853
-0.004780086031740355
-0.04020919327695477
-0.008669839240081767
0.012015546140518276
-0.024766332227955094
-0.030332298787836293
-0.0322674776217077
-0.039404118344120595
-0.016571543471415824
