# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; GDP276A
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=11, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.0069683316776179135
0.005020565993635604
0.00551813292990218
0.008322224500251179
0.005923872423391821
0.005152029599732488
0.008229669282018431
0.0038504973176557967
0.006469793176982043
0.0060299894445108694
0.007422792076887045
0.006824682922014923
0.0061212884899538465
0.00780604125935207
0.004545205494602021
0.003760216679454528
0.0060192962075413785
0.005341345862132646
0.008501886384354767
0.007494686494404543
0.009414713858447513
0.006772007029209468
0.005840595921255533
0.0044389085560378695
0.006131113607959109
0.006703395670967382
0.00704726955774656
0.00953491240634505
0.008115976265999213
0.009655577095298407
0.0066002691838196615
0.006365652326772345
0.007038339656660647
0.006353813612380755
0.009134091202031097
0.006180849237780051
0.006230027856872742
0.005147631286857794
0.007466931958808281
0.005607776237340669
0.007791116141884104
0.004358261365882495
0.004744150871567411
0.0047771894879852575
0.007194701065601137
0.004617747989098915
0.0063903067155370455
0.004483948953829526
0.006628008378124363
0.006992044688296706
0.004084675057753684
0.0057123275800225574
0.00571499342792424
0.005275933019751361
0.005964591600028438
0.0058039231667981386
0.0045118411283542785
0.004929875940570348
0.004465551032399571
0.00342865452520105
0.004609730819345083
0.004612032451967877
0.004560204984389047
0.005135427027496632
0.0037500664617881744
0.006439119010021017
0.006195411318331698
0.006233954731605906
0.004209278410472696
0.005447783931558118
0.008667971890772569
0.005746506048180373
0.006616414780405584
0.007181871948497287
0.008216987313414036
0.006241208954794704
0.004891143478230843
0.00677104837766356
0.006976629413353541
0.006554121307030352
0.00811589310867146
0.007953977024412393
0.008268596366394525
0.007395722782828925
0.008483608623780566
0.007522015054844682
0.008444333983846445
0.008350186731569176
0.007359478059546776
0.010679503137344224
0.00806805502601083
0.00964708547080129
0.008644075163356146
0.007912550431959387
0.00978649706149538
0.009071568872341259
0.008755030366586824
0.008575047347427684
0.008770660696434044
0.01004676357456111
