# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; GDP276A
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=6, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.006725969178541696
0.0044118157596020145
0.006445728574204893
0.0072784660948659805
0.007296855920866249
0.005311701709900937
0.007368985379564342
0.004621507382291503
0.0055549259176033905
0.0053456140451570485
0.005884384359358605
0.006198565812845004
0.006387362881715376
0.006421190813910003
0.0046115583910066905
0.003690825558188753
0.005329118350054916
0.0052466505847161855
0.00833117914216032
0.0065003217921575755
0.008786454065049652
0.006594893750134391
0.007794732365681778
0.006957769624452045
0.006615759720916955
0.00645305109467474
0.006900541593940441
0.009053506319707394
0.009007425053321737
0.008338310043257113
0.007005313553362169
0.006507316622279332
0.0073680884456298265
0.006342434964789336
0.008105861944888665
0.006595245957737974
0.006238851242665457
0.006443845545986784
0.006634395201568765
0.006025041736874315
0.006379310816095035
0.00433596843627282
0.00482261449451164
0.00543388142459117
0.006754083394129926
0.004810657360957907
0.005988120475513825
0.0050320947643825815
0.005980513757890585
0.006066780829883875
0.004478112541076011
0.0060653875232941365
0.005838910221726775
0.006003892300477151
0.005941242193559508
0.005764658980089706
0.004625845344739053
0.004527188741653986
0.004682539697732799
0.0037083662517418316
0.004449282729802665
0.0046642962659753495
0.004028001952530654
0.00534557561924705
0.004517448614045462
0.0057012157045738905
0.00610225848446061
0.00585131238504264
0.005098150278019418
0.005052674241825209
0.008307297206150895
0.006337911624393797
0.00690946904744082
0.007277726694239321
0.007262494873520524
0.006607272483743345
0.005310993362330486
0.006129079186861809
0.007187757240408928
0.0063252636190663175
0.008763705586859532
0.007261614274068166
0.008058381224346167
0.007829779323680185
0.008081369754854323
0.007106861288496259
0.008329748230325956
0.008480161674509485
0.008220899750426818
0.010282426287649653
0.00831076340292627
0.009689791360457925
0.008432535882070321
0.008114493451300879
0.009429912746142122
0.009129276752589583
0.00836934345141023
0.008554248632873908
0.008416560998286034
0.009926321482724123
