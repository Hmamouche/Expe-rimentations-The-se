# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; CES003
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=9, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.047188061502080644
0.06283552940083723
0.02458587672588305
-0.012508977435342086
-0.0006190484160212215
-0.014123165942764186
0.045860747866724325
0.0006119542687259474
0.012175373225914124
-0.007402382204554551
0.017486406146768434
-0.030370384592135057
-0.027236274837927534
-0.003487550123024456
0.00989818173155333
0.035371259241410796
0.030624287098206866
0.002433457472642495
0.015135727223674001
0.01965519390151678
0.027650138968262405
0.04898584570286286
-0.024360718754330435
-0.015284895586999778
-0.03769047216551018
0.009160579550937464
0.011843727798577171
-0.022501417712509502
-0.03345545356895298
-0.09227825360087324
-0.07857176101694639
-0.01799663382551549
0.031003185162275362
-0.02038254802117658
-0.030119907245383336
-0.016407167325317832
-0.03762470726879939
0.021002580372319533
0.014707421287176982
-0.024077478406176328
0.022301981164513672
0.021476536953796005
0.03806607538427806
0.030381574775348207
0.022815013821531883
0.02585959128230847
0.004535310367077158
-0.005715683724937323
0.003072200581417002
-0.024366570146784228
0.02144876803596905
0.03854296629422552
0.014768099509001553
0.034748757786892875
0.024755255971480476
0.0002490783184429645
0.03002780512850328
0.011853885480625121
0.0122812502533853
0.02298927453033963
-0.01596048950991822
0.012499603789203164
0.02162720715409818
-0.0042399466939454945
0.037639167477950375
0.01017996993064952
-0.005025383749960168
-0.023539280212271807
-0.0010435005151928503
-0.004804543259602873
-0.030449495465006728
-0.046120511312864695
-0.09503975847494119
-0.07406914661402975
0.030926715952029935
-0.030877124090663365
-0.01923607926393078
-0.04923494756956946
-0.04840412134494913
-0.023539234450836992
-0.0010918072484156424
0.0031739006019987403
0.006115386995928403
0.003474248534133706
-0.008699594043769864
0.00035557051806055706
-0.0029315372651784136
0.008011833843496184
0.028514642645834004
0.00767600003774094
0.04130397233600304
0.0008771934204788176
-0.01964678007205443
-0.010650815664081999
0.019577545047117156
-0.015202873828052944
-0.033809855804734015
-0.04265962376114
-0.040530876786964605
-0.032735814728218904
