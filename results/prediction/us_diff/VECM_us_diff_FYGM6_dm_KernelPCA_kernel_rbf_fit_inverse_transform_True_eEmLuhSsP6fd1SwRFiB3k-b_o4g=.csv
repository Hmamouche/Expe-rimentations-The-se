# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FYGM6
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=4, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.11912019385929555
-0.06764884693068138
0.06930836154767295
0.030757065794422656
-0.032747846936831526
-0.06590898747384828
0.0763268862700356
-0.05794627181009654
-0.05065880049218763
-0.070120313664495
-0.034329858412760664
-0.009414465891377
-0.04867034724792191
-0.04634975299620544
-0.021739863125260012
-0.017698680540508077
0.010781278226999876
-0.002509611977150454
0.06501183815917172
0.003502294225348282
0.023920054117259885
0.010749980802261413
0.027182160389694277
0.007644899356466454
0.0035346521540359603
0.01892397154541186
-0.005970422895116578
-0.0233365069679708
-0.07013132395284886
-0.038830420097858975
-0.03549030419693827
-0.01305146038914837
0.01760393766000589
-0.020386132992692776
-0.0013340226254601184
-0.0471149491911874
-0.05114078824296136
-0.01616033170528905
-0.036859047215525825
-0.02796932853079953
0.005903045666284145
0.007213558153977596
0.02702666529177098
0.020075189747703632
0.021696522918245465
0.0639878144685535
0.017183280312966953
0.019718486490230835
0.020035467269751184
-0.006915472059671541
-0.007709362768275373
-0.006364100802694353
-0.017062835884094747
0.04046403590505168
-0.002930006361240311
0.02408531037668259
0.02932922545100944
0.010779181622376295
0.008689462113501785
-0.028178232239291837
-0.03181027248330938
-0.029127821484864584
0.03451695161928686
-0.042149624437004
0.008077802189547928
0.026722604341830866
0.029773071873662325
-0.0032858562032158723
0.00028042274186845795
-0.01513755943282524
-0.03625723625768593
-0.015026521559508838
-0.06258813727655912
-0.10329710195338421
0.014171893231835535
-0.06999467538161018
-0.0034430155592340885
-0.04739738995741559
-0.040335807692259334
-0.009241080622673247
0.011077537626757653
0.04658916585618565
0.03323943635206499
0.007240144014458282
-0.014345551505762787
0.00683636201535553
0.01023597520359305
-0.0022725225786378446
0.0448125472339634
0.024885250753894653
0.05056809604528252
0.01813985018021569
0.017706287467353622
-0.0022268653480280592
0.04398524153719839
0.011176200273402284
-0.0040496431578954765
-0.03272251138559523
-0.0581730848406609
-0.036794098878536394
