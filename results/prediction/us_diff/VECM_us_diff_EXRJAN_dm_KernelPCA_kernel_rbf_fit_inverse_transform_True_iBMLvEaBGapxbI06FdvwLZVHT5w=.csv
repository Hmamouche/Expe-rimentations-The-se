# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; EXRJAN
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=15, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.010897999835616118
0.04492921149068995
0.14536082165493364
-0.06958836728980024
0.035993132985170234
0.034729024365912665
0.05399966275975963
-0.022955059174694314
0.0010174081328937264
-0.08647818703781127
-0.12106645255308242
-0.0599033475339458
-0.0922549391600626
-0.011273511454562325
0.01143572853599998
-0.020826438054880395
0.010851254490417246
-0.043060517073483344
-0.05619991241120376
-0.033934540944462764
0.046482741525041324
-0.05249363012148894
0.03345936882696305
-0.002704877815238007
0.04927949184664525
-0.015079305603732537
-0.024888708706331735
0.04117813401173928
-0.06831357300880887
-0.018773070288467905
-0.001235116922764918
-0.06527643018926099
-0.005845856904186857
0.009366611267229126
0.019206846377346494
-0.012696135109011318
-0.05357119700336061
0.006873394302202636
-0.0442509618472781
-0.04484662691139027
0.04530722176045336
-0.05238201562473116
-8.345921494263935e-05
-0.022747171632585047
-0.020607342899425975
0.023451326746406324
0.013133378917931201
-0.029063449952975537
0.02576114405089104
0.002374752180065918
-0.011187796657200717
-0.014635867039373507
-0.009801269265927875
0.08338924647708561
-0.01073897426205896
0.0011238178017820358
0.0012224793119298399
0.032823066039624406
0.0030106131786821694
0.020369130004571162
-0.026773500977971945
-0.04845446211658435
0.013129944437600832
-0.014555703005201417
0.00431220721086643
0.02252748762104531
-0.011498664483544498
-0.05317607006857991
0.02120847947090362
-0.005829529020054906
0.016528004785616522
0.0020103320842917126
-0.018494949323315298
0.025369169634235873
0.021253516028845284
-0.06373583340325292
-0.005812159522760509
0.008294026201675762
-0.058581639080286706
0.030222452562316683
-0.018761141947871762
-0.01849001225411315
0.004147531412682988
-0.0027370145350316764
0.004955461501946239
-0.002477378199978973
0.004094696264563182
-0.02130951535920695
-0.0011000632397069193
0.047963130540911676
-0.004491681784867491
0.0380538140080997
-0.011270744400306752
0.04638164168941186
-0.01302380937278768
-0.01726409281223024
0.015609204488990672
0.04622130182973706
-0.04057302700930597
-0.0052772251394261034
