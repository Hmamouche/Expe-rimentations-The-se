# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FYAAAC
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=10, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.0271989764664674
-0.03237741258736452
0.08038830820629513
-0.0014073795967994712
0.02859068099126538
-0.07758658974020147
-0.037738488781760185
-0.0343843080424013
-0.015442087194348815
-0.00430465358499842
-0.08567000548283456
-0.031094551102235354
-0.06263868954774184
-0.008082475401484147
-0.04617145099010292
0.004848190342786749
0.030130843359552604
0.004319526661382836
0.06699680775377533
-0.024275383665619477
0.02317898252035093
0.06717155452297635
-0.00492347265344726
-0.06470298000174153
-0.036480147440718716
0.0018059107611185336
-0.010192995609281615
-0.0028534598759955634
-0.02944855220124878
0.01933443330835629
-0.03612626097139987
-0.007167623328024631
0.032482605577499025
0.0018700810865065337
-0.03041707074090838
0.0002769702381153153
-0.05409080207025926
-0.008027639583334828
-0.001931354953681623
-0.03714126536484493
0.014693246656528821
-0.008166653519282831
0.035376996859804076
0.032003803303020203
0.03394873081660756
0.08435625269359238
-0.045723572405033525
-0.004729026900430465
-0.040666866910416016
-0.03243075907344522
-0.048137003736395526
0.028844754424095048
0.007254886168668189
-0.0030819052312339203
0.003117139206538744
0.015795629422197255
0.017963428666560283
-0.027320027325298857
-0.03098921423878174
-0.04288800383480301
-0.013635057950288406
-0.06298576086099429
0.035032435990158164
0.011538324824236296
0.01548377152451933
0.07834054906498739
0.030951184646536307
-0.022720867190334466
-0.0030227348060008824
-0.0243480462392063
-0.028580472103625822
0.004540858464509935
-0.021736335114768787
-0.0364623409525851
0.022658653315264253
0.024057774907726308
-0.09614841217658215
0.04092019088412995
-0.035135117524664736
-0.0002547729918178261
-0.019349356355967753
0.012421453437537144
0.037148972976061956
-0.02984580750130348
-0.031217854828792153
0.00352014238973305
-0.03108868358244085
-0.00483600856242665
0.0122837935317416
0.02726214787540193
0.03162469157887445
0.02634070276372777
-0.0070214192316899815
-0.008310121202145405
0.006179626596970113
-0.016814151826596686
0.0193077841805563
-0.041064251066091205
-0.031256930262620705
-0.0023871612650816065
