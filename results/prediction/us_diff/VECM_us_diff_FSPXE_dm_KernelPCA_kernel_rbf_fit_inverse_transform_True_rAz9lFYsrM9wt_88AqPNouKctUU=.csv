# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FSPXE
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=5, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.01289556575170977
-0.058338661663653125
-0.040000225689888286
-0.04064170913682012
-0.008854403590454564
0.0059566607225392375
0.021021626471022386
0.02659644785287656
0.02519214590463116
0.030960475226788575
0.04540104664033792
0.0390910018072699
0.008718509131174335
0.014171473415769652
0.06547491644727342
0.005474759251867738
0.01777025497943621
-0.035177850975173376
-0.036187549879932376
0.018133125709308338
-0.05605022350561811
-0.04443007519289842
-0.0077332107000959854
-0.008994867946320838
0.011833401644578998
0.035603878849257566
0.013654719418972118
0.028698677683613796
0.014368101478279478
0.01878620745754663
0.06404013988327203
0.019221294793131553
0.00896851641560195
0.027988702994738452
0.04787452741800425
0.025893080269763842
0.022689440242875156
0.034258674262026965
-0.008708010995591185
-0.005796061508471778
-0.005076217278023771
-0.006172655755421239
-0.04411940709268816
-0.05563753163860664
-0.030221979548633558
-0.05289577814196493
-0.03287546669354816
0.0060790096284290855
0.0016141654237705368
0.011530988865717982
0.0260145568432183
0.006625005375447344
-0.006532250199751788
0.02449166812994853
0.00518509549093009
-0.009914073206182988
0.029342203268422743
0.009408018216673133
0.033101686074413335
0.0634528223572411
0.02085760468958519
0.06064964873022563
0.07130240320496507
0.030756766286672847
0.012793564540607838
0.008560319471988046
-0.019526106874932517
-0.044426782098939084
-0.014440907878919631
-0.03909564594263608
-0.004768050886825526
-0.0009278503885510616
-0.009516842018744505
0.1024839480421803
-0.014364251054196604
-0.060454028474929575
0.38326588890137453
0.20692904285217842
-0.010105481376835236
-0.09624542821569218
-0.026870623059481957
-0.05001911686149379
-0.04058958829367865
-0.055500853735181044
-0.10404294418098861
-0.06104244904143648
-0.04257993807516098
-0.018206698713366325
0.008046132270923392
-0.016702932583708315
-0.01226128674476866
-0.048742273714769344
-0.00788779577937036
-0.010682569485306142
0.004912443277259119
0.0016350265913529709
-0.008040815589543088
-0.02499576486659275
0.029692541505707677
0.017625912455736213
