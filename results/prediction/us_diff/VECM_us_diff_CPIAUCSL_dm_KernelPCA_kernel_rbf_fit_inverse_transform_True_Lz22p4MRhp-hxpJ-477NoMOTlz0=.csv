# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; CPIAUCSL
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=11, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.0003621620954435559
0.006311803769905188
0.00674473871784448
0.007994524492252329
0.005905267767251725
0.004498750520326535
0.006722993230878101
0.00480793175240055
0.0037146537028275445
0.005731815989141844
0.004775907504517187
-0.003264289137480839
0.002604025004257401
-0.000881541880382585
0.00847695598187538
0.004754327611451806
0.008578298047492798
0.0060550618594317875
0.0051233191567931395
0.007275355777659017
0.006016752518646612
0.008548737198919559
0.007549518714049626
0.010012206033156513
0.005097048371051049
0.00809051533982703
0.007277420876581878
0.007825980552790771
0.011153928115425964
0.008602760297206474
0.009882481115955521
0.005983253781298912
0.006419945796380324
0.0069646903915825
0.0024819082806706297
0.0049225287117399975
0.005866172687792893
0.007727408017169565
0.0074004136376896335
0.005553220965502135
0.003521836602900604
0.005227728458552777
0.0047946483355549045
0.00802505943963087
0.004508590518681729
0.005623336098505349
0.007830882838485164
0.0038129849506919776
0.0063680004460364694
0.00410596662520952
0.0045180587244399465
0.007534464416006444
0.00573063309661809
0.006463521656756504
0.005486479994533253
0.004916417698168682
0.003639285741091348
0.004024675433937526
0.0026293592128529804
0.00291869312704035
0.002868298559937979
0.004096909757472624
0.004873490136463118
0.004859494409238322
0.0051090393822373886
0.007946080059426026
0.007278751097485334
0.007634916987172542
0.008291197076771756
0.0069235123703672834
0.00749666789100289
0.005539661741539488
0.006725135294244732
-0.0007254166825097593
0.004921884641246797
0.004024016309627443
0.00535470028452264
0.005920722642871561
0.007265944718800423
0.003032280921125459
0.007973098133023541
0.0005899911332041959
0.008788371004731916
0.0065493772415543385
0.007201326713206392
0.008232123979679592
0.007340535898187063
0.008564473413854074
0.008301251680483306
0.010551290879672678
0.011697970391872075
0.009047444764931602
0.008233819483312749
0.003520428573840314
0.008103873880790773
0.0019699299945147805
0.009218724207478539
0.011999482848433328
0.009001559912864634
0.014975356356460372
