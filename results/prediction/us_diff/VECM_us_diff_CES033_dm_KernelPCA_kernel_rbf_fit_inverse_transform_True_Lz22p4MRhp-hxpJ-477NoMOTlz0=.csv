# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; CES033
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=11, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.0061289939423555834
0.03081097353095391
0.023205625118249484
-0.01535337219974335
-0.03362187445698274
-0.023687604181683794
-0.008002127011669517
0.010131062466774508
0.0010680330896821986
0.0024445867575792653
0.020650359883019644
-0.017464564019081613
-0.01219137001661674
-0.014415880264341215
0.023846534912967867
0.02980030313763327
-0.002546221310857478
-0.0033624099743595273
0.0070829756123095885
0.017174010619453138
0.001806676130059478
0.0115706102298827
-0.004133723939731342
-0.018235979385400572
-0.014858881807181284
0.01692745964439135
0.0021481705972733085
-0.006051458507444336
-0.0046839271053737246
-0.03503482606792863
-0.02393107710669768
0.017244988177447206
0.005646853376058358
0.007541161898814905
-0.013513880563948097
-4.584416688102524e-05
-0.019772468591334583
0.02414568938639383
0.021763367589464986
-0.01366548813622728
0.016841562108294135
0.006006923666650318
0.01443959458455913
0.0015813104224533508
0.014484555951352071
-0.0019404055475723886
-0.004167540845151856
-0.012129329438196435
-0.010003318440250486
-0.0349480574075011
-0.01987540707501098
0.0011377297230033773
-0.002899599674976972
0.018266806495760533
0.011102565724309638
-0.01606697729312515
-0.0019122565855897652
-0.010846605153898493
-0.004570692315354468
-0.011183539504531116
-0.03721206461301012
-0.020501033406807492
0.0008240194865790536
-0.012185276875373144
0.009589577697325387
-0.005207626747911739
-0.01639595988509867
-0.03738256687380531
-0.01679835687939501
-0.030473364406648183
-0.03360045146030576
-0.04858025133311772
-0.048306316624027186
-0.03665060255200966
0.0024922854960918728
-0.025776158016654588
-0.02951895946021907
-0.026978656506547176
-0.02655258488664393
-0.0305180553365808
-0.014080366541374723
-0.008624393617141932
-0.015021415695801243
-0.006138028474911583
-0.015532218552975163
-0.021290931676324527
-0.04361918484318948
-0.010983153432643276
-0.01040980889103972
-0.009647697442113997
-0.004888352784001753
-0.004820357874824179
-0.026486568800564596
-0.014170232758084099
0.017510353643656707
-0.031351074821549814
-0.010818047243506332
-0.026001607936839966
-0.02033566469687237
-0.012306712905347655
