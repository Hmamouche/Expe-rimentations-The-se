# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FYBAAC
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=10, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.053138802369484595
0.002336079397948526
0.027188046688581963
0.07173115864099754
0.01257743491136933
-0.060581420653532864
-0.026231981154460255
-0.058411351227845666
0.0013277828277491258
-0.012147330126752051
-0.09407474453449463
-0.04873449178816722
-0.041587771293911675
6.229786865379755e-05
-0.049408339171257795
0.05926356379776642
0.023404905189963826
0.03152502494665533
0.008195720089024841
0.0035872532568990284
0.01522657823904874
0.014088317790305777
-0.01016092243701366
-0.06696899302048813
-0.050816981006158904
-0.0016309554748204745
-0.010772298153230015
0.01159013895761592
-0.026975922190398363
0.017536365486568443
-0.028774451232107437
0.024862063930895142
-0.009306460088923104
0.009770670562394682
-0.03752605279048095
-0.010015298890844583
-0.048811449196414135
0.003104355876447251
-0.011534309932846468
-0.019353927458195877
-0.0016822304737267937
-0.013179289279533293
0.029465540034016916
0.027269418280612842
0.023966285059594583
0.05317523836975256
-0.03022364018475605
-0.02224983295547482
-0.03304143740800917
-0.016171072887490763
-0.015611211325114754
0.021820417593218445
0.018651127594311687
0.009245224058709901
-0.009688887043797022
0.009931510508538955
0.012090810910735177
-0.016074181654857946
-0.02230314293302183
-0.02303915128133572
-0.001135517625469689
-0.04214163221881545
0.0377781662828061
0.01133089112171402
0.004819334869706342
0.05458959162119684
0.00427342338949787
-0.006003488232648685
-0.022048608312992338
0.011094333960522708
-0.028084385543886398
0.02583303741944811
0.01353324902342428
-0.02321224419408813
0.03263811187505403
-0.004821535001285791
-0.06359600072293994
0.013098822381443186
-0.03955890491565764
0.00027040032091040726
-0.007686896628370514
-0.013649366078151898
0.020099022323819916
-0.007539106972869436
-0.03228717868649898
0.012421480024113276
-0.03588070279304315
-0.018607332962019052
0.011638142855136958
0.026617927093669
0.022006781943489637
0.028260639025629787
-0.007771870467367548
-0.009470459504636295
0.032706375149350424
-0.01823712794379946
0.023083782365052784
-0.02835970609919939
-0.03419689869571907
0.007821693343075316
