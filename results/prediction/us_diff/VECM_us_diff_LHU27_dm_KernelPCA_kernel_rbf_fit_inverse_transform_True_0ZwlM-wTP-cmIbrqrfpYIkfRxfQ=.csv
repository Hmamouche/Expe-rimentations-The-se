# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; LHU27
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=13, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.032546527263953934
-0.1361665321296615
-0.05908827367814594
0.034608310372708714
-0.014273270452717209
-0.005777842930189908
-0.06410185110901166
-0.03301561524148123
0.06708419976934778
-0.05932223790184665
-0.040980505371496424
0.0029499473667600855
0.01560836798610366
-0.030842273477378057
-0.027393548749835805
-0.062311844248496076
-0.09618560647285201
-0.012471501738611784
0.019118693371940308
-0.0010640673267620915
0.0007151253910928704
-0.036368610891971284
0.00966471848932873
-0.005461771966794761
0.022960603842986448
-0.0026368745240926347
0.007621424220281309
-0.012819872073295166
0.040721892829261405
0.04731042188182853
0.09345545717889488
0.04985748843379671
0.000520900070278961
0.024485126772349965
0.06197308220208085
0.07537639778329384
0.08222885357365792
-0.010667970595035468
-0.046072954011828836
0.01123157988049733
-0.037941469945371134
0.02717975947920174
-0.011771596690690863
-0.053658325368075216
-0.046345794780808615
-0.020348398807000644
-0.07953505222209759
-0.00893425902443784
-0.020177474649755514
0.014885538211631893
0.02343465161939384
0.04280347041388107
-0.0384921727376833
-0.05242507806677263
-0.04638455809013786
-0.013218305182604768
0.00039636578357379274
-0.026464694306687034
-0.022397602046192698
-0.037963764339185226
0.0318471113710706
0.007258567127736651
-0.016082568449799512
-0.03708845275463877
-0.029799148343646668
-0.035656662146180185
-0.0099913589599199
-0.012684445354008995
0.03367000670639329
0.015063115796255694
0.03512238698563247
0.015394725116941411
0.09602452564087997
0.1162486693084369
0.05539230723569681
0.058549220364341
-0.03252428894840545
-0.015919251511427063
0.044467311935608905
0.08055390248997295
0.02496810101710259
-0.04337215337924083
0.002097047391361977
-0.01381959658213219
-0.054077056511731905
0.0009346377041922361
0.006684272763988638
-0.046211280973707594
-0.03309808692678643
-0.002572801387393102
-0.03197614424205547
-0.037526361442315775
0.03490659364054415
-0.03357336111811215
-0.03889342956034175
0.004353378122426248
0.02929425062323345
0.03343382701162678
0.03446457953734912
0.028272883422543055
