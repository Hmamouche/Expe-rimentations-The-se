# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; LHU26
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=11, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.038336973590420494
-0.11044357764384065
-0.0013953077239678528
0.03686549311562759
-0.04183385628836919
0.00011085088831549322
-0.05176313004991237
-0.0034087601438707593
-0.005946430296928336
-0.013839176463991833
-0.014673452450682398
0.02510219010089133
0.012977806062416223
-0.011435399225968512
0.027434073778031563
-0.01363711774614361
-0.060106313839032435
-0.028677756046287986
-0.005816435574282372
-0.05835590529820261
0.0063922767045460245
-0.07659595937210632
0.0598194798467728
-0.02756303250313148
0.05195034770925214
0.01841923600869344
-0.015563374012628025
0.04906469753537732
0.015520444756977503
0.09455095311496349
0.08823044426130026
-0.02169578067427904
-0.06028524173068823
0.08593280872435913
0.06100965873884492
0.01198526726217116
0.021144975483208504
-0.019765288563817082
-0.05977880026543481
0.024620948219572517
-0.025930399867303427
-0.007839621309023858
-0.044194761514693676
-0.036438055338751094
-0.04329474163990244
-0.06898320414197602
0.009471312519685669
-0.006739064722450817
0.027873781043443863
0.07370743912613271
0.0038038021902856017
0.017162064676447143
-0.0216588270886768
-0.07832773898785883
-0.05423163001879562
0.006551685521889405
-0.06675446920715007
-0.02020541254184454
0.022816589211198456
-0.014032838118105566
0.04351878373062829
-0.036603364302012525
0.017880885023500652
-0.0435503691416216
-0.022403826891667103
-0.03301992328550952
-0.019717388696145057
-0.014847858542489219
0.02317043090149709
-0.014748554566645158
0.07357443185491182
0.07500970308848061
0.10853990343958086
0.11929950927032403
-0.012622373572291313
-0.0048059019171350715
-0.04650481584765365
0.06299020188318859
0.061528137802786084
0.0036336250193444233
-0.00040096067040901705
-0.04312126118592416
0.0017949715055660632
-0.03380674074216754
-0.01436680863579749
0.007702314299947683
0.01336198753828998
-0.00904094233989413
-0.01117171822147134
-0.016641296243025894
-0.018281300861395133
-0.030466967317024116
0.06258079489263
-0.07271899627709759
0.016950121138854798
-0.01993021181151472
0.041742189792026166
0.060829585494128924
-0.01425353235365545
0.08167079940463327
