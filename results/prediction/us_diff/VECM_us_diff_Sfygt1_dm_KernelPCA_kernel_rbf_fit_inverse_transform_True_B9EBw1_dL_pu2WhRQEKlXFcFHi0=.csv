# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; Sfygt1
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=10, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.15025332161714955
0.23905498058557123
0.16521128084674558
-0.010395358662497106
-0.03843275383378367
0.13331996447155714
-0.09469776650980928
-0.01299136287396644
-0.19993639152563875
-0.18280934214336614
-0.004729724457843275
-0.052351562177801944
-0.1441829521549699
0.054682068723368685
0.01742525319269817
0.08453485931944564
0.11637477241676913
0.16116242349737966
0.12284504928989826
0.00810948418799845
-0.11921555807161158
0.21795747496589463
0.05067176975325752
-0.07402988736924808
-0.1012325507610776
-0.12061617037316731
-0.09425220322369902
0.13836632257747267
-0.08136567488504565
-0.05318611285896353
-0.008123671479537917
-0.05548321064549047
0.12610216993270296
0.11487259141145255
-0.07625645713258809
-0.05908739122926404
-0.061893216128860576
0.05960405727158911
0.03757803275550166
-0.12167923749336346
-0.030133783534289527
0.02692211200514609
0.14945126309635695
0.11673641890903352
0.04011873660613849
0.17886079538346583
-0.07738656240824013
-0.004232941441288884
-0.1341885880237373
-0.12761947275553334
-0.08537419940690105
0.08717617031210692
0.08776785085736134
0.013599094476151697
0.09532477896635762
-0.027634279385727826
-0.06850423573047948
0.01631266731977027
-0.03720249797018797
-0.04836325180005209
-0.020642021684611215
-0.08276812049250269
0.050051480280712025
0.07546841253278774
-0.04572746211010443
0.08854709458569704
-0.029086555753284844
-0.09294402725407275
-0.012854619382329152
-0.06275001323537838
-0.12640250522064111
-0.005947888925789152
0.05955388823920345
0.08834633471992852
0.023794248238696677
0.11321119621320763
-0.22833708017667306
0.1351984285520609
-0.1732980631940172
0.03376457097397027
-0.030232898059329036
0.14046466850370404
0.11073977047925884
-0.13554345529948034
-0.07810713417895004
-0.035537654044623196
0.00999797648746207
0.05207413683746358
0.06137122501940082
0.012575484187250761
0.01886651974654899
-0.03408984191100292
-0.019833439088992884
0.014076114522618297
-0.06988497319479645
0.0005502232448892494
-0.03570215855509934
-0.047981650158182385
-0.09242218348478554
0.016041115723127
