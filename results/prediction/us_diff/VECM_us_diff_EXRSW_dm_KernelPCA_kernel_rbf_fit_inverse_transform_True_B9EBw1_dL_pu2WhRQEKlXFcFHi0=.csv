# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; EXRSW
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=10, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.018992097182073356
-0.04783785105956922
-0.007117464681260762
0.04055418058575111
0.05718232826121042
-0.035362924099050286
0.07119214808468322
0.009404670900655242
-0.008166543644031608
-0.0412537232557033
-0.01580065941173963
-0.03842586899506279
-0.07202585129295119
-0.02105587627837965
-0.046101144495014554
-0.028954914112309223
-0.02259856156030911
-0.041506238287243605
-0.036782736829749044
0.0453764165047665
0.044860381604603386
-0.024884597975530054
-0.006294509846408624
0.033314773678249386
0.011549267583137592
-0.010120775803410517
-0.03943775086830037
-0.0008237584502807198
-0.04812083524688014
-0.04169585476222785
0.019244242522727793
8.504253782484587e-05
-0.02351873767580555
-0.013252054791608455
0.03588192340505657
0.024360198374316862
-0.045076606271541815
0.017646776401830336
0.002593493260686282
-0.014502360840126172
0.027836875041929523
-0.024570674581122665
-0.004307187066200037
0.0029859699089760093
-0.03022885589229628
0.010322560858977663
-0.0032899340185251844
-0.02852528481268698
0.005674215545719707
-0.02109433825834597
0.011136917397875184
-0.011638973254193368
0.004268832197436127
0.03261364479338093
0.024478682925661667
0.010092332628529009
0.005522165544369389
-0.005948425089696577
0.030782006907764392
0.0005129238120293177
-0.009900198453910622
-0.018919048022214963
0.023003299407818246
-0.005578687469734916
0.007362212866507171
0.00901366057762145
0.010168624822202417
0.022873609824005374
0.017619913511175745
0.0001767160897600227
-0.014847648076729225
-0.003204170367255969
-0.005018951944906768
0.00640119123876338
0.034381614375025495
-0.027881836553888883
-0.012393566301303338
-0.0038261142129583856
-0.05865552745480587
0.011151664132674686
-0.0016908450976766687
-0.03498898836209698
-0.013426482509908133
-0.011809360496382112
-0.003803349670287868
-0.010786855648513297
0.004715328775607467
0.014154986411157993
0.004223777506923062
-0.007345829679696223
0.027493179141095846
-0.004023628903430778
-0.005665202319664981
0.03746571728003881
-0.017413808685458863
-0.03293801006692011
0.009409604832170954
-0.005120495455195906
-0.027361209110905338
-0.006275788073227306
