# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FYGM3
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=5, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.08218382909343386
-0.13040257902727956
0.13500551914348022
0.009565874277867602
-0.02997940282702507
-0.06527208545360297
0.060723931562058166
-0.04200119506940768
-0.04407277043771056
-0.058054033912743386
-0.026031175582824948
3.62738085484815e-05
-0.07313403613464987
-0.039130027516322746
-0.022373593814558633
-0.03485427130887338
0.02695696392949825
-0.023515141315031997
0.06499946959195849
-0.011305373879867178
0.015554185989307284
0.02428321764249557
0.030035803874192434
-0.00806388593131149
0.021110899648932233
0.0324598997878008
0.003548750037784005
-0.008060795744178674
-0.08258948636373487
-0.024344611149866743
-0.033927408314338534
-0.03437479598298016
-0.015252525105742177
0.011573540581081981
0.020312372925929204
-0.03346445135155514
-0.06647226183818178
-0.03047717725272197
-0.024166139228803923
-0.045693115484426505
0.020390310373739482
0.016560589332943576
0.015281444408218693
0.01567262518499717
0.02641305736384535
0.06142565378194063
0.0017238335926427857
0.003998976453909696
0.016040051129020256
0.017071501045648647
-0.0073030447953318575
-0.004088428531792277
-0.004711040924373325
0.02484396828469629
-0.014958279666898392
0.03352847685310235
0.03146350485450282
0.0020207289657237937
0.015709098131215272
-0.032117885960063944
-0.01804777109548523
-0.031401931889661686
0.04744637263873463
-0.053697545010040926
0.0015319320754930477
0.04335938097757066
0.017285188269677752
-0.00928973203214372
0.0001616386912350349
-0.012751241057604404
-0.019669572267385305
-0.021139020773228837
-0.04559443073219689
-0.11202291692597251
0.023856462585178373
-0.06590508527948667
-0.007878842706724523
-0.02697223644820062
-0.04495461174483087
-0.01078625718975094
0.007030258668788725
0.038174846031652616
0.041427370892188446
-0.0022208568640063094
-0.007356089912278381
0.014362825458853128
-0.00692922869356364
-0.0009619561151708294
0.03716128817966038
0.02439251940015523
0.055925776969783396
0.015837801288204084
0.002265926132347905
0.0017112177068171544
0.04324070951115189
0.004765853533672682
0.01776192081352253
-0.033177072313718456
-0.06082752652935219
-0.02724092741155844
