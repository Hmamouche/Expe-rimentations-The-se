# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; HSFR
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=14, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.09096712326818555
-0.0745910031130253
-0.2553221979061022
-0.11020569352040313
0.040138041012171596
0.02680915106307276
0.047071449967375
-0.07149700791842772
-0.005120740030404705
0.005264971520406586
0.19621686792627188
0.14488236564209062
-0.07588208316546494
0.04692197035165081
-0.12031120618516693
-0.07735146498871726
0.13478072763241036
-0.10427494275908461
-0.04481932076298929
-0.10419790910895176
0.06599568498500896
-0.00952707000383368
-0.027073290736221053
0.015366221630399357
-0.07411834684841354
0.034741748683152635
0.05844874226140601
-0.08722330057299996
-0.06782032125625195
-0.03175756725946417
-0.0939915191281794
0.11989794128557538
0.0039035332356765536
-0.016087796870642258
0.03556557173676704
0.03418950022580461
-0.01739003489470466
-0.04142452879341016
0.014129182124429412
-0.1145594013016275
0.0678281093511562
0.03205509320033057
0.06465520396500772
-0.08409462909790186
0.1071530544426946
-0.09984008819342755
0.051279099636724515
-0.050580980196059805
0.05495918078995704
0.03527522452364444
-0.016140724547923598
0.005661046296829496
0.036441549973528775
-0.08715004327201523
0.06992990919423525
-0.03757643049060794
0.04973523178768994
0.01209016714929978
0.019745239045659807
-0.041121944961897916
0.04469593180522302
0.005539627837031354
0.022217846678906972
0.0007564150045282049
-0.031602830886169365
-0.029048571950426316
-0.0648912329718016
0.04707060834198195
-0.037001305571519
0.05008327379891866
0.03518873093511689
-0.01406078282741657
0.006641055619886019
-0.002022781037354305
0.05360166517479914
0.07754480331552532
-0.07744984245754004
-0.008289607560680475
-0.05418795155225395
0.021741558773200862
0.10610970960383663
-0.011329365728094131
0.031353165465007805
0.0018242688424900746
-0.0009613856082168791
-0.028761023493534654
0.015595479298864252
0.03008706727957871
-0.009092990231866983
-0.019053548581075188
-0.0021785316753906783
-0.04000701091349398
-0.0342004403128332
-0.0020713999411134075
-0.050823497749281625
-0.036312052581291505
-0.032429108325738726
-0.15752015436037076
-0.025569950906548743
-0.10946439586783009
