# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; PMDEL
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=9, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.02045729626161387
0.08422803436906605
0.18336568946359688
-0.0836980905254893
-0.10993463299299559
-0.005777518479388892
-0.039246654451770946
0.04760183552973277
-0.08379914192762439
0.01577303294104378
0.03740973996442366
-0.0020199748417246075
-0.07042061338595343
-0.002457871690684229
-0.028523478170193232
0.06538452140548741
0.026113552678484194
-0.010797456377219596
0.030570660519071244
-0.020790435255118693
0.01796889554892682
-0.006419369579109312
-0.01662292709490946
-0.0840303381866731
-0.01370873088517367
-0.03960283490636081
-0.017418891998882458
0.05111971681924703
-0.12601588234343747
-0.0307935559271717
0.02890614803281284
0.14694080065693188
0.06913942824685661
0.02452419005891395
0.015093487321242521
-0.05409448318344974
-0.024443129749987816
0.06817223634112621
0.0043942902329053915
-0.08069898405786691
0.04190272509214527
0.027346668315750656
0.12867706951357832
-0.0014976994348161157
-0.018449174528742354
0.02613101349927208
-0.0674217933073592
-0.01246723657985923
-0.02602055473753785
-0.06410649050467637
0.0025947500211564757
0.023602628278404553
0.03694309708007011
0.026618838567374743
-0.007325141976415962
0.03853289817593154
0.012944046565565335
-0.01630425803719946
-0.009888790152081033
-0.018688678019544967
-0.04729585477540124
-0.023579075386020054
0.08107607800495116
-0.058752966801272205
0.05582076243503263
0.01576225750925791
-0.018470147432305628
0.001351503640859196
-0.06366245986799023
-0.003820568950997619
-0.09272233361355908
0.08298874017793156
0.03081756900111658
0.01833745564475995
0.00337138717285497
-0.03236527432191132
-0.01694369021460401
0.005842472200631804
-0.058216900589820035
0.085086580546924
0.05486607570173951
0.028096892719246896
0.06527226966147782
0.011424007196866558
-0.04010939725143311
-0.06371281086383739
-0.07135336453318887
-0.0062801755383500996
0.012066406811334304
0.04333965319205223
0.015643431089016353
-0.03759282014980764
-0.0691121298931465
0.027714232191565526
-0.015880932527211362
-0.0033587619421130213
0.04529352550155714
-0.0350020333480482
-0.04105741548559308
-0.020054098731477468
