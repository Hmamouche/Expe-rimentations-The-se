# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; sFYBAAC
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=3, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.025123605791558153
-0.01949764326879019
-0.05030529819604139
0.010259202599987433
0.029704727602049983
-0.018775447530229063
-0.08190133312944087
0.03125701067662564
0.042101054566738676
-0.013280811431282456
-0.008233841242320749
-0.00585393714858164
0.09204383615558281
0.08831571869676824
0.0034835154743300045
-0.005578940060057113
-0.04988154513710558
0.022434611718140502
-0.056616620066815795
-0.009769300150458566
-0.03144444382910953
-0.06462004984495551
-0.035280723101097915
-0.0076165893746810865
-0.007072358742562163
0.017310514617735213
0.01882958835847227
0.03281445373522354
0.015220872668985388
0.04267038535429414
-0.005825884088498482
-0.05363398680651095
-0.024310189139164523
-0.0110579767417176
-0.031736474962208244
0.02855721083449728
0.026619854293196805
0.014631234012841617
0.012814102298811449
0.023301495409215223
-0.012656869686305243
-0.030888939590943965
-0.014017311127173367
0.0013218511152902881
-0.011396363342537573
-0.021028206594949382
-0.03367941947915963
-0.030333029656496806
-0.004774731052664545
0.020020316483525064
0.026299148467559243
0.027289193907891654
-0.015476609521571438
-0.015607923383652031
-0.020823178332399057
-0.00839795039974758
-0.013633595028553947
-0.010839246698414275
-0.006755965361882317
0.03811324338181366
0.02800491890663382
0.06054725079612461
0.016335050495336942
0.029221294127666782
0.03405584641999187
-0.011264545474875464
-0.04464308274442312
0.029594282388703153
0.010384301011782768
0.013096428397338487
0.04003415616316865
0.025148163154002427
0.028926774046392736
0.026843100833060587
-0.011457899259353243
0.005811625468662809
0.005262525438647352
0.023003560706364175
0.030983480928016872
-0.015088966982677034
-0.038890155582634506
-0.06277248263230634
-0.030954690664304726
-0.02162148519851639
-0.0014118736906720336
-0.018545164491333355
-0.04875098845585335
5.5108708336530816e-05
-0.016379957839354694
0.004265848328199419
-0.031201071218720126
0.03024322501272109
0.020450446804019414
-0.01685833804402046
-0.02396666268741799
-0.0008090130190620457
0.016254698727510907
0.051706341948584374
0.0894404639632662
0.05312999274202262
