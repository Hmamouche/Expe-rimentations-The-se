# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; LHEL
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=7, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.009131119755133577
0.03104866347478453
0.0686801173211083
0.02299722872111659
0.03650454915438789
0.07579373044432004
0.0917801737110107
0.038355808346387535
0.0026737480059085064
-0.019405384890664693
0.017042933788208394
0.007857288150408159
-0.02590637808631735
-0.004756011154581771
0.0031909545643041444
0.018067040172061097
0.04574494193393966
0.013399068441752812
0.007443584909663798
-0.008680934770241743
0.015094093048464567
0.01854408634466525
-0.04460498497407667
-0.02509364828739235
-0.028269862154986808
0.023045610609726017
-0.025488919258693843
-0.06604306958596609
-0.0896790186356795
-0.1403098070667393
-0.043110019751987444
-0.018298343840637738
0.005475392859116086
0.006618611570203617
0.027732604238758153
0.0007157208255530977
-0.014297313204339569
0.01962937087661707
0.0024590422998974175
-0.005402997707948391
0.03655433207663375
0.06234908712768104
0.0869278613906229
-0.007622762234443393
0.021190915538441754
0.011102408929937855
0.012738544207602363
-0.004252307476048323
0.02436366268510298
-0.00016997858532872417
-0.003960615703587782
-0.02039032236926468
-0.01622864958048556
0.00807502536935426
0.03956674922825028
0.022025101531913338
0.027411046085938893
-0.0019503385526723899
0.02911401212453094
-0.01088339890365577
-0.019521747050271657
-0.014502381919321683
0.017902134842681022
-0.04342980505211422
0.0022145232454649196
-0.0048014415822137555
-0.01697804755853457
-0.025107837874783004
-0.03101122645662836
-0.028373027848019615
-0.0599070175374481
-0.09106490267244922
-0.07845219793413416
-0.10572853724061428
-0.024193895626770823
-0.005505093960309335
-0.04758430429411323
-0.0200762481652752
-0.05189536737472672
-1.604853418486589e-07
-0.0012655357424670027
0.03962269501538876
0.00126684820618116
-0.030288024265289617
-0.012235342052514381
-0.0035437686509243705
0.006496723770779089
0.010290396834448132
-0.0034982561923299254
-0.004069309645345083
0.019687186470568288
-0.07956739027324769
-0.02791508835088355
0.007427726587371989
-0.012876720295993203
0.006258861870566242
-0.01019623356815934
-0.08276296737161497
-0.008811455075002944
-0.03195094708003718
