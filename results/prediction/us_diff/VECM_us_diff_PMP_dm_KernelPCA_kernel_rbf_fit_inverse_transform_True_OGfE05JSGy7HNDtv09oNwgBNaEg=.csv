# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; PMP
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=7, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.18240334873961422
-0.20142712365253035
0.1111545379125439
-0.15230756656492933
-0.1044302066532305
-0.0360007858735926
0.009596112469065752
0.14602517181804833
0.028712621501289996
-0.024304752130843876
0.12585459967809956
0.029094685333149687
-0.07824026882999184
-0.030691870455968592
-0.012567906223778952
0.017303174824113948
-0.038129829686438466
-0.05830889068973785
0.10932971626731094
-0.027722399565688678
0.05563193413985096
0.059909555663143925
-0.11485649615618634
-0.05096218377817596
-0.04690954511439805
0.03609502796463964
-0.011644725384087576
0.030612925095475328
-0.025751919943964703
0.003578283039508575
0.22337493837689745
0.0640072965166314
-0.040225019754985075
0.03816739964571808
0.05935501123128359
0.04712515507642063
-0.01201111138911766
0.02158165097242644
-0.09918373747248413
-0.09647149083958359
0.01077101742453827
0.02946015314337176
0.034184987932855695
-0.06595572214804354
-0.02454444945660155
-0.03381110200119933
-0.09948048478329363
0.016850997317697656
0.027396041552359664
0.013908417997043666
-0.0005508481814943088
-0.012581100099448713
-0.044490334550123044
-0.002389068623622115
-0.00991857283231875
0.081810428541452
0.006855942068668649
-0.027110382157217998
0.007294666057762564
-0.047395822376615965
-0.03782703396162573
-0.05001093716995719
0.0546966615992362
-0.044495947909711134
0.07989098722315328
0.06579443431466384
-0.0006346844954254281
-0.08552652469639545
-0.015398851275274171
-0.04823457084710259
0.02809460885097062
0.09504092166369398
-0.03903362877911397
0.05138713637085471
0.19251711730233043
0.030413735334093566
-0.058964385619265966
0.0007071771304292471
-0.11714271890233449
0.10426595957288469
0.039252827412775276
-0.06483215786078259
-0.03329232731148936
-0.078305883892934
-0.026004128702070084
0.05673839525421083
-0.09698509838774369
-0.013558638645479584
-0.007783889822736584
-0.10806108076491115
0.11415559700577933
-0.08925460488829923
-0.045634801752277285
0.03463928300461088
0.02510031747170252
-0.025940315281176615
0.03869136264416077
-0.03496103417806917
-0.10744744648584303
0.14563697612915827
