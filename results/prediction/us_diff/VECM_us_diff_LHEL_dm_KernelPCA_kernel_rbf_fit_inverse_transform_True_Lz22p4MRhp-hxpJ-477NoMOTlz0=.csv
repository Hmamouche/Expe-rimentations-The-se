# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; LHEL
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=11, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.013055877077408084
0.04963475022661051
-0.013892683344898923
-0.024080436219442405
6.338381716665571e-05
0.035292017432205884
0.06009434041288681
0.025835195209904443
0.020582355196140364
0.020084151318125947
0.008541368511720255
-0.02390533890827102
-0.01811333550299504
0.007217723458865987
0.02218644579235068
0.0356863574381296
0.01590673907747505
0.014727557413874362
0.0011535760704477586
-0.014643253132167964
0.048901105071995396
0.04525287810795129
-0.08476133369238711
-0.03167167285225785
-0.03214987943835343
0.014145677156010478
-0.02809823924323869
-0.09927325419915226
-0.07067417909956376
-0.11117269290796769
-0.07512158361106115
-0.024513998974294557
0.0029930468028919587
-0.001319309404192219
0.06657092649216984
0.008755054708618912
-0.007933299001426336
0.014850569991321123
-0.001253736523188326
-0.026572743275823686
0.05439288606923265
0.04989094083600083
0.09669649540893527
-0.024014544386149497
0.03666173047516249
0.0037554087438562405
-0.01876706845010205
0.0006867081978446758
0.025741739721710746
-0.007816474093418325
-0.023692940798237624
-0.038017554533962813
0.004297063423625234
-0.005388681270350098
0.07079217486084488
0.016898764037376147
0.020571431964286395
-0.015995456449607773
0.03228441452917414
-0.013824636320152078
-0.027874977602327933
-0.009668691940359245
-0.001379782348799916
-0.058846294692442944
-0.0021369720941502414
0.010903816041307147
-0.027094555435426493
-0.030075059840237277
-0.04109662924400635
-0.037100611808781145
-0.07621565481254289
-0.06836230326717632
-0.1065078599709355
-0.08400741146543106
-0.007379159851843564
-0.009073969150169838
-0.03271634501923236
-0.012074988986324588
-0.05950962512194273
-0.028871987480693764
0.016574349570183146
0.0632408244915321
-0.014326889970020584
-0.04842474542698277
-0.008694219239355538
-0.021478763589137827
-0.020104425113085356
0.0036595450429521906
0.016502239758471413
-0.009857870648929552
0.013534719710261649
-0.06705371407545437
-0.05387745193632003
-0.008551648191335092
-0.02603717971880782
-0.016874212872461714
-0.02957874146133789
-0.08655326875127678
-0.04949564743007169
-0.05819167227897595
