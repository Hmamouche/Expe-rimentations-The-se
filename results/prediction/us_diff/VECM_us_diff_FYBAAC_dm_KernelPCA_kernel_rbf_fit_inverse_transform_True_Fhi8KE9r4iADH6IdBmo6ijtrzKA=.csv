# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FYBAAC
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=12, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.06732565067240945
0.006947562743298573
0.04311774819863129
-0.011372760463663818
0.0032240007598737337
-0.04686938897009271
-0.014376608864332945
-0.05716804029057272
-0.05722195766079469
0.04845866071709748
-0.05347986018031498
-0.05691568128981852
-0.04717110945494561
-0.02585002014342168
-0.03242146575206045
0.06520888017928969
0.03125922450215063
0.019316637761482895
0.016187975824764576
-0.015718601375559353
-0.058451711528650516
0.026312179449107204
-0.01329145905650371
-0.06872952218481576
-0.06750367538063747
0.004647965099078626
-0.011465277291826875
0.04287832408358701
0.01113261431150827
0.005118792730215286
-0.018008295176472504
-0.056495996042494945
0.022548422921863126
-0.006778461178228064
-0.045892281302579684
-0.005475704789814149
-0.05684677569293983
0.0304651931300204
0.018552893010364942
-0.014452710958253994
0.005447206749722107
0.014638156434400376
0.0049893267823915306
0.055754266032833545
0.024348191232863112
0.07479262004629501
-0.022167459358980533
-0.033153418218364816
-0.043267164535492376
-0.04159809951915776
-0.03544655341617441
0.03965171612405426
0.02912924457354439
0.0236155270840768
-0.033294347088776995
-0.008573879203594523
-0.0018998530276117262
-0.027622070958761957
-0.017207359176233398
0.005352667805286337
-0.025779235020981405
-0.012118901126596138
0.03853893879699083
0.003968181461960971
0.016346921003940064
0.031698350914813524
-0.02557966509021202
-0.018574636870675156
-0.02296229057667862
-0.019865505851423784
-0.023756729898767686
0.01649720937177149
0.0037137051083085803
-0.005577584856749487
0.04794901914934504
0.002754928816806144
-0.05058893724817684
0.02546629600847501
-0.06484162460232293
0.01875820093427392
0.018745520997216734
-0.048652157719205
-3.645384001493921e-05
-0.028617469734971385
-0.05610619033138202
0.011943112060115833
-0.03964083351788256
-0.014508841620599644
0.001962735691367507
0.012484632681632692
0.02169517941903153
0.034835002729866796
-0.020842387347554378
-0.028395726247226544
0.039636217966844564
-0.03207405614312121
0.003514332678388078
-0.02137475847118484
-0.047828291688970126
0.028838571263635124
