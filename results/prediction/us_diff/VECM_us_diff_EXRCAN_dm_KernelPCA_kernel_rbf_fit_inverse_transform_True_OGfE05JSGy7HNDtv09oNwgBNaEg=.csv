# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; EXRCAN
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=7, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.0112925034060137
0.008778261768326235
0.035862309909493656
0.037163280200184806
0.02947782400290261
0.017022597867297783
0.02087288025880039
0.026638530321481786
0.0067915551750149395
0.00983897060710761
0.026695478532602734
0.0013942316009869513
-0.015862409642292467
0.010153977459137289
-0.03298132425065972
-0.018149049990807825
-0.021775668505082814
-0.00018258833541534136
-0.04229390660698428
-0.03342742707643717
-0.03304850510908597
-0.008119853360607922
-0.03933408273342916
-0.027884392383164626
-0.024878251391892495
-0.014118342290768478
-0.005234907825315689
-0.023013882761244116
-0.030124706653500154
-0.02986798907537439
-0.007140385010488194
0.028901397489264163
-0.023381467830806017
0.0007288026843325236
0.03514036685934431
-0.00791105620406538
0.025305422635977064
0.06223154219872157
0.009745636493993258
0.039304018667147264
0.03359400465619475
0.039452564797869946
0.04713153705938247
0.03471127646589754
0.028888678660470533
0.025436595066496297
0.012785819394870588
-0.013495502572309727
0.022623965857008717
-0.05597161297460845
0.020440944893017485
-0.005131000335131581
-0.0035754014790468705
-0.014961535119967936
0.005484174251093814
0.011139293370052029
0.024824258857212473
0.03474595437480425
0.01414592230162023
0.03246989437154895
0.06829780437349608
0.031738512887586036
0.03940720853597159
-0.026300746135917152
-0.007283888410960369
-0.012680837555443447
-0.010580385160785383
0.008646372279765264
-0.015527544403707308
0.0540228901061225
-0.005598661105242052
0.0410290789961152
0.0057333449674809266
0.024877189643064686
0.01783168462148487
0.002407256953242431
0.0048007489199489465
-0.02531667086981145
-0.014765582149231467
-0.08769461400828973
-0.018557232576311988
-0.13709211290216164
-0.021370787610129546
-0.03610590513368236
-0.03699750851301689
-0.042985776259169745
-0.03817255189945697
-0.05528861545977114
-0.03726594105290666
-0.025650103180148553
-0.04843065617975255
-0.04059965504731908
-0.02516535071860155
-0.02921700660097254
0.023173014344735097
-0.02738516104699419
0.004602682863257719
-0.10541938562376936
-0.02709231356374494
-0.07241849038053967
