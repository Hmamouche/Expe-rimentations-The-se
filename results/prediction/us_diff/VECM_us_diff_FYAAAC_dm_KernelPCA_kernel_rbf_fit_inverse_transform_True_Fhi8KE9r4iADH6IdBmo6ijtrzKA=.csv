# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FYAAAC
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=12, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.005470267929119782
0.04692377791195669
0.009216786149990067
-0.07172207021127976
-0.006065352931943501
-0.021371221950875582
-0.0497941101094926
-0.037584056045789634
-0.033161061921044226
0.04057256721140911
-0.05695174523329784
-0.05819388867920357
-0.06961877739371687
-0.011984464517705969
-0.03823652755842622
0.04992996132914182
0.03997784384200817
0.0017241646409293394
0.03770599244727741
-0.03639461264608832
-0.033697569777186936
0.025731725358965982
-0.009100739064809647
-0.09247148966817215
-0.04557385973645493
0.011876430569251358
-0.006810387250682372
0.036999883056347875
-0.020533094351619617
0.02989266992780419
-0.03868750722544695
-0.06793672694409703
0.04963142872554777
-0.013348283944252783
-0.03584405467788996
0.011474454149029437
-0.03880694619869794
0.013061826131220325
0.030327157209091515
-0.026451359925551178
0.006144839372744751
-0.0028799696460319376
0.028841083318934663
0.038484721437661
0.028928117022999973
0.08624970014494975
-0.05927470630206298
-0.020451809820708898
-0.05939327772758881
-0.02689319697295245
-0.02868780127988678
0.03407332580863852
0.04760584518003831
0.006751335929773317
-0.020130919053643264
-0.006647571176650096
0.012395140177867197
-0.044020750004583636
-0.029720318394209133
-0.014311312446977718
-0.021055091956077628
-0.034820425795488656
0.08013905131425036
0.013107584032937217
0.018149598836795622
0.06051665308684062
-0.015217959363071548
-0.04447938881078264
-0.010105720384533725
-0.031093137577486885
-0.03297862307170242
0.006121798223167278
-0.016762750748196754
-0.005212833971654739
0.03576101178136146
0.03324492368226893
-0.07114583753578854
0.0211199862281424
-0.06490065852367209
0.021844487576672202
0.0014612694460735473
-0.013768218049431655
-0.010479636524905425
-0.06668243478841085
-0.044309747780074044
0.004231262677738344
-0.03766209533132196
-0.017505283149340096
-0.012629530236815102
0.017185017547365615
0.024450807311205393
0.045075275558839216
-0.019290738893977644
-0.02567559912632803
0.004953462426801156
-0.030392993852936978
0.0077513183424626755
-0.044833950086553845
-0.04391571053670309
0.00016332876552709995
