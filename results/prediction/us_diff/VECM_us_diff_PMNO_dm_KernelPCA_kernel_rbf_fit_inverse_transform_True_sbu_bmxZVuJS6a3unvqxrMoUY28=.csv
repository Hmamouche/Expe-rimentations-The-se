# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; PMNO
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=1, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.021808714673500634
-0.06287032301226864
0.01570403219777134
-0.05272962624964379
-0.014554656568657901
0.0749349740245833
0.0654199870518908
0.03649600202313753
0.021220403658288093
-0.010024067481155188
-0.01114638537594059
0.000228927105236712
-0.018792053744801397
-0.02106629324731387
0.0033509114094890206
-0.013810012073374397
0.04043683287825029
-0.0355896263032876
-0.006846359658364885
0.008094561351768127
-0.023925422290377852
0.06699566535672848
-0.03642616294362476
0.005004128932269549
0.00016058326728558587
0.013327714597393014
0.05811406454996439
5.630881427017009e-05
-0.009638112233757632
-0.07481546976534459
-0.005064113075517922
0.0626932948149285
0.042277332748582905
-0.035500745472673054
-0.07335993873168792
-0.06975172563458279
0.0348967099347085
-0.006728984094753013
-0.025074535322109116
0.022023509886410733
-0.0076810724744290435
0.001846559658226349
0.046255178921287776
0.013275101449236373
-0.05967000320436339
0.028530877477445125
-0.04345509480463851
0.04313384599405995
0.016023713297072993
0.03619104753031745
0.06962653447067849
-0.0048091921542755735
-0.012428745592206913
0.01638594216702499
-0.050397981385919575
-0.014160846846168463
0.0005386154620174825
-0.021320294511874838
-0.0014978773174587267
0.0013690534888816412
0.004878368177178464
0.027925497523411044
0.026238460388340417
0.01725405239281408
0.01159130923910019
-0.033407146274537454
-0.027522680879643394
-0.02327023709791948
-0.008594813179295753
0.014416465555348592
0.03057506479599178
0.02702786770362779
0.02777680712680556
0.0185949989285912
-0.010711845006005447
-0.09223846797806567
0.017189380241829533
-0.09235352048936048
-0.028253295595303766
0.06158674627408721
0.021870995160080724
0.01217527812908023
0.011541438063311225
-0.029746903832202007
-0.08232655792962662
-0.00014598116697935298
0.0542577390620608
0.00015267256701382778
0.04420486645703128
-0.0007287328665568586
0.04750034244127558
-0.023559339647247407
-0.02531588745725375
0.00621028513560285
0.02892599616944748
0.020671714044957063
-0.014419567648683018
0.01453940578339121
-0.06120648624843633
0.01180623438617212
