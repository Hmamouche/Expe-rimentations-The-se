# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FYBAAC
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=6, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.06120709731913828
0.0240269417573702
0.06009145491981427
0.0697519373032871
0.0052806013746479925
-0.0644763168843808
-0.01773617856041585
-0.07401919654337405
0.007520473096818763
-0.04456438720939485
-0.07488341544852525
-0.036885144268635796
-0.05188102995724218
-0.014456440096940953
-0.042620163001651515
0.021801009919769698
0.03209451665902449
0.04477668021922245
0.025906164970073593
-0.0066985063075136984
0.006662923506793409
0.0009040661014902886
0.01600357698744659
-0.0377166451724481
-0.026647009279768222
-0.02332473151259186
-0.009756718269892634
0.020479421528277045
-0.018857959532476917
-0.004902547296686467
-0.015144250786011448
-0.004370982850614543
-0.019576556470070818
0.007161968437160177
-0.006121505856271518
-0.013906612871321156
-0.03609383640147452
0.007345131454693902
-0.0330638431712721
-0.038204996520235096
-0.026397279535734508
-0.012042209240593241
0.021301145039557896
0.050197207186646255
0.032512589543245735
0.04954153465667667
-0.008887593581948664
-0.033446643814202884
-0.03243188654261317
-0.029951504066802567
-0.0030080172138877287
0.008437066820750194
0.013484369141173364
0.013789464371191308
-0.004684350492032501
0.009306289836568579
0.013477164626911679
-0.013970956295024947
-0.011475879578456832
-0.028501175080858622
-0.014586751072534009
-0.032684370920666776
0.02246516631174916
0.017895783808111368
0.013140465422761193
0.03962265258439384
0.023778054763614713
0.007634665761207148
-0.021033267817401297
-6.668672379930851e-08
-0.04311075395874123
0.012307825307312404
-0.00698216370808952
-0.03272063423440472
0.00987642557131168
-0.011825160770520371
-0.025949616996980146
-0.011881524762820991
-0.03750432348984177
-0.01504386541337856
0.017599805134165043
-0.002942394956695912
0.03507393924349229
0.0075331004346189685
-0.03949673972454647
0.0018971482475438305
-0.026088874878503143
-0.018089034217327438
-0.005915044211432916
0.023357650815343606
0.012364097253203912
0.02519330910528378
0.005389540970430287
-0.005762064777334305
0.00494733048688372
0.012115685500753334
0.027972517186021246
-0.01953380663088795
-0.0220804862120871
-0.0020151632830948227
