# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; CES151
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=11, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.10239188973483762
0.15094070061133097
0.007798010342478415
-0.06706441319569413
-0.1391293770201948
-0.1871305627403554
0.16142042658204983
-0.059005319585591284
0.1600804372138642
0.024243889576381947
0.11094549774899382
0.02741566159268384
-0.005099762236850748
-0.018814258691603684
-0.08924946265029374
0.15286909519397154
0.0453966711703264
-0.03895202045111089
0.044299067215015836
-0.029471452303083324
-0.0026244073788470323
0.02597030638809232
0.0007106192590992863
-0.08270495693700919
-0.06064363395209396
0.024034336900149608
0.08676816880464455
-0.007853677462786817
-0.07105194985693844
-0.1983813944878976
0.0709091794738369
-0.026491530377126236
0.10183309602071794
0.005209426558552603
0.007283756572260998
0.06808010375284973
-0.05216524909228068
0.1606969139616125
-0.05189737298502529
-0.056172080743062396
0.027868357553492398
0.10666473243300106
0.09685034042113239
0.04440622042050506
0.026752230653471935
-0.016825342648681407
0.04257534539982573
0.0001640743547766388
-0.0820580944240744
-0.11404981192600802
-0.008526243376250901
0.056098780493202854
-0.04035237916668376
0.1021557017952747
0.109704053934238
0.01772796017325579
0.03411158420577283
0.04701668332626284
-0.0831654131170072
0.0068130588017714065
-0.133798344818835
-0.014730506662129921
0.08828823770092453
-0.007970943758831839
0.08135789734193442
-0.03112838447812462
0.0075032448113659805
-0.1108436432403784
-0.013239495992802492
-0.05057798988610598
-0.14741394250771797
0.028490104978415323
-0.09986419498386413
-0.01889861088924803
0.10535785948887455
0.03746213046832404
-0.16093908097809462
-0.05707933580874882
-0.09795060780502869
0.04121392535985433
0.027260431548388704
0.007871937865492902
0.06555097150150169
0.08652327968646054
-0.041009214729134935
-0.1289327305249396
0.0016195735233418725
-0.1233113144541555
0.042977669146573506
0.034774821808379944
0.054160790667884956
-0.007706143244579439
-0.0029961142600486296
0.03180414406526802
0.039030892771737005
0.12075369136375613
-0.11448212979158863
-0.05138378319656802
-0.03372219087475779
-0.015773394018938135
