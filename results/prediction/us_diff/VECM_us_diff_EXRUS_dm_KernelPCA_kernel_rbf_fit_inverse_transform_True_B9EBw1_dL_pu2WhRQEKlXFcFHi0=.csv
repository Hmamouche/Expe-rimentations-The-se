# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; EXRUS
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=10, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.052674162380693515
-0.01775648060008969
0.015114597160755935
0.07192834066862935
0.08009514216272746
0.015642935708618796
0.08920646935272555
0.00445940581632524
-0.018540579577323954
-0.09697639732011751
-0.10651873252583857
-0.04617806419285309
-0.06371005196149813
-0.026778417237930736
-0.08645194135405333
-0.04716000277415629
-0.010140413311501612
-0.05366203944956988
-0.035110682801838634
0.06048701366272337
0.07659333711133129
-0.023618198873099232
0.014623606625330712
0.05185148786979646
0.008733568240335356
-0.01234983420594162
-0.054234772466428646
-0.0008096016270434572
-0.10002480860010632
-0.07839017480766858
-0.026581800047439905
-0.0029655656166039605
-0.019704500727672554
-0.003164557765605206
0.06261191915033909
-0.007178245697911971
-0.08092142882470918
0.04432130207255029
-0.04104329410268756
-0.013397115536961572
0.07660774434238683
-0.0254626944032893
0.02261818936860159
0.0061786057391330605
-0.04822930210837848
0.03377968223870659
-0.005556530311349651
-0.0579923255800065
0.039989185200153016
-0.01983745218519565
0.03298789193489451
-0.025512774696981766
-0.03768311886094804
0.02982698434522942
0.018429062627781826
0.03950452840099061
0.03480089844976009
0.03044253291660192
0.04558013933328221
0.007636932147242946
0.005199600554628151
-0.04566919377683318
0.005679318086317286
-0.027621077390248914
-0.02659813777396438
0.03394132896103169
0.032518524326017706
0.03490275308793272
0.032123980019851624
0.04021177580830769
-0.00946343796520531
0.019119557832883074
-0.028825856487614543
0.013546534727495334
0.04674303454258133
-0.03542569878652037
-0.027888474286465372
0.008441866480479551
-0.09329278470076871
-0.014450235594985848
-0.031789848380881766
-0.06719160003430315
-0.04060378599858956
-0.027381537489063707
-0.0181277478673488
0.0005286443943319924
0.018974912603167797
0.00317942676449837
-0.005146333772156361
0.0073055741037551586
0.021900396279739623
-0.00669558276967171
-0.021645673856417587
0.0405293506573455
-0.03544612767334597
-0.04454476925767312
-0.014144406967426363
-0.016311777059216548
-0.04159324359430105
-0.04045189431420607
