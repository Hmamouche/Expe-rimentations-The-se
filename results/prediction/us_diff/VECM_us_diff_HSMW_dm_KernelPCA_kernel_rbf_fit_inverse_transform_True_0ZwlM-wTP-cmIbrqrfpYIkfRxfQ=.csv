# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; HSMW
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=13, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.2133403195268004
-0.05690805383103706
-0.3854390882528652
-0.03583932259421002
-0.25068091743153187
0.2855068242581618
0.035250732913302114
-0.09280333781745836
-0.13213955255998638
-0.0163979167923681
0.03392887791479783
0.1451013088185838
-0.024850319021125875
0.12313282961562245
-0.2577290580869301
0.12855972122198334
0.29028720138971753
-0.07706466976139337
-0.032342967135601614
-0.20552060060250327
0.15301850216210583
0.05152617061473962
-0.03018170708030593
0.14440193692173742
-0.07801682384193309
0.07988006081940427
0.08827991613406369
-0.14933929857953981
-0.051536446982605885
0.002510094071945211
-0.2288465964274129
0.19846223856332262
0.05163958528537386
-0.0006688123106355151
0.059223701603517434
0.07770680070846854
-0.06768667879681345
0.03292724340062411
0.07116432241479052
-0.24657954089626272
0.03301403841488068
-0.03857117496549701
0.1315609225751008
-0.19938352936207407
0.18455595775260628
-0.09668215528019461
0.13508503112269366
-0.0980382142174046
0.010166985876795702
0.07947810386511131
0.032265957621514074
-0.01618776125697616
-0.05830584293658039
-0.06518138519385244
0.12316418747978179
-0.06320652046756782
0.052331467632487375
0.0255439286830943
-0.027756103185506222
0.018726100134116994
0.07600397109379808
-0.0501300662288486
0.06215936714148736
-0.085842100560179
-0.09112823027838246
-0.02622188814665491
0.041935657066447335
0.0020648097652137265
-0.0595040120107452
0.07299836544818303
-0.019899950567458413
-0.028640824943584046
-0.08752267255549956
-0.030142442905003647
0.17023234332589776
0.05929077128630007
-0.10923132385944498
0.026917944966065255
-0.040456697598239
-0.016768334394207118
0.02371502925508036
0.0993981752320882
0.19978622407404512
0.0067577459070833085
-0.05308238406787394
-0.07006198384700908
0.015571211182338764
0.042265203953450334
-0.03847428192979681
0.05206870653647285
-0.0577209596103386
-0.08426521281542536
-0.06259297498026899
-0.0572381614692025
0.011523622255322448
-0.0784737801841962
-0.038354188006473736
-0.16343112571880952
0.14081299381096515
-0.07717924203699299
