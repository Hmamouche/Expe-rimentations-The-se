# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; EXRUK
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=8, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.04087926429132595
0.023334936842588763
0.04238843610912724
-0.05424189463174746
-0.09629922955427046
0.01859580799596239
0.0032324289579476403
-0.017692549042683234
0.048139292130204396
0.044158765829013014
0.08697044066396378
0.00908657873713659
0.03607245829283373
0.040968552144630424
0.10667370169016875
0.025509599017172875
-0.05347977933685384
0.08167571198409446
0.028524016352263395
0.011962418431926372
-0.07309676830487831
-0.006205131151474634
-0.006119676861661641
-0.05833430745310238
-0.013935661758329082
0.004937038882943431
0.01611227588608336
-0.027245515734127337
0.12120382279404389
0.0893994884701659
0.03609851981276772
-0.01814805623716419
0.058661681062822074
-0.0255889903815779
-0.08026351776712176
-0.012483762055618068
0.08798857413980084
-0.10673347198819963
-0.0030410480812326987
-0.05144364909358362
-0.061690420435955344
-0.009583258634551825
-0.023973262932002082
0.029084477365275883
0.029422658000503317
0.005643842724294282
-0.019872521430559804
0.0007104292550290693
-0.043628418747257716
-0.006525859977152869
-0.008323011803488312
0.05038093276965759
0.05060207565028356
0.01012143515971712
0.026577653374152736
-0.025330473490192833
0.0008732453219020316
0.004941521593032315
-0.01886342858633578
-0.006255943488415034
0.012191961900617596
0.01859966613436299
0.010364212148493267
0.02336226908375412
0.026843937052635644
-0.004100632619698608
-0.04168121673411731
-0.06115176631522249
-0.032819512907350974
-0.010816846566452668
-0.02017566097982838
-0.040379567243235
0.020506058834777234
0.010538806451787418
0.025205830310304066
0.037343295146335734
0.009706830928468344
-0.004194376753240518
0.010018770458644074
-0.0036966968981389653
0.01555338669719037
0.037864221295302924
0.004559970564765658
0.03383746011657908
0.010930554887548382
0.009384679793536462
-0.022904114150672596
-0.006427168544220212
0.015742912232348454
-0.010510239231343876
-0.01298993637114034
-0.005589554889997791
0.023722797611707147
-0.023484929716098033
0.0886471847836178
0.022623745670747895
1.9945050491465555e-06
0.002950855869050492
-0.008777129019928706
0.028171696317475937
