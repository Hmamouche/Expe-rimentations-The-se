# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; LHUR
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=10, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.006418490291059183
-0.071712872122075
-0.10840929770948976
0.051812066753873544
0.01990764477554502
-0.03074884949455695
-0.030290317702620997
-0.022370621867325233
0.0037679408244105372
-0.04724333446336546
-0.01562757066792304
0.01115664003089047
0.015790809036331954
-0.0001251584398069168
-0.03065616980237773
-0.023395137411198065
-0.08302182687180136
0.012225773344003111
-0.04272147518909752
-0.041611846184521434
-0.03999595067343863
-0.08843864499979187
0.028242083367389204
0.015745381591821588
0.04345955001267136
-0.0075985244743205175
0.013040169778038763
0.012907120068100571
0.03597026754851369
0.11106670361538168
0.10784124425966585
-0.0011609573915183878
-0.05535752912386583
-0.03282420164798635
0.02913037270469658
0.02803123271837077
0.03783748011489349
-0.016487372980060075
-0.013300528976263881
0.048862754232145755
-0.054240800238037984
-0.028467590514527383
-0.05865675113979013
-0.05159330623627379
-0.03724659149366771
-0.028617585179904082
-0.027083664683046044
0.006101310297387799
0.005461618031586637
0.01616148645679478
0.049663521363680685
-0.029900912519376266
-0.04672506008469213
-0.04919715897801864
-0.038057096348070744
0.0018339593875695455
-0.02839495164599417
-0.015684548022718986
-0.008231404761940335
-0.016048445319781154
0.03259810401914966
-0.0074965401550050595
-0.03715702634662248
-0.007462999960255645
-0.028279533075298437
-0.05191916374189368
0.03439055505416055
0.018060820481262237
0.023872264543718602
0.011261036819213789
0.05726377294268509
0.03399709644579725
0.07350657114567473
0.06824379818854337
-0.06611792748618991
0.006005943839179602
-0.005124046087993861
0.009098426497006297
0.07070822163899826
0.01695075078570737
-0.01521249447280862
-0.06629998274598282
-0.02652144116742209
0.022371918421945525
-0.009104988929562303
0.028470334195650135
-0.0008635224887995133
-0.01926831676479209
-0.0311372255188456
-0.0016424367723119927
-0.05068462026502103
0.007248917325832714
0.011178751722015247
-0.0204932315786946
-0.027394921968073582
0.011168341829492025
0.018059573181134977
0.050330294103506285
0.056314387210023846
0.05577934698974712
