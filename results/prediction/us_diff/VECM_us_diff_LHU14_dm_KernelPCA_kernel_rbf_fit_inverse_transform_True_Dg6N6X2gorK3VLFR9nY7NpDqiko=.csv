# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; LHU14
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=3, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.011697294039334057
-0.0072668560543523025
-0.03667385035562152
-0.00996700661575465
0.05969206663082757
-0.03127309535026816
-0.04809349675361122
-0.007220140243279369
0.017963097511215883
-0.037975839401717705
-0.021909862967162016
0.004957017970409856
0.02812660167013974
0.045198427407448644
-0.01400775333659373
-0.007622609626708684
-0.07017326687089978
-0.01067724020473173
-0.012714634766482946
-0.06142288674543306
-0.013697581232227717
-0.024804276886557206
-0.005287781120626482
0.021992323820430915
0.018271520321142726
-0.00040516833153119054
0.00465995714866935
0.034266434191232875
0.0302930240325617
0.06298356225053642
0.07644974691508552
0.0020894917382251457
-0.02800931688041986
0.02826430524340535
0.011536526374467223
0.029868933500532185
0.03172096667541539
-0.011121254817539597
-0.006955900846400312
0.01271043638861705
-0.028867106003382804
-0.058642083376724004
-0.02726677421977853
-0.01417456131526065
-0.011217677349934483
-0.039482690747374566
0.014389887025077354
0.016058943828299664
-0.009530980747135263
0.003912883335845042
0.002140534224478831
-0.017187654316911393
0.0020848594389001104
-0.025684272567708888
-0.02275210561763126
-0.008939777959057968
-0.03218370559725829
-0.013285878430194353
-0.016402712964024572
0.0011519463703313533
0.013030087765029254
-0.019474017398619668
-0.018904600988408245
-0.00725570072005215
-0.01934767308431861
-0.03789610956581063
0.009048374377332356
0.0015180578159211013
0.016172127848249646
0.015896820967656086
0.04626775242104488
0.01889263645156227
0.02938709647620854
0.08097597894101098
-0.008857333593163218
0.033680980352402176
0.03548173344211002
0.03383414051533685
0.035970072565027725
0.04031939849087829
-0.05303763443693864
-0.05169719634542056
-0.01652032877102861
0.010751353036008444
0.005068972012181655
-0.008629615866317683
-0.020688076486587305
0.0046082286706971304
-0.012192343554646923
-0.02224055897032013
-0.026815782936801598
0.006673285824747886
0.031567061022549914
-0.0017411352134737122
-0.018317151988321703
-0.007594931114567265
0.023364667456476345
0.02005179619075806
0.05234552073809166
0.04504973874562235
