# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; HSBR
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=5, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.09291790871842977
-0.09427878602483541
-0.05622512464845687
0.0032885756824099586
-0.0308135871390456
0.04457101226006646
0.09699958312279774
0.0708861358876185
0.020194492688649993
-0.00040042406705234623
0.060515443275233734
0.05949820092690576
-0.011203394108173153
-0.061460244274446
-0.05983803649917159
-0.06461575151756532
-0.05780669459745691
-0.04187576459519606
-0.024185975265236276
-0.07261251116830857
0.035119542135433994
-0.032907631009402215
-0.020376644638637
0.017248856968524677
0.020659687071444964
0.04314845361226671
-0.0007823176541682589
-0.029250534623748863
-0.07278804047690626
-0.04594253319826011
0.02377585858616453
-0.01807251751700633
0.036724335142427754
-0.0028385358602534064
0.004041497776261787
0.0026070178174253753
0.008066471752167209
-0.0401763231345979
-0.02662147418250363
-0.034286903427035324
-0.018595914721629164
0.08432492197079922
0.03504833291075288
-0.04659467599638492
0.03837782437891567
-0.061679119176368494
0.01832549991585627
0.0459397266139919
0.03395339647468353
0.054611081859096425
0.017167875013214557
-0.016677024932580536
0.019312887060963495
0.0037273865985782556
-0.012096469061682323
-0.004826203141669223
0.015845900185641394
-0.00948305158841159
0.05409670306621536
-0.013093378170719542
0.05596069532415919
0.04281274201245714
0.03531528196954082
-0.012171710294314242
-0.004830540376230285
-0.03391634373168052
-0.012745106868372349
-0.009431865086461363
0.009192488829474474
0.0022364176022908197
0.08389070042781593
0.039915545659979706
0.05298571679475503
0.057341197268657426
-0.035306523146446986
0.016816237213656826
-0.05313493744461864
0.029000016914594813
-0.04162617184458932
0.032227364225004614
0.008110114490294047
0.04719369188844405
0.00045074860940396365
-0.023915861903281842
0.019097233560125864
-0.0013570845788764586
-0.015870883444817144
0.042490290024319956
0.013679624490525816
0.015142418196490667
0.01856876002847751
-0.051348512118705794
-0.054995745030957305
-0.07722082340495816
-0.03761438765136262
-0.04196704186955454
-0.038097512895274535
-0.0990146560269368
-0.01779181781440741
-0.08511153676521861
