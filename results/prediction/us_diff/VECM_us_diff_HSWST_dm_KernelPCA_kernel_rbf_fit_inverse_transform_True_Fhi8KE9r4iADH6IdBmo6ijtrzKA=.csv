# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; HSWST
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=12, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.11462888096056364
0.06518581504831383
-0.21968602881856275
0.019044308867589406
0.033482234537489705
0.08236700236748119
0.024174163770779425
0.11771666161427526
0.0570494879264163
0.06281207578443919
0.1425524387818875
0.0361927851271704
0.0011054761673366092
-0.022889446165384245
-0.11002806269156801
-0.05387628108583836
-0.033207024685302136
-0.13306232041823451
-0.14898262939172202
-0.07838087811479715
0.01214227000608575
0.083924611841976
-0.014336233511344611
0.02184337479703012
0.0035579588899444965
0.033714002090119384
0.0315764747798539
-0.09079104419376524
-0.07400391093912753
-0.08672013928946545
-0.10585030152395591
-0.020919163386949137
0.0328417162676022
0.08563971223842974
-0.005167949687982808
-0.01631996016898306
-0.06391565007604139
0.07247563106803842
0.027409880552142725
-0.1298363524679877
0.11151717863483449
0.022712890005660485
-0.009938246022166954
-0.010474914707193642
0.03964106725164268
-0.11453224191109679
0.09854802170127169
-0.023066012286434328
0.14979863125684872
0.06012975658602497
-0.08840016333936579
0.024072851967432255
0.030698067718504272
-0.09499879652693981
0.013591989661434494
0.008621787482558683
0.055402249417196625
-0.009184909413202533
0.05503687212201126
-0.02170400191704997
0.03406080199188983
0.044002605802107075
-0.045308434556574045
-0.08446082770102578
0.005943820057053687
0.0012941051096328551
0.005206928801740399
-0.010844245552918201
-0.0636339945061874
0.0731876025328751
0.08546431600446948
0.012826954128808176
-0.014718153062131038
-0.011993176748664007
0.10709370642779072
-0.0028783633690324557
-0.10256201692389133
-0.01768380558189834
-0.04466900258086446
0.026994217168192423
0.19394031812664964
-0.003975207368641206
0.0304513558683597
-0.015138712160808851
0.0009066293922636894
-0.035472149789562576
-0.011071418715988959
0.08913742597750898
-0.0632864582969662
-0.001246453080006529
-0.02557517035617094
-0.02637973060262919
-0.06542898580534838
-0.005467614572355993
-0.03688166169249728
-0.07976926032116416
0.010756996382601226
-0.1637596831375433
-0.007784117183817959
-0.1653865923902413
