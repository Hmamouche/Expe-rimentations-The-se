# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; LHU15
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=4, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.04132912526111154
-0.08629313016552058
-0.08966111288563844
-0.02792006688421351
-0.029058003462150782
-0.023013254448329155
-0.02701322173809738
-0.00992843721853839
-0.0026501660434800423
-0.023471331019412147
-0.023060480356335875
-0.005060025519688514
0.018687218193182392
-0.006474121903286053
-0.012275161002493805
-0.009881059700140141
-0.06622830169342032
-0.024855226163130843
-0.01632580080726949
-0.03274054562456468
-0.013481064754093261
-0.03662344932669123
-0.01805721491368962
0.003435211594847867
0.029278313062053102
0.011249127248140277
-0.010580841967835968
0.013790208108367188
0.053537812771789664
0.0636179445051008
0.06801167584137455
0.05208755919322573
-0.011839309509676902
0.055954167161130934
0.05249732883549478
0.06395774254275857
0.03152182530406158
0.02402778332490084
-0.04391344732766009
0.014162855618861938
-0.002697113484025236
-0.02486225251973387
-0.024157365931412336
-0.07487197625566712
-0.012823056046033399
-0.043145179759311605
-0.040371522402002626
0.016757273281227555
-0.022508420444293225
0.030487534000803562
-0.016250460600475594
0.017022287184686954
-0.03746135268493352
-0.031034192514357174
-0.02287684418905733
-0.007865295926511537
-0.038614596366901635
-0.04619092722634135
-0.016521211757467277
-0.028025954156964076
0.024431416879114647
-0.00809486111129896
-0.005172866801163018
-0.03402118636671743
-0.008836482983335867
-0.03114547178597413
-0.033274369431564074
0.004460181200529619
0.016154597249924206
0.013516943181043123
0.05596333369631753
0.040570519651167924
0.07003069541152639
0.09681650247047781
0.05644676512268623
0.0261613123517615
0.008275901861714244
0.06186139721154656
0.02679240281247805
0.06417519434770003
0.011270743401184634
-0.033631443925544194
-0.05082081183206871
-0.029739196783021577
-0.018408103739312717
-0.011146029579811999
-0.008933492686946171
-0.025582445175055524
-0.03763958062246167
-0.009575849630114788
-0.02657088103171986
-0.025196925430426387
0.0031188317366538723
-0.01692419671249368
-0.0007464909697613873
-0.00863459564196973
0.04117360558061203
0.01620811291388574
0.03464936215365933
0.0733602292943833
