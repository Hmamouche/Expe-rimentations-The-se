# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; LHU26
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=8, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.0006140235607279415
-0.1226126291709306
-0.05173970986303637
0.05938081279228895
-0.01610235695519516
0.06347284567072273
-0.0729213677699222
0.00037986010344375973
-0.026703095726443166
0.014287287486963687
-0.01907591714266515
-0.003051730008514013
0.033736926003593924
-0.014796082169304956
-0.0006713643175337951
-0.022006587341344513
-0.05626768058937656
-0.028679060939274772
-0.0005896884564789906
-0.04991895761462671
-0.010372626080121699
-0.06549860997500637
0.037980703011938385
0.013991536555969235
0.044395112075718994
0.01580809366390601
-0.006518856371579075
0.0491917082710996
0.020423191387838546
0.12536509144977703
0.07756355053702864
0.01135110626181712
-0.0812235574700452
0.07178400906544144
0.04231357156961309
0.023879520059257545
0.03786429990008451
-0.014686653871600005
-0.04917737884442854
0.00043131470690919646
-0.019820409035796918
-0.03162267986257314
-0.01774849942278766
-0.0756742744831347
0.00677037533338137
-0.07957006117290882
0.012934963942521409
0.0009376659977763796
0.023574873097171472
0.07613318554719362
0.0022239906801501327
-0.02792650380062637
-0.04278125568882655
-0.06492945439472689
-0.037513222577135005
0.0255596005703464
-0.05895576404161529
-0.009249485974588984
0.01625548578205365
-0.03352766966580311
0.04020091323098425
-0.041286693148968004
0.02470724116031791
-0.05495072107418182
-0.030921495134867055
-0.017810698666661604
-0.030697651180284422
0.035325615964432226
0.0008955262190407437
0.024815007737939578
0.07621809570266039
0.07754992655983646
0.1026053430619393
0.112515002248004
-0.0036237411460847066
-0.009227220355240904
-0.0006902991430380451
0.06315359679827794
0.0163261876605906
-0.0005194883110027472
0.008591025178490197
-0.04355346751322614
-0.02971963634369483
-0.02857288232836495
0.026772302324451117
-0.007311789349762298
0.0039638926574129246
-0.01387984014602129
-0.024085378875159984
-0.013600407736044443
-0.034505096698164955
-0.03725689331534929
0.056280922996070404
-0.04305506839185853
0.005039801054213035
-0.01686555966323236
0.03549055722538903
0.06278280610582775
0.010577000909448516
0.09779466392632263
