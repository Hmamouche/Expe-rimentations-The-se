# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; EXRCAN
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=4, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.008689026971067954
0.007194823288034988
0.02959360602192383
0.05038676526369887
0.029334317479131546
0.024936593887339143
0.02782673382582957
0.02270034751016255
0.010868361995567592
0.0072302644680991915
0.03118776613485396
-0.00900883070572677
0.000968113566234388
-0.00536799163366211
-0.026336620147980622
-0.0252854657802181
-0.033441025480112435
0.009379225645909768
-0.043289679859354495
-0.03550015632882326
-0.043024261379151
-0.024508403045482713
-0.03709849450757419
-0.015465975244390634
-0.018420324563951786
-0.014342163781429398
-0.0028578350011111216
-0.025351940812046184
-0.01459171655976391
-0.009756419136185357
-0.00655821481050923
0.007340526996914844
-0.01766894663448789
-0.014405769787655339
0.052368056309769186
-0.001397722236021878
0.03490858725275848
0.05499049574636991
0.004846831995630692
0.05148838258881041
0.011423212084708671
0.044184720502834986
0.03694588151851341
0.04149231637153056
0.019125759701506373
0.04136079475710458
0.012924630105924451
-0.008042329130633385
0.018893738782925256
-0.05758465969491662
0.019637002954858734
-0.010394863243482922
0.0039791696885189375
-0.0060529913850691916
0.007788846658093021
0.0014187183241152984
0.02558879733348539
0.035904870216714124
0.0131455930812691
0.035753392203150006
0.05727865158490129
0.040498083594800324
0.03587602924668797
-0.024969612696136075
0.003635850387076738
-0.025293339886885327
-0.0018235549732978076
0.004773470764452531
-0.009892803925367285
0.04649615161234745
-0.01583726826297771
0.04898438330659551
0.0063154469299239475
0.03035779263483322
0.010685763024980043
-0.008852270203683007
0.01610970849925263
-0.02405140802779939
-0.020782367022266893
-0.08625829115475737
-0.038249129164285016
-0.12972550768076924
-0.01723176602138936
-0.03187541845420105
-0.03624887945563833
-0.04691960823348801
-0.04055145809484511
-0.056353365255507076
-0.04193254960616969
-0.029713179989059632
-0.031872732073900664
-0.04332489910879407
-0.023981239533890735
-0.014411601605604655
0.014791948789576325
-0.04544054357188014
0.004270194601460931
-0.08505664139426738
-0.02328619878508332
-0.07017826590848877
