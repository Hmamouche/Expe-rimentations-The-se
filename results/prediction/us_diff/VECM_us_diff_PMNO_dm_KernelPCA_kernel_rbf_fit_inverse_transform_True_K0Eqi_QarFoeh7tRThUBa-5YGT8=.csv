# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; PMNO
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=6, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.11667420691034014
-0.23680519234640252
0.07640315826050813
-0.1914481822087181
-0.05082950667644754
0.07304177322509006
0.1182003715424845
0.02456470785261588
-0.081335367292961
0.06390819599920587
0.07625460711121702
0.09520115421249059
-0.09517329959574583
-0.04029852236326059
0.04443183254799629
-0.020756007547708115
-0.022422634722492264
-0.09754826277280818
0.053529380774983315
-0.01888687147940371
-0.0027639587482778706
0.09706519825527382
-0.10504631671082053
-0.008975214526288945
0.017801478085001682
-0.01061135288225674
0.0015120485978428189
-0.023166081860604426
-0.04763158573769051
-0.008024152230233048
0.22093510649975923
0.08254647703547148
0.0505595462738777
-0.029398782590265248
-0.043283671365388084
0.019986155891174584
0.012869645889899
0.04278724224076451
-0.06134732060271241
-0.011810853022223688
-0.03328407267922231
0.023643182826178638
0.054156938504857685
-0.15276092579305017
-0.025960914860797265
0.038124622446960965
-0.07876765045230724
0.06067406957255436
-0.009481186657927881
-0.009262108226832004
-0.0027169868165893424
-0.07151686757245444
0.04007144540162842
0.012092633608272969
-0.055882374355307116
0.06839395700279396
0.045464935195297526
-0.038689888596672675
0.012310964776272474
-0.03976668339943049
-0.063992476439028
0.018196900285286227
0.062188099802979006
-0.08078759667938747
0.008861426657894215
0.019644980047452902
0.02513639200246977
-0.029636060938023418
-0.031154895361059847
-0.019391737380266436
-0.0016925670062110688
0.0851403881415234
0.03492987994351957
0.1055842560997547
0.09856149404069814
0.030964482970166372
0.0035572511382384875
-0.08653507936716054
-0.0997103657056864
0.1260436523965069
-0.0486038417319998
0.0002645469824447376
-0.015469961548667371
-0.0759440875387718
-0.032257730497405705
-0.016019642080170704
0.007562911965505155
-0.0003684243699035965
-0.034746826133276267
-0.05587200561883309
0.08986661777505041
-0.1070013841979364
-0.038837980414635785
0.08623033350337671
-0.01662189749736174
-0.026884453504800288
0.05007852772537578
-0.057171001333594595
-0.015881860008738948
0.10476964554361504
