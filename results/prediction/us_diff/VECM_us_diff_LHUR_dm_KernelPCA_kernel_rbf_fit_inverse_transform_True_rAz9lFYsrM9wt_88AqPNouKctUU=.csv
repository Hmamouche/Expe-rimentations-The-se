# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; LHUR
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=5, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.007545354624238866
-0.07014156055259063
-0.07756324934007366
-0.029378432808468864
0.019490332086581484
-0.017840682391206036
-0.056066990535568906
0.00849231466713051
0.014367512836857596
-0.0271859344369696
-0.02155482121577404
-0.013376093638715949
0.005979599968045375
0.030661089052688637
-0.019010609648570376
-0.03048907718968698
-0.07865662465334634
-0.014445493228333427
-0.04670358960335668
-0.014491821441918507
-0.02033409958803651
-0.04514021654203375
-0.01692657987753589
0.013681587797458667
0.015876165868108787
0.012438665195232521
-0.005589565098439456
0.007907548848784612
0.05659226922456612
0.08684019644726172
0.06976252638322773
0.009814257255449632
-0.04745941720080046
0.007873047950419241
0.027223780216835206
0.05427145737234945
0.028506157106331753
-0.015355977359116294
-0.009892432730577622
0.028577159379682936
-0.04173754780163637
-0.03543522591749338
-0.03972741768264733
-0.05196646850348517
-0.02317090859873661
-0.04241447369677249
-0.014594066300650061
0.011359785053610665
-0.01053111968250721
-0.005939391241207613
0.0028367277172619797
-0.002022620191157087
-0.03136806491799151
-0.018073179976773636
-0.021527214366056046
-0.027244612571935632
-0.03866283959395649
-0.02606715329991571
-0.015793403800809534
-0.002551017175804751
0.026102765752584946
-0.005573337378641142
-0.025311396009407662
-0.004536718009294696
-0.015321959898165639
-0.0363381358546738
-0.007052348909177719
-0.003456659057802671
0.015673106398293314
0.004755223702462608
0.04840269872405388
0.0271737509367937
0.04589303576358971
0.07645846645051765
4.914084866377774e-06
0.02845209349363445
-0.0007388706190728823
0.03832377808923603
0.038518714117614504
0.026730707965347485
-0.025047618785668014
-0.060367957131619306
-0.03027711895934981
0.0016542192565141625
-0.0069926250323235125
0.006773052139977335
-0.017196458710921892
-0.0037791287377103725
-0.022006613120538625
-0.00529025121673819
-0.042346491905614
0.00633024050182915
0.0054106253083535385
-0.00848444562977085
-0.01551501250137236
-0.018070545249064145
0.018522023367368873
0.030083914421380854
0.03539230519732095
0.052988756871543946
