# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FYGT10
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=10, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.05281411073047257
0.15352955879098495
0.09701566433797795
-0.04553201107115522
0.025133254603733522
-0.05171324971322158
-0.07222482221733718
-0.06249255472257531
-0.0020970548387392457
-0.0034917692949485574
-0.07167354426069814
-0.0768084301757394
-0.07163682631540566
-0.0006048668219863496
-0.027786769296041333
-0.005208720846517054
0.03882524855860016
0.013019394737290605
0.04829911511152927
-0.05710526948435237
0.01802202578000877
0.10153124899618302
0.0036807868355374293
-0.054639135045927816
-0.03998364794851338
0.005245034664003584
-0.002725024937136576
-0.0008213496288548353
-0.04460139073478588
0.008648716750019686
-0.04918859295104854
-0.053777562179723896
0.06449412559292475
0.039606220525893826
-0.019934926147238807
-0.006106263559789996
-0.06525968055669935
-0.013146230233894884
-0.0306983188940301
-0.049414561121352
0.022274113544448783
2.2351518248708152e-05
0.04923186959041943
0.02825905555240467
0.060144984080309855
0.09132888280434912
-0.05453812017139906
-0.0073839092459551895
-0.024651920388291913
-0.02869925440994666
-0.04773758011141153
0.04613070140879538
0.04519050368894832
0.005475885508328428
-0.0028315192731182266
0.005815496142577285
0.01598934013969759
-0.022979013629855313
-0.02028491875396632
-0.03536989612907768
-0.0035567341642735242
-0.05601299517766318
0.05838366335826248
-0.024363440391150126
0.005103905506538347
0.06213702534101545
-0.011558080197310895
-0.06870225134640648
-0.017154741903343755
-0.021835364128390378
-0.022962108619875535
0.021134723393608442
-0.015736977563530898
-0.036040238251085197
0.04694130829677549
-0.0050198303452059605
-0.07010973285099473
0.01571337547096451
-0.06302645263390289
0.013902842660790126
0.012120157076203703
0.03984098545079119
0.038481414175098226
-0.04267607575402064
-0.050826581379648295
0.007769519589685166
-0.0200994648228564
-0.013980673231475064
0.037527344723113254
0.03845226713778151
0.06494752959812641
0.023370370938921824
0.0078518141056481
-0.0034347073441162003
0.008483998734841569
-0.017926546395149217
0.019978713045898207
-0.06485300649117251
-0.07752313755173476
-0.011887199253123886
