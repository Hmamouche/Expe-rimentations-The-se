# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; GDP286A
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=5, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.006693350031812658
0.003331698538537876
0.010806233974841779
0.01093375197841331
0.005701233062385826
0.00487043233063277
0.005859207739707167
0.0023104878015565832
0.0026141911891085965
0.003174266202670443
0.00236442549245219
0.0020095219957096708
0.002228705472596366
0.0024203880336269696
0.0049755958485967425
0.005059125654996497
0.0048677344343726146
0.005662497514284306
0.002272226353513242
0.0032605074718095994
0.0031941062080851237
0.0038639938484148406
0.005194991580265944
0.003570804532033848
0.0044728056372780834
0.004802843308543447
0.004780177899961
0.00545417693964146
0.006298885834005825
0.007881967628188945
0.005089490989267064
0.005888577452116706
0.007403883082724449
0.003420667158071664
0.002578275589634352
0.004177214076966869
0.004171182415462584
0.00465992154475342
0.0034332229460108015
0.0032850047792768095
0.004080103153589148
0.003642054318991587
0.004640080939236466
0.006548963909173764
0.005573462304159712
0.005493903912138135
0.0032567651495355133
0.003760246167945869
0.004530046889395392
0.004755622612960675
0.004983345988610993
0.004619937273256399
0.005880521489142287
0.004139515561450184
0.0014275582380529233
0.004683522531256504
0.004321643716632015
0.004035599499825806
0.0011152827819142044
0.0030585672823540927
0.0028989342012010182
0.002643916982974745
0.002927478236296407
0.0046156702427792426
0.0061641170660948165
0.008741913200413334
0.008213305401532976
0.006781386092922792
0.008266324912019242
0.008139532154545851
0.0041588906354704
0.006212385245928646
0.004455857100699022
0.0044146993093687765
0.0053563245587876784
0.004974867800728094
0.005991741855346336
0.007146349226539082
0.007220601366040798
0.007014700106498533
0.012490897602846694
0.011032355162717751
0.004364744559458548
0.009808279053513683
0.009764458399426778
0.014160373099201724
0.011389322183926583
0.011779131979127678
0.014286356954091748
0.014866454474558012
0.011025144308945682
0.015776827573252758
0.010597671756715133
0.010486481932385388
0.010810713061722968
0.009957467464666334
0.010222904134321333
0.014524727421881998
0.012383305176006084
0.012989497704282876
