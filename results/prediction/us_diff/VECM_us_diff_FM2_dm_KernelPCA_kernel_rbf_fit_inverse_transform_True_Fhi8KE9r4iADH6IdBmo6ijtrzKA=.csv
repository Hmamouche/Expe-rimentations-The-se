# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FM2
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=12, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
0.013106503591133533
0.004117722544675165
0.0049542194868440565
0.0040640370024320185
0.006777395264019013
0.006134534474644944
0.005686410046907803
0.007005584795614753
0.005150877836962572
0.00732399762827261
0.0056006996154661665
0.00799822229613871
0.006378541996888828
0.006481712299951675
0.003178000639760553
0.005149852526057684
0.006840642404321944
0.0041934625435371894
0.006216687962290437
0.0052671325264638
0.004618759642888432
0.005894456927086823
0.004718939946930277
0.005532129742088436
0.0061622242523476254
0.0051848367333334625
0.003398005027717422
0.004196437842058448
0.004588915585903693
0.004138054679842798
0.00717381192137791
0.004685651475315047
0.004640485034066668
0.00478978654148185
0.0038366308191039524
0.0010520511738587107
0.0023057581949812314
0.0023617157581688904
-0.0008661622525832336
0.0010448586419769701
0.00029003150100679833
0.00259325788778244
0.0006773208083503049
0.0005417651887349245
0.002130865457807611
-0.0006774617249075048
0.0019932400989722994
0.004426078596630741
0.005047409075439115
0.00374637755030179
0.004869346669024932
0.0023168247765788107
0.00635303388496485
0.0064388544142505635
0.004500061786290085
0.0067052538397012786
0.007603857224077734
0.006740378075142202
0.01003504876958524
0.007651283671701668
0.010114689542933574
0.012060259130080004
0.009053211768963862
0.009616550287823575
0.00748310888946878
0.009377230938819214
0.008019146901633415
0.011903703921593815
0.008002275955369823
0.008896616816592732
0.014708785253663675
0.012577034509366631
0.013648112922071649
0.012981776197689289
0.011805786650405317
0.009399789372543885
0.015870468530955705
0.014353806728720254
0.012095623875424793
0.01287664886344362
0.011000845041235663
0.006712218898444689
0.014790833726203708
0.00854681561049308
0.009223769128428405
0.012140699524048759
0.002324877423435488
0.013325457774018758
0.008753809097710914
0.011604413135607247
0.010329146518942687
0.006118484876780895
0.009930025477600466
0.012045052233603418
0.011023011584120305
0.01332921381109386
0.014089129107924085
0.013178522199838349
0.020712045711668306
0.011166616069944488
