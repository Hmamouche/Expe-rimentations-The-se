# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FSDXP
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=11, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.032496191403861376
0.13174796393486268
0.05552460031381273
0.14981243817225537
-0.0483369951420673
0.008138366072428394
-0.05784502093182129
0.06942272374625817
-0.14155953777677777
-0.1028552935630035
-0.10334721620570265
-0.07290771620882028
-0.04390128084023792
-0.026496650879422025
0.0027333918899756157
-0.04400789248555577
0.060278937196599505
0.09668944037419458
0.016922616680032202
-5.364506339509273e-06
-0.0312784434710939
0.0404882448284577
-0.017899296286067613
0.018453363033262723
0.003953139256578052
-0.03170771314400494
-0.011355884516714881
0.026584167348015934
-0.0037745879752083015
-0.009832891824022118
-0.06449865141016767
0.05997598355876295
-0.01151816149531618
-0.026231753273523555
-0.020428066014996724
-0.00954372433255546
-0.07524801123121705
0.04524632740639473
0.0290421061822124
-0.009585170092625057
0.014070974296534233
-0.018117292093427648
0.03368005372785789
0.042150220128610666
-0.03168977880268592
0.007012682368493334
-0.002171271568512682
-0.04163939032342243
-0.10526464531379112
-0.02875205408296195
-0.026844269054256256
0.06288622130725978
0.0037260542806025355
0.012670051540311287
-0.0765435710944627
0.04998385691945486
-0.03292710815348156
-0.0409711662770366
-0.03272559192666401
-0.04725808152909573
0.006349428664391605
-0.061257025243898616
0.008966217842298499
0.037414058523913045
-0.03451279540639734
0.0036903842741852055
-0.037356357654275564
0.0023524517384331586
-0.012546456962233186
-0.027434157980253292
-0.012867222369843417
-0.01087017742478091
0.05905639633626655
-0.0591750732613221
-0.006264898746916839
-0.001756201920018739
-0.03632870691227526
0.06313782505771207
0.05880412136868203
0.05377150567696304
-0.05508444887411489
0.006213943602432506
0.029801441393128427
0.032669750292091675
0.04965074038348406
-0.048937630278671505
-0.035953649628251816
0.0070523600569701375
0.01837599014302589
0.015575838401581263
-0.006687238101177474
0.010022808903889001
-0.04571271404635832
0.002260484579760323
0.016014356988923294
-0.03832562925728432
-0.011717253386244336
0.00941457027016777
-0.018739260474532924
0.022417489791453773
