# name_prefix; us_data_stock_watson_2012;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# description; Stock and Watson 2012: US indicators;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# lag_parameter; 4;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# predict; FYFF
# number_predictions; 100;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# horizon; 1;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# prediction_type; rolling;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# max_attributes; 15;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
# method; dm_KernelPCA_kernel_rbf_fit_inverse_transform_True(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto', fit_inverse_transform=True, gamma=None, kernel='rbf', kernel_params=None, max_iter=None, n_components=8, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)
# predict_model; VECM
Predictions
-0.06450456835970764
-0.03638663572594
0.0406745908024269
-0.017710795190419668
-0.03146104643406174
-0.0767986820477689
0.025794454195162714
-0.044363528687031056
-0.01568753746618766
-0.037984843410759694
0.023770071131526675
-0.043083431214480314
-0.054581597280589304
-0.0008410678600802822
0.029839839497442153
-0.0031689863818748086
0.0039477767117720875
-0.03612943234855806
0.006162416070449539
0.026707364285056538
0.01982366028145731
0.08001400479185188
-0.03287113895289677
-0.017735224840975828
-0.02743011997054214
0.012957487469066203
0.05107109019032902
0.019294221104566443
-0.0972383578227203
-0.003962584286303897
-0.08238639354378156
-0.10336922163559153
-0.008709874187169467
-0.003408179452382809
0.035416495932952985
-0.04549091804412354
-0.06534361009997229
0.020374115172881985
-0.018882911286649845
0.02498966707784451
0.10948929856273194
-0.008433713778242959
-0.0028733841589372623
-0.016890751473878984
-0.009031657643273306
0.07917935532932087
0.052057189810053334
-0.02111848498695843
0.028159342967485575
-0.03041774257518068
-0.02328309513076574
0.016906669999156056
-0.03281315093273364
-0.011478831686456524
0.014338088057217738
0.026484428363916727
0.001211761032685086
0.016420360744246
0.004542174428538156
-0.020692721996679854
-0.04919014686964559
-0.03804719609128257
0.02874602223175262
-0.04666426731603049
0.04826409150734956
0.03347132705923625
0.03136487439386282
0.014526899256015037
-0.028520612366340993
-0.014400385048680662
-0.0018631041401997788
-0.026494437923688045
-0.1020062297122238
-0.15371366278957263
0.04980088442605895
-0.06262731013526907
0.062047809445875304
-0.033990314962063874
0.010250433130729772
0.014046623370060563
-0.003826592037714872
0.012868303528327001
0.02639843013834829
0.03993196988276811
0.005905367910282632
-0.005896246937726551
0.007443650184197962
-0.002712351954921831
0.024329768959200054
0.025857943040256758
0.0391266784440746
0.01232067477356748
0.00024025674838210054
-0.007049203504494905
0.032063610229496727
-0.003706135107967177
-0.009138452390036893
-0.0622962508770927
-0.037978564799065154
0.03801905156854503
